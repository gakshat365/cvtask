{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14760510,"sourceType":"datasetVersion","datasetId":9434511}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 3-Layer CNN for Colored MNIST Classification\n# Converted from task1_v2.ipynb\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\n\n# ===== CONFIGURATION =====\n# All hyperparameters and settings in one place\n\n# Data paths\nTRAIN_DATA_PATH = '/kaggle/input/cmnistneo1/train_data_rg95z.npz'\nTEST_DATASETS = {\n    'rg95z': '/kaggle/input/cmnistneo1/test_data_rg95z.npz',\n    'gr95z': '/kaggle/input/cmnistneo1/test_data_gr95z.npz',\n    'gr95e': '/kaggle/input/cmnistneo1/test_data_gr95e.npz',\n    'gr95m': '/kaggle/input/cmnistneo1/test_data_gr95m.npz',\n    'gr95h': '/kaggle/input/cmnistneo1/test_data_gr95h.npz',\n    'gr95vh': '/kaggle/input/cmnistneo1/test_data_gr95vh.npz',\n    'bw95z': '/kaggle/input/cmnistneo1/test_data_bw95z.npz',\n    'bw100z': '/kaggle/input/cmnistneo1/test_data_bw100z.npz'\n}\n\n# Model hyperparameters\nNUM_CLASSES = 10\nCONV1_CHANNELS = 32\nCONV2_CHANNELS = 64\nCONV3_CHANNELS = 64\nFC1_UNITS = 128\nDROPOUT_RATE = 0.0\n\n# Training hyperparameters\nEPOCHS = 20\nBATCH_SIZE = 128\nLEARNING_RATE = 0.02\nVALIDATION_SPLIT = 0.1\n\n# Optimizer settings\nOPTIMIZER = 'Adam'  # Options: 'Adam', 'SGD', 'RMSprop'\n\nNAME = 'task1approach4gf1'\n\n# ===== END CONFIGURATION =====\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:42:16.626678Z","iopub.execute_input":"2026-02-07T16:42:16.627518Z","iopub.status.idle":"2026-02-07T16:42:16.63368Z","shell.execute_reply.started":"2026-02-07T16:42:16.627487Z","shell.execute_reply":"2026-02-07T16:42:16.632804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load training data\ntrain_data = np.load(TRAIN_DATA_PATH)\nX_train = train_data['images']\ny_train = train_data['labels']\n\n# Load test datasets from dictionary\ntest_data_dict = {}\nfor name, path in TEST_DATASETS.items():\n    data = np.load(path)\n    test_data_dict[name] = {\n        'images': data['images'],\n        'labels': data['labels']\n    }\n    print(f\"{name} shape: {data['images'].shape}, labels: {data['labels'].shape}\")\n\nprint(f\"\\nTraining data shape: {X_train.shape}\")\nprint(f\"Training labels shape: {y_train.shape}\")\nprint(f\"Number of classes: {len(np.unique(y_train))}\")\n\n# Normalize pixel values to [0, 1]\nX_train = X_train.astype('float32') / 255.0\n\n# Normalize test datasets\nfor name in test_data_dict:\n    test_data_dict[name]['images'] = test_data_dict[name]['images'].astype('float32') / 255.0\n\n# Create validation split\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=VALIDATION_SPLIT, random_state=42\n)\n\n# Convert to PyTorch tensors and change format from (N, H, W, C) to (N, C, H, W)\nX_train_tensor = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\nX_val_tensor = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n\ny_train_tensor = torch.LongTensor(y_train)\ny_val_tensor = torch.LongTensor(y_val)\n\n# Convert test datasets to tensors\ntest_tensors = {}\nfor name in test_data_dict:\n    X_test = test_data_dict[name]['images']\n    y_test = test_data_dict[name]['labels']\n    \n    test_tensors[name] = {\n        'X': torch.FloatTensor(X_test).permute(0, 3, 1, 2),\n        'y': torch.LongTensor(y_test)\n    }\n\n# Create DataLoaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Create test loaders\ntest_loaders = {}\nfor name in test_tensors:\n    dataset = TensorDataset(test_tensors[name]['X'], test_tensors[name]['y'])\n    test_loaders[name] = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Training set: {X_train_tensor.shape}\")\nprint(f\"Validation set: {X_val_tensor.shape}\")\nfor name in test_tensors:\n    print(f\"Test set {name}: {test_tensors[name]['X'].shape}\")\nprint(f\"Input shape: {X_train_tensor.shape[1:]}\")\n\n# Helper function to create Gaussian kernel\ndef create_gaussian_kernel(kernel_size=3, sigma=11.0):\n    \"\"\"\n    Create a 2D Gaussian kernel for filtering.\n    \n    Args:\n        kernel_size: Size of the kernel (default: 3)\n        sigma: Standard deviation of the Gaussian distribution (default: 1.0)\n    \n    Returns:\n        Normalized Gaussian kernel of shape (kernel_size, kernel_size)\n    \"\"\"\n    # Create a coordinate grid\n    ax = np.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1.)\n    xx, yy = np.meshgrid(ax, ax)\n    \n    # Calculate Gaussian kernel\n    kernel = np.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n    \n    # Normalize the kernel\n    kernel = kernel / np.sum(kernel)\n    \n    return kernel\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:42:16.637066Z","iopub.execute_input":"2026-02-07T16:42:16.637738Z","iopub.status.idle":"2026-02-07T16:42:18.525953Z","shell.execute_reply.started":"2026-02-07T16:42:16.637711Z","shell.execute_reply":"2026-02-07T16:42:18.525181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Build the 3-Layer CNN Model\nclass CNN3Layer(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES, use_gaussian_conv1=True):\n        super(CNN3Layer, self).__init__()\n        \n        self.use_gaussian_conv1 = use_gaussian_conv1\n\n        # First Convolutional Layer\n        self.conv1 = nn.Conv2d(3, CONV1_CHANNELS, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(2, 2)\n\n        # Second Convolutional Layer\n        self.conv2 = nn.Conv2d(CONV1_CHANNELS, CONV2_CHANNELS, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(2, 2)\n\n        # Third Convolutional Layer\n        self.conv3 = nn.Conv2d(CONV2_CHANNELS, CONV3_CHANNELS, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(2, 2)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(CONV3_CHANNELS * 3 * 3, FC1_UNITS)\n        self.dropout = nn.Dropout(DROPOUT_RATE)\n        self.fc2 = nn.Linear(FC1_UNITS, num_classes)\n        \n        # Initialize first conv layer with Gaussian filter if enabled\n        if self.use_gaussian_conv1:\n            self._initialize_gaussian_conv1()\n    \n    def _initialize_gaussian_conv1(self):\n        \"\"\"\n        Initialize the first convolutional layer with Gaussian filter weights.\n        This creates a hybrid approach where conv1 uses Gaussian filtering.\n        \"\"\"\n        # Create Gaussian kernel\n        gaussian_kernel = create_gaussian_kernel(kernel_size=3, sigma=51.0)\n        \n        # Get the weight tensor of conv1\n        # Shape: (out_channels, in_channels, kernel_height, kernel_width)\n        # For conv1: (32, 3, 3, 3)\n        with torch.no_grad():\n            # Initialize all weights with the Gaussian kernel\n            for out_ch in range(self.conv1.out_channels):\n                for in_ch in range(self.conv1.in_channels):\n                    # Apply Gaussian kernel to each channel combination\n                    self.conv1.weight[out_ch, in_ch] = torch.FloatTensor(gaussian_kernel)\n            \n            # Initialize biases to zero for the first layer\n            if self.conv1.bias is not None:\n                self.conv1.bias.zero_()\n\n    def forward(self, x):\n        # First conv block\n        x = self.pool1(F.relu(self.conv1(x)))\n\n        # Second conv block\n        x = self.pool2(F.relu(self.conv2(x)))\n\n        # Third conv block\n        x = self.pool3(F.relu(self.conv3(x)))\n\n        # Flatten\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:42:18.52759Z","iopub.execute_input":"2026-02-07T16:42:18.52785Z","iopub.status.idle":"2026-02-07T16:42:18.537346Z","shell.execute_reply.started":"2026-02-07T16:42:18.527829Z","shell.execute_reply":"2026-02-07T16:42:18.536561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Initialize model and move to device\nmodel = CNN3Layer(num_classes=NUM_CLASSES).to(device)\n\n# Display model architecture\nprint(model)\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"HYBRID FILTERING APPROACH\")\nprint(\"=\"*60)\nprint(\"Conv Layer 1: Gaussian filter initialization (sigma=1.0)\")\nprint(\"Conv Layer 2: Standard PyTorch initialization (Kaiming uniform)\")\nprint(\"Conv Layer 3: Standard PyTorch initialization (Kaiming uniform)\")\nprint(\"=\"*60 + \"\\n\")\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\nif OPTIMIZER == 'Adam':\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nelif OPTIMIZER == 'SGD':\n    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\nelif OPTIMIZER == 'RMSprop':\n    optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\nelse:\n    raise ValueError(f\"Unknown optimizer: {OPTIMIZER}\")\n\nprint(\"Model compiled successfully!\")\nprint(f\"Optimizer: {OPTIMIZER}\")\nprint(f\"Loss function: CrossEntropyLoss\")\n\n# Lists to store history\ntrain_loss_history = []\ntrain_acc_history = []\nval_loss_history = []\nval_acc_history = []\n\n# Training loop\nfor epoch in range(EPOCHS):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        train_loss += loss.item()\n        _, predicted = torch.max(output.data, 1)\n        train_total += target.size(0)\n        train_correct += (predicted == target).sum().item()\n\n    # Calculate average training metrics\n    avg_train_loss = train_loss / len(train_loader)\n    train_accuracy = train_correct / train_total\n\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n\n            val_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            val_total += target.size(0)\n            val_correct += (predicted == target).sum().item()\n\n    # Calculate average validation metrics\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = val_correct / val_total\n\n    # Store history\n    train_loss_history.append(avg_train_loss)\n    train_acc_history.append(train_accuracy)\n    val_loss_history.append(avg_val_loss)\n    val_acc_history.append(val_accuracy)\n\n    # Print epoch results\n    print(f'Epoch [{epoch+1}/{EPOCHS}] - '\n          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f} - '\n          f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n\nprint(\"\\nTraining completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:42:18.538258Z","iopub.execute_input":"2026-02-07T16:42:18.538531Z","iopub.status.idle":"2026-02-07T16:43:06.226004Z","shell.execute_reply.started":"2026-02-07T16:42:18.53851Z","shell.execute_reply":"2026-02-07T16:43:06.225369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Save the model\nmodel_save_path = NAME+'_model.pth'\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"\\nModel state dict saved to: {model_save_path}\")\n\n# Optionally save the entire model\nfull_model_path = NAME+'_full_model.pth'\ntorch.save(model, full_model_path)\nprint(f\"Full model saved to: {full_model_path}\")\n\n# Save training history\nhistory_path = NAME+'_training_history.npz'\nnp.savez(history_path,\n         train_loss=train_loss_history,\n         train_acc=train_acc_history,\n         val_loss=val_loss_history,\n         val_acc=val_acc_history)\nprint(f\"Training history saved to: {history_path}\")\n\n# Save model metadata\nmetadata_path = NAME+'_metamodel.txt'\nwith open(metadata_path, 'w') as f:\n    f.write(\"=\"*60 + \"\\n\")\n    f.write(\"MODEL METADATA\\n\")\n    f.write(\"=\"*60 + \"\\n\\n\")\n    \n    f.write(\"MODEL NAME:\\n\")\n    f.write(\"  3-Layer CNN for Colored MNIST Classification\\n\\n\")\n    \n    f.write(\"MODEL ARCHITECTURE:\\n\")\n    f.write(str(model) + \"\\n\\n\")\n    \n    f.write(\"MODEL STRUCTURE:\\n\")\n    f.write(\"  HYBRID FILTERING APPROACH:\\n\")\n    f.write(\"  - Conv1: Gaussian filter initialization (sigma=1.0)\\n\")\n    f.write(\"  - Conv2 & Conv3: Standard PyTorch initialization\\n\\n\")\n    f.write(\"  Layer 1: Conv2d(3 -> 32, kernel=3x3, padding=1, Gaussian init) + ReLU + MaxPool2d(2x2)\\n\")\n    f.write(\"  Layer 2: Conv2d(32 -> 64, kernel=3x3, padding=1) + ReLU + MaxPool2d(2x2)\\n\")\n    f.write(\"  Layer 3: Conv2d(64 -> 64, kernel=3x3, padding=1) + ReLU + MaxPool2d(2x2)\\n\")\n    f.write(\"  FC Layer 1: Linear(576 -> 128) + ReLU + Dropout(0.5)\\n\")\n    f.write(\"  FC Layer 2: Linear(128 -> 10)\\n\\n\")\n    \n    f.write(\"TOTAL PARAMETERS:\\n\")\n    f.write(f\"  {sum(p.numel() for p in model.parameters()):,}\\n\\n\")\n    \n    f.write(\"HYPERPARAMETERS:\\n\")\n    f.write(f\"  Epochs: {EPOCHS}\\n\")\n    f.write(f\"  Batch Size: {BATCH_SIZE}\\n\")\n    f.write(f\"  Learning Rate: {LEARNING_RATE}\\n\")\n    f.write(f\"  Optimizer: {OPTIMIZER}\\n\")\n    f.write(f\"  Loss Function: CrossEntropyLoss\\n\")\n    f.write(f\"  Dropout Rate: {DROPOUT_RATE}\\n\")\n    f.write(f\"  Validation Split: {VALIDATION_SPLIT} ({VALIDATION_SPLIT*100:.0f}%)\\n\\n\")\n    \n    f.write(\"INPUT SPECIFICATIONS:\\n\")\n    f.write(f\"  Input Shape: {X_train_tensor.shape[1:]}\\n\")\n    f.write(f\"  Number of Classes: {NUM_CLASSES}\\n\")\n    f.write(f\"  Image Size: 28x28x3 (RGB)\\n\\n\")\n    \n    f.write(\"TRAINING DATA:\\n\")\n    f.write(f\"  Training Samples: {len(X_train_tensor)}\\n\")\n    f.write(f\"  Validation Samples: {len(X_val_tensor)}\\n\\n\")\n    \n    f.write(\"TEST DATASETS:\\n\")\n    for name in test_tensors:\n        f.write(f\"  {name}: {len(test_tensors[name]['X'])} samples\\n\")\n    f.write(\"\\n\")\n    \n    f.write(\"FINAL TRAINING METRICS:\\n\")\n    f.write(f\"  Final Training Loss: {train_loss_history[-1]:.4f}\\n\")\n    f.write(f\"  Final Training Accuracy: {train_acc_history[-1]*100:.2f}%\\n\")\n    f.write(f\"  Final Validation Loss: {val_loss_history[-1]:.4f}\\n\")\n    f.write(f\"  Final Validation Accuracy: {val_acc_history[-1]*100:.2f}%\\n\\n\")\n    \n    f.write(\"SAVED FILES:\\n\")\n    f.write(f\"  Model State Dict: {model_save_path}\\n\")\n    f.write(f\"  Full Model: {full_model_path}\\n\")\n    f.write(f\"  Training History: {history_path}\\n\")\n    f.write(f\"  Metadata: {metadata_path}\\n\\n\")\n    \n    f.write(\"=\"*60 + \"\\n\")\n\nprint(f\"Model metadata saved to: {metadata_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:43:06.227624Z","iopub.execute_input":"2026-02-07T16:43:06.227951Z","iopub.status.idle":"2026-02-07T16:43:06.245211Z","shell.execute_reply.started":"2026-02-07T16:43:06.22793Z","shell.execute_reply":"2026-02-07T16:43:06.244494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n# Evaluate on all test datasets\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ntest_results = {}\nall_predictions = {}\nall_targets = {}\n\nfor name, loader in test_loaders.items():\n    model.eval()\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    predictions = []\n    targets = []\n    \n    with torch.no_grad():\n        for data, target in loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n            \n            test_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            test_total += target.size(0)\n            test_correct += (predicted == target).sum().item()\n            \n            predictions.extend(predicted.cpu().numpy())\n            targets.extend(target.cpu().numpy())\n    \n    avg_test_loss = test_loss / len(loader)\n    test_accuracy = test_correct / test_total\n    \n    test_results[name] = {\n        'loss': avg_test_loss,\n        'accuracy': test_accuracy\n    }\n    all_predictions[name] = np.array(predictions)\n    all_targets[name] = np.array(targets)\n    \n    print(f\"\\n{name} Results:\")\n    print(f\"  Loss: {avg_test_loss:.4f}\")\n    print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n\n# Visualization 1: Accuracy comparison across all datasets\nplt.figure(figsize=(12, 6))\ndataset_names = list(test_results.keys())\naccuracies = [test_results[name]['accuracy'] * 100 for name in dataset_names]\n\nplt.bar(dataset_names, accuracies, color='steelblue', edgecolor='black')\nplt.xlabel('Dataset', fontsize=12, fontweight='bold')\nplt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\nplt.title('Model Accuracy on All Test Datasets', fontsize=14, fontweight='bold')\nplt.ylim(0, 100)\nplt.grid(axis='y', alpha=0.3)\n\nfor i, (name, acc) in enumerate(zip(dataset_names, accuracies)):\n    plt.text(i, acc + 1, f'{acc:.2f}%', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('accuracy_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Visualization 2: Training and Validation Accuracy vs Epoch\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nepochs_range = range(1, len(train_acc_history) + 1)\nplt.plot(epochs_range, [acc * 100 for acc in train_acc_history], 'b-o', label='Training Accuracy', linewidth=2)\nplt.plot(epochs_range, [acc * 100 for acc in val_acc_history], 'r-s', label='Validation Accuracy', linewidth=2)\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\nplt.title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, train_loss_history, 'b-o', label='Training Loss', linewidth=2)\nplt.plot(epochs_range, val_loss_history, 'r-s', label='Validation Loss', linewidth=2)\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Loss', fontsize=12, fontweight='bold')\nplt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Visualization 3: Confusion Matrix for each dataset\nnum_datasets = len(test_results)\ncols = min(3, num_datasets)\nrows = (num_datasets + cols - 1) // cols\n\nfig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows))\nif num_datasets == 1:\n    axes = [axes]\nelse:\n    axes = axes.flatten() if num_datasets > 1 else [axes]\n\nfor idx, name in enumerate(dataset_names):\n    cm = confusion_matrix(all_targets[name], all_predictions[name])\n    \n    ax = axes[idx] if num_datasets > 1 else axes[0]\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n                cbar_kws={'label': 'Count'}, square=True)\n    ax.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n    ax.set_ylabel('True Label', fontsize=11, fontweight='bold')\n    ax.set_title(f'Confusion Matrix - {name}\\nAccuracy: {test_results[name][\"accuracy\"]*100:.2f}%', \n                 fontsize=12, fontweight='bold')\n\n# Hide extra subplots\nfor idx in range(num_datasets, len(axes)):\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"EVALUATION SUMMARY\")\nprint(\"=\"*50)\nfor name in dataset_names:\n    print(f\"{name:15s}: {test_results[name]['accuracy']*100:6.2f}%\")\nprint(\"=\"*50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T16:43:06.246158Z","iopub.execute_input":"2026-02-07T16:43:06.246387Z","iopub.status.idle":"2026-02-07T16:43:16.228569Z","shell.execute_reply.started":"2026-02-07T16:43:06.246366Z","shell.execute_reply":"2026-02-07T16:43:16.227949Z"}},"outputs":[],"execution_count":null}]}