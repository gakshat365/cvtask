{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-07T16:42:16.627518Z",
     "iopub.status.busy": "2026-02-07T16:42:16.626678Z",
     "iopub.status.idle": "2026-02-07T16:42:16.63368Z",
     "shell.execute_reply": "2026-02-07T16:42:16.632804Z",
     "shell.execute_reply.started": "2026-02-07T16:42:16.627487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3-Layer CNN for Colored MNIST Classification\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# All hyperparameters and settings in one place\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/kaggle/input/cmnistneo1/train_data_rg95z.npz'\n",
    "TEST_DATASETS = {\n",
    "    'rg95z': '/kaggle/input/cmnistneo1/test_data_rg95z.npz',\n",
    "    'gr95z': '/kaggle/input/cmnistneo1/test_data_gr95z.npz',\n",
    "    'gr95e': '/kaggle/input/cmnistneo1/test_data_gr95e.npz',\n",
    "    'gr95m': '/kaggle/input/cmnistneo1/test_data_gr95m.npz',\n",
    "    'gr95h': '/kaggle/input/cmnistneo1/test_data_gr95h.npz',\n",
    "    'gr95vh': '/kaggle/input/cmnistneo1/test_data_gr95vh.npz',\n",
    "    'bw95z': '/kaggle/input/cmnistneo1/test_data_bw95z.npz',\n",
    "    'bw100z': '/kaggle/input/cmnistneo1/test_data_bw100z.npz'\n",
    "}\n",
    "\n",
    "# Model hyperparameters\n",
    "NUM_CLASSES = 10\n",
    "CONV1_CHANNELS = 32\n",
    "CONV2_CHANNELS = 64\n",
    "CONV3_CHANNELS = 64\n",
    "FC1_UNITS = 128\n",
    "DROPOUT_RATE = 0.0\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.02\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "# Optimizer settings\n",
    "OPTIMIZER = 'Adam'  # Options: 'Adam', 'SGD', 'RMSprop'\n",
    "\n",
    "NAME = 'task1approach4gf1'\n",
    "\n",
    "# ===== END CONFIGURATION =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T16:42:16.637738Z",
     "iopub.status.busy": "2026-02-07T16:42:16.637066Z",
     "iopub.status.idle": "2026-02-07T16:42:18.525953Z",
     "shell.execute_reply": "2026-02-07T16:42:18.525181Z",
     "shell.execute_reply.started": "2026-02-07T16:42:16.637711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load training data\n",
    "train_data = np.load(TRAIN_DATA_PATH)\n",
    "X_train = train_data['images']\n",
    "y_train = train_data['labels']\n",
    "\n",
    "# Load test datasets from dictionary\n",
    "test_data_dict = {}\n",
    "for name, path in TEST_DATASETS.items():\n",
    "    data = np.load(path)\n",
    "    test_data_dict[name] = {\n",
    "        'images': data['images'],\n",
    "        'labels': data['labels']\n",
    "    }\n",
    "    print(f\"{name} shape: {data['images'].shape}, labels: {data['labels'].shape}\")\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "\n",
    "# Normalize test datasets\n",
    "for name in test_data_dict:\n",
    "    test_data_dict[name]['images'] = test_data_dict[name]['images'].astype('float32') / 255.0\n",
    "\n",
    "# Create validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors and change format from (N, H, W, C) to (N, C, H, W)\n",
    "X_train_tensor = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "X_val_tensor = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "# Convert test datasets to tensors\n",
    "test_tensors = {}\n",
    "for name in test_data_dict:\n",
    "    X_test = test_data_dict[name]['images']\n",
    "    y_test = test_data_dict[name]['labels']\n",
    "    \n",
    "    test_tensors[name] = {\n",
    "        'X': torch.FloatTensor(X_test).permute(0, 3, 1, 2),\n",
    "        'y': torch.LongTensor(y_test)\n",
    "    }\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Create test loaders\n",
    "test_loaders = {}\n",
    "for name in test_tensors:\n",
    "    dataset = TensorDataset(test_tensors[name]['X'], test_tensors[name]['y'])\n",
    "    test_loaders[name] = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training set: {X_train_tensor.shape}\")\n",
    "print(f\"Validation set: {X_val_tensor.shape}\")\n",
    "for name in test_tensors:\n",
    "    print(f\"Test set {name}: {test_tensors[name]['X'].shape}\")\n",
    "print(f\"Input shape: {X_train_tensor.shape[1:]}\")\n",
    "\n",
    "# Helper function to create Gaussian kernel\n",
    "def create_gaussian_kernel(kernel_size=3, sigma=11.0):\n",
    "    \"\"\"\n",
    "    Create a 2D Gaussian kernel for filtering.\n",
    "    \n",
    "    Args:\n",
    "        kernel_size: Size of the kernel (default: 3)\n",
    "        sigma: Standard deviation of the Gaussian distribution (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized Gaussian kernel of shape (kernel_size, kernel_size)\n",
    "    \"\"\"\n",
    "    # Create a coordinate grid\n",
    "    ax = np.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1.)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    \n",
    "    # Calculate Gaussian kernel\n",
    "    kernel = np.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n",
    "    \n",
    "    # Normalize the kernel\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    \n",
    "    return kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T16:42:18.52785Z",
     "iopub.status.busy": "2026-02-07T16:42:18.52759Z",
     "iopub.status.idle": "2026-02-07T16:42:18.537346Z",
     "shell.execute_reply": "2026-02-07T16:42:18.536561Z",
     "shell.execute_reply.started": "2026-02-07T16:42:18.527829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build the 3-Layer CNN Model\n",
    "class CNN3Layer(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, use_gaussian_conv1=True):\n",
    "        super(CNN3Layer, self).__init__()\n",
    "        \n",
    "        self.use_gaussian_conv1 = use_gaussian_conv1\n",
    "\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(3, CONV1_CHANNELS, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(CONV1_CHANNELS, CONV2_CHANNELS, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Third Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(CONV2_CHANNELS, CONV3_CHANNELS, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(CONV3_CHANNELS * 3 * 3, FC1_UNITS)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.fc2 = nn.Linear(FC1_UNITS, num_classes)\n",
    "        \n",
    "        # Initialize first conv layer with Gaussian filter if enabled\n",
    "        if self.use_gaussian_conv1:\n",
    "            self._initialize_gaussian_conv1()\n",
    "    \n",
    "    def _initialize_gaussian_conv1(self):\n",
    "        \"\"\"\n",
    "        Initialize the first convolutional layer with Gaussian filter weights.\n",
    "        This creates a hybrid approach where conv1 uses Gaussian filtering.\n",
    "        \"\"\"\n",
    "        # Create Gaussian kernel\n",
    "        gaussian_kernel = create_gaussian_kernel(kernel_size=3, sigma=51.0)\n",
    "        \n",
    "        # Get the weight tensor of conv1\n",
    "        # Shape: (out_channels, in_channels, kernel_height, kernel_width)\n",
    "        # For conv1: (32, 3, 3, 3)\n",
    "        with torch.no_grad():\n",
    "            # Initialize all weights with the Gaussian kernel\n",
    "            for out_ch in range(self.conv1.out_channels):\n",
    "                for in_ch in range(self.conv1.in_channels):\n",
    "                    # Apply Gaussian kernel to each channel combination\n",
    "                    self.conv1.weight[out_ch, in_ch] = torch.FloatTensor(gaussian_kernel)\n",
    "            \n",
    "            # Initialize biases to zero for the first layer\n",
    "            if self.conv1.bias is not None:\n",
    "                self.conv1.bias.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "\n",
    "        # Second conv block\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Third conv block\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T16:42:18.538531Z",
     "iopub.status.busy": "2026-02-07T16:42:18.538258Z",
     "iopub.status.idle": "2026-02-07T16:43:06.226004Z",
     "shell.execute_reply": "2026-02-07T16:43:06.225369Z",
     "shell.execute_reply.started": "2026-02-07T16:42:18.53851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model and move to device\n",
    "model = CNN3Layer(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# Display model architecture\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYBRID FILTERING APPROACH\")\n",
    "print(\"=\"*60)\n",
    "print(\"Conv Layer 1: Gaussian filter initialization (sigma=1.0)\")\n",
    "print(\"Conv Layer 2: Standard PyTorch initialization (Kaiming uniform)\")\n",
    "print(\"Conv Layer 3: Standard PyTorch initialization (Kaiming uniform)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if OPTIMIZER == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "elif OPTIMIZER == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "elif OPTIMIZER == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown optimizer: {OPTIMIZER}\")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(f\"Optimizer: {OPTIMIZER}\")\n",
    "print(f\"Loss function: CrossEntropyLoss\")\n",
    "\n",
    "# Lists to store history\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_total += target.size(0)\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Calculate average training metrics\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_total += target.size(0)\n",
    "            val_correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Calculate average validation metrics\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    # Store history\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    train_acc_history.append(train_accuracy)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    val_acc_history.append(val_accuracy)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}] - '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f} - '\n",
    "          f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T16:43:06.227951Z",
     "iopub.status.busy": "2026-02-07T16:43:06.227624Z",
     "iopub.status.idle": "2026-02-07T16:43:06.245211Z",
     "shell.execute_reply": "2026-02-07T16:43:06.244494Z",
     "shell.execute_reply.started": "2026-02-07T16:43:06.22793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "model_save_path = NAME+'_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"\\nModel state dict saved to: {model_save_path}\")\n",
    "\n",
    "# Optionally save the entire model\n",
    "full_model_path = NAME+'_full_model.pth'\n",
    "torch.save(model, full_model_path)\n",
    "print(f\"Full model saved to: {full_model_path}\")\n",
    "\n",
    "# Save training history\n",
    "history_path = NAME+'_training_history.npz'\n",
    "np.savez(history_path,\n",
    "         train_loss=train_loss_history,\n",
    "         train_acc=train_acc_history,\n",
    "         val_loss=val_loss_history,\n",
    "         val_acc=val_acc_history)\n",
    "print(f\"Training history saved to: {history_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata_path = NAME+'_metamodel.txt'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"MODEL METADATA\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"MODEL NAME:\\n\")\n",
    "    f.write(\"  3-Layer CNN for Colored MNIST Classification\\n\\n\")\n",
    "    \n",
    "    f.write(\"MODEL ARCHITECTURE:\\n\")\n",
    "    f.write(str(model) + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"MODEL STRUCTURE:\\n\")\n",
    "    f.write(\"  HYBRID FILTERING APPROACH:\\n\")\n",
    "    f.write(\"  - Conv1: Gaussian filter initialization (sigma=1.0)\\n\")\n",
    "    f.write(\"  - Conv2 & Conv3: Standard PyTorch initialization\\n\\n\")\n",
    "    f.write(\"  Layer 1: Conv2d(3 -> 32, kernel=3x3, padding=1, Gaussian init) + ReLU + MaxPool2d(2x2)\\n\")\n",
    "    f.write(\"  Layer 2: Conv2d(32 -> 64, kernel=3x3, padding=1) + ReLU + MaxPool2d(2x2)\\n\")\n",
    "    f.write(\"  Layer 3: Conv2d(64 -> 64, kernel=3x3, padding=1) + ReLU + MaxPool2d(2x2)\\n\")\n",
    "    f.write(\"  FC Layer 1: Linear(576 -> 128) + ReLU + Dropout(0.5)\\n\")\n",
    "    f.write(\"  FC Layer 2: Linear(128 -> 10)\\n\\n\")\n",
    "    \n",
    "    f.write(\"TOTAL PARAMETERS:\\n\")\n",
    "    f.write(f\"  {sum(p.numel() for p in model.parameters()):,}\\n\\n\")\n",
    "    \n",
    "    f.write(\"HYPERPARAMETERS:\\n\")\n",
    "    f.write(f\"  Epochs: {EPOCHS}\\n\")\n",
    "    f.write(f\"  Batch Size: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"  Learning Rate: {LEARNING_RATE}\\n\")\n",
    "    f.write(f\"  Optimizer: {OPTIMIZER}\\n\")\n",
    "    f.write(f\"  Loss Function: CrossEntropyLoss\\n\")\n",
    "    f.write(f\"  Dropout Rate: {DROPOUT_RATE}\\n\")\n",
    "    f.write(f\"  Validation Split: {VALIDATION_SPLIT} ({VALIDATION_SPLIT*100:.0f}%)\\n\\n\")\n",
    "    \n",
    "    f.write(\"INPUT SPECIFICATIONS:\\n\")\n",
    "    f.write(f\"  Input Shape: {X_train_tensor.shape[1:]}\\n\")\n",
    "    f.write(f\"  Number of Classes: {NUM_CLASSES}\\n\")\n",
    "    f.write(f\"  Image Size: 28x28x3 (RGB)\\n\\n\")\n",
    "    \n",
    "    f.write(\"TRAINING DATA:\\n\")\n",
    "    f.write(f\"  Training Samples: {len(X_train_tensor)}\\n\")\n",
    "    f.write(f\"  Validation Samples: {len(X_val_tensor)}\\n\\n\")\n",
    "    \n",
    "    f.write(\"TEST DATASETS:\\n\")\n",
    "    for name in test_tensors:\n",
    "        f.write(f\"  {name}: {len(test_tensors[name]['X'])} samples\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"FINAL TRAINING METRICS:\\n\")\n",
    "    f.write(f\"  Final Training Loss: {train_loss_history[-1]:.4f}\\n\")\n",
    "    f.write(f\"  Final Training Accuracy: {train_acc_history[-1]*100:.2f}%\\n\")\n",
    "    f.write(f\"  Final Validation Loss: {val_loss_history[-1]:.4f}\\n\")\n",
    "    f.write(f\"  Final Validation Accuracy: {val_acc_history[-1]*100:.2f}%\\n\\n\")\n",
    "    \n",
    "    f.write(\"SAVED FILES:\\n\")\n",
    "    f.write(f\"  Model State Dict: {model_save_path}\\n\")\n",
    "    f.write(f\"  Full Model: {full_model_path}\\n\")\n",
    "    f.write(f\"  Training History: {history_path}\\n\")\n",
    "    f.write(f\"  Metadata: {metadata_path}\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"Model metadata saved to: {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T16:43:06.246387Z",
     "iopub.status.busy": "2026-02-07T16:43:06.246158Z",
     "iopub.status.idle": "2026-02-07T16:43:16.228569Z",
     "shell.execute_reply": "2026-02-07T16:43:16.227949Z",
     "shell.execute_reply.started": "2026-02-07T16:43:06.246366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Evaluate on all test datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "test_results = {}\n",
    "all_predictions = {}\n",
    "all_targets = {}\n",
    "\n",
    "for name, loader in test_loaders.items():\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            test_total += target.size(0)\n",
    "            test_correct += (predicted == target).sum().item()\n",
    "            \n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(loader)\n",
    "    test_accuracy = test_correct / test_total\n",
    "    \n",
    "    test_results[name] = {\n",
    "        'loss': avg_test_loss,\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "    all_predictions[name] = np.array(predictions)\n",
    "    all_targets[name] = np.array(targets)\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"  Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Visualization 1: Accuracy comparison across all datasets\n",
    "plt.figure(figsize=(12, 6))\n",
    "dataset_names = list(test_results.keys())\n",
    "accuracies = [test_results[name]['accuracy'] * 100 for name in dataset_names]\n",
    "\n",
    "plt.bar(dataset_names, accuracies, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('Model Accuracy on All Test Datasets', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (name, acc) in enumerate(zip(dataset_names, accuracies)):\n",
    "    plt.text(i, acc + 1, f'{acc:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Visualization 2: Training and Validation Accuracy vs Epoch\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "epochs_range = range(1, len(train_acc_history) + 1)\n",
    "plt.plot(epochs_range, [acc * 100 for acc in train_acc_history], 'b-o', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(epochs_range, [acc * 100 for acc in val_acc_history], 'r-s', label='Validation Accuracy', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_loss_history, 'b-o', label='Training Loss', linewidth=2)\n",
    "plt.plot(epochs_range, val_loss_history, 'r-s', label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "plt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Visualization 3: Confusion Matrix for each dataset\n",
    "num_datasets = len(test_results)\n",
    "cols = min(3, num_datasets)\n",
    "rows = (num_datasets + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows))\n",
    "if num_datasets == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten() if num_datasets > 1 else [axes]\n",
    "\n",
    "for idx, name in enumerate(dataset_names):\n",
    "    cm = confusion_matrix(all_targets[name], all_predictions[name])\n",
    "    \n",
    "    ax = axes[idx] if num_datasets > 1 else axes[0]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "                cbar_kws={'label': 'Count'}, square=True)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Confusion Matrix - {name}\\nAccuracy: {test_results[name][\"accuracy\"]*100:.2f}%', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(num_datasets, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for name in dataset_names:\n",
    "    print(f\"{name:15s}: {test_results[name]['accuracy']*100:6.2f}%\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9434511,
     "sourceId": 14760510,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
