{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14760510,"sourceType":"datasetVersion","datasetId":9434511}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":105.172537,"end_time":"2026-02-09T17:55:44.89031","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-02-09T17:53:59.717773","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 3-Layer CNN for Colored MNIST Classification\n# Converted from task1_v2.ipynb\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\n\n# ===== CONFIGURATION =====\n# All hyperparameters and settings in one place\n\n# Data paths\nTRAIN_DATA_PATH = '/kaggle/input/datasets/akshatatkaggle/cmnistneo1/train_data_rg95z.npz'\nTEST_DATASETS = {\n    'rg95z': '/kaggle/input/datasets/akshatatkaggle/cmnistneo1/test_data_rg95z.npz',\n    'gr95z': '/kaggle/input/datasets/akshatatkaggle/cmnistneo1/test_data_gr95z.npz',\n    'gr95e': '/kaggle/input/datasets/akshatatkaggle/cmnistneo1/test_data_gr95e.npz',\n    'gr95m': '/kaggle/input/datasets/akshatatkaggle/cmnistneo1/test_data_gr95m.npz',\n    'gr95h': '/kaggle/input/datasets/akshatatkaggle/cmnistneo1/test_data_gr95h.npz',\n    'gr95vh': '/kaggle/input/datasets/akshatatkaggle/cmnistneo1/test_data_gr95vh.npz',\n    'bw95z': '/kaggle/input/datasets/akshatatkaggle/cmnistneo1/test_data_bw95z.npz',\n    'bw100z': '/kaggle/input/datasets/akshatatkaggle/cmnistneo1/test_data_bw100z.npz'\n}\n\n# Model hyperparameters\nNUM_CLASSES = 10\nCONV1_CHANNELS = 32\nCONV2_CHANNELS = 64\nCONV3_CHANNELS = 64\nFC1_UNITS = 128\nDROPOUT_RATE = 0.0\n\n# Training hyperparameters\nEPOCHS = 50\nBATCH_SIZE = 256\nLEARNING_RATE = 0.045\nVALIDATION_SPLIT = 0.1\n\n\n# Optimizer settings\nOPTIMIZER = 'Adam'  # Options: 'Adam', 'SGD', 'RMSprop'\n\n# Model Name\nNAME = 'task1app1v11'\n\n# ===== END CONFIGURATION =====","metadata":{"_uuid":"136e7ad1-fc65-4f99-8c79-93f96c1cc968","_cell_guid":"0182eb4b-d10b-4b15-8fb4-d1255f773757","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-09T23:59:31.449028Z","iopub.execute_input":"2026-02-09T23:59:31.449373Z","iopub.status.idle":"2026-02-09T23:59:31.455539Z","shell.execute_reply.started":"2026-02-09T23:59:31.44934Z","shell.execute_reply":"2026-02-09T23:59:31.454843Z"},"papermill":{"duration":6.142138,"end_time":"2026-02-09T17:54:08.449019","exception":false,"start_time":"2026-02-09T17:54:02.306881","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load training data\ntrain_data = np.load(TRAIN_DATA_PATH)\nX_train = train_data['images']\ny_train = train_data['labels']\n\n# Load test datasets from dictionary\ntest_data_dict = {}\nfor name, path in TEST_DATASETS.items():\n    data = np.load(path)\n    test_data_dict[name] = {\n        'images': data['images'],\n        'labels': data['labels']\n    }\n    print(f\"{name} shape: {data['images'].shape}, labels: {data['labels'].shape}\")\n\nprint(f\"\\nTraining data shape: {X_train.shape}\")\nprint(f\"Training labels shape: {y_train.shape}\")\nprint(f\"Number of classes: {len(np.unique(y_train))}\")\n\n# Normalize pixel values to [0, 1]\nX_train = X_train.astype('float32') / 255.0\n\n# Normalize test datasets\nfor name in test_data_dict:\n    test_data_dict[name]['images'] = test_data_dict[name]['images'].astype('float32') / 255.0\n\n# Create validation split\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=VALIDATION_SPLIT, random_state=42\n)\n\n# Convert to PyTorch tensors and change format from (N, H, W, C) to (N, C, H, W)\nX_train_tensor = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\nX_val_tensor = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n\ny_train_tensor = torch.LongTensor(y_train)\ny_val_tensor = torch.LongTensor(y_val)\n\n# Convert test datasets to tensors\ntest_tensors = {}\nfor name in test_data_dict:\n    X_test = test_data_dict[name]['images']\n    y_test = test_data_dict[name]['labels']\n    \n    test_tensors[name] = {\n        'X': torch.FloatTensor(X_test).permute(0, 3, 1, 2),\n        'y': torch.LongTensor(y_test)\n    }\n\n# Create DataLoaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Create test loaders\ntest_loaders = {}\nfor name in test_tensors:\n    dataset = TensorDataset(test_tensors[name]['X'], test_tensors[name]['y'])\n    test_loaders[name] = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Training set: {X_train_tensor.shape}\")\nprint(f\"Validation set: {X_val_tensor.shape}\")\nfor name in test_tensors:\n    print(f\"Test set {name}: {test_tensors[name]['X'].shape}\")\nprint(f\"Input shape: {X_train_tensor.shape[1:]}\")","metadata":{"_uuid":"b82865b7-d6d5-43ee-8a42-723b32e967fd","_cell_guid":"1872510f-9fca-4232-981b-9091d705a684","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-09T23:59:31.456928Z","iopub.execute_input":"2026-02-09T23:59:31.457153Z","iopub.status.idle":"2026-02-09T23:59:32.998688Z","shell.execute_reply.started":"2026-02-09T23:59:31.457133Z","shell.execute_reply":"2026-02-09T23:59:32.997905Z"},"papermill":{"duration":3.697541,"end_time":"2026-02-09T17:54:12.148577","exception":false,"start_time":"2026-02-09T17:54:08.451036","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Build the 3-Layer CNN Model\nclass CNN3Layer(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super(CNN3Layer, self).__init__()\n\n        # First Convolutional Layer\n        self.conv1 = nn.Conv2d(3, CONV1_CHANNELS, kernel_size=1, padding=0)\n        self.pool1 = nn.MaxPool2d(2, 2)\n\n        # Second Convolutional Layer\n        self.conv2 = nn.Conv2d(CONV1_CHANNELS, CONV2_CHANNELS, kernel_size=1, padding=0)\n        self.pool2 = nn.MaxPool2d(2, 2)\n\n        # Third Convolutional Layer\n        self.conv3 = nn.Conv2d(CONV2_CHANNELS, CONV3_CHANNELS, kernel_size=1, padding=0)\n        self.pool3 = nn.MaxPool2d(2, 2)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(CONV3_CHANNELS * 3 * 3, FC1_UNITS)\n        self.dropout = nn.Dropout(DROPOUT_RATE)\n        self.fc2 = nn.Linear(FC1_UNITS, num_classes)\n\n    def forward(self, x):\n        # First conv block\n        x = self.pool1(F.relu(self.conv1(x)))\n\n        # Second conv block\n        x = self.pool2(F.relu(self.conv2(x)))\n\n        # Third conv block\n        x = self.pool3(F.relu(self.conv3(x)))\n\n        # Flatten\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\n# Initialize model and move to device\nmodel = CNN3Layer(num_classes=NUM_CLASSES).to(device)\n\n# Display model architecture\nprint(model)\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")","metadata":{"_uuid":"5002f4fc-7fbd-4778-8846-63bc223d9c0f","_cell_guid":"693c4489-cd06-4056-b2b5-24c55ccf4bcd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-09T23:59:32.999847Z","iopub.execute_input":"2026-02-09T23:59:33.000118Z","iopub.status.idle":"2026-02-09T23:59:33.011005Z","shell.execute_reply.started":"2026-02-09T23:59:33.000097Z","shell.execute_reply":"2026-02-09T23:59:33.010368Z"},"papermill":{"duration":0.317195,"end_time":"2026-02-09T17:54:12.468599","exception":false,"start_time":"2026-02-09T17:54:12.151404","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\nif OPTIMIZER == 'Adam':\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nelif OPTIMIZER == 'SGD':\n    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\nelif OPTIMIZER == 'RMSprop':\n    optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\nelse:\n    raise ValueError(f\"Unknown optimizer: {OPTIMIZER}\")\n\nprint(\"Model compiled successfully!\")\nprint(f\"Optimizer: {OPTIMIZER}\")\nprint(f\"Loss function: CrossEntropyLoss\")\n\n# Lists to store history\ntrain_loss_history = []\ntrain_acc_history = []\nval_loss_history = []\nval_acc_history = []","metadata":{"_uuid":"23cf17d1-a26d-4589-9a49-4b8e29ce1933","_cell_guid":"31bf1ea4-3407-4b19-bfde-14e915b6fd54","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-09T23:59:33.01257Z","iopub.execute_input":"2026-02-09T23:59:33.012833Z","iopub.status.idle":"2026-02-09T23:59:33.02082Z","shell.execute_reply.started":"2026-02-09T23:59:33.012814Z","shell.execute_reply":"2026-02-09T23:59:33.020211Z"},"papermill":{"duration":4.858123,"end_time":"2026-02-09T17:54:17.32896","exception":false,"start_time":"2026-02-09T17:54:12.470837","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Training loop\nfor epoch in range(EPOCHS):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        train_loss += loss.item()\n        _, predicted = torch.max(output.data, 1)\n        train_total += target.size(0)\n        train_correct += (predicted == target).sum().item()\n\n    # Calculate average training metrics\n    avg_train_loss = train_loss / len(train_loader)\n    train_accuracy = train_correct / train_total\n\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n\n            val_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            val_total += target.size(0)\n            val_correct += (predicted == target).sum().item()\n\n    # Calculate average validation metrics\n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = val_correct / val_total\n\n    # Store history\n    train_loss_history.append(avg_train_loss)\n    train_acc_history.append(train_accuracy)\n    val_loss_history.append(avg_val_loss)\n    val_acc_history.append(val_accuracy)\n\n    # Print epoch results\n    print(f'Epoch [{epoch+1}/{EPOCHS}] - '\n          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f} - '\n          f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n\nprint(\"\\nTraining completed!\")","metadata":{"_uuid":"d553977c-4058-40ae-b5a9-4bc5dee03c2d","_cell_guid":"5121aee8-6bba-4221-be46-5fc50d9cd9d5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-09T23:59:33.021706Z","iopub.execute_input":"2026-02-09T23:59:33.022078Z","iopub.status.idle":"2026-02-10T00:00:56.352833Z","shell.execute_reply.started":"2026-02-09T23:59:33.022056Z","shell.execute_reply":"2026-02-10T00:00:56.352177Z"},"papermill":{"duration":74.55006,"end_time":"2026-02-09T17:55:31.881386","exception":false,"start_time":"2026-02-09T17:54:17.331326","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Save the model\nmodel_save_path = NAME+'_model.pth'\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"\\nModel state dict saved to: {model_save_path}\")\n\n# Optionally save the entire model\nfull_model_path = NAME+'_full_model.pth'\ntorch.save(model, full_model_path)\nprint(f\"Full model saved to: {full_model_path}\")\n\n# Save training history\nhistory_path = NAME+'_training_history.npz'\nnp.savez(history_path,\n         train_loss=train_loss_history,\n         train_acc=train_acc_history,\n         val_loss=val_loss_history,\n         val_acc=val_acc_history)\nprint(f\"Training history saved to: {history_path}\")\n\n# Save model metadata\nmetadata_path = NAME+'_metamodel.txt'\nwith open(metadata_path, 'w') as f:\n    f.write(\"=\"*60 + \"\\n\")\n    f.write(\"MODEL METADATA\\n\")\n    f.write(\"=\"*60 + \"\\n\\n\")\n    \n    f.write(\"MODEL NAME:\\n\")\n    f.write(\"  3-Layer CNN for Colored MNIST Classification\\n\\n\")\n    \n    f.write(\"MODEL ARCHITECTURE:\\n\")\n    f.write(str(model) + \"\\n\\n\")\n    \n    f.write(\"MODEL STRUCTURE:\\n\")\n    f.write(\"  Layer 1: Conv2d(3 -> 32, kernel=3x3, padding=1) + ReLU + MaxPool2d(2x2)\\n\")\n    f.write(\"  Layer 2: Conv2d(32 -> 64, kernel=3x3, padding=1) + ReLU + MaxPool2d(2x2)\\n\")\n    f.write(\"  Layer 3: Conv2d(64 -> 64, kernel=3x3, padding=1) + ReLU + MaxPool2d(2x2)\\n\")\n    f.write(\"  FC Layer 1: Linear(576 -> 128) + ReLU + Dropout(0.5)\\n\")\n    f.write(\"  FC Layer 2: Linear(128 -> 10)\\n\\n\")\n    \n    f.write(\"TOTAL PARAMETERS:\\n\")\n    f.write(f\"  {sum(p.numel() for p in model.parameters()):,}\\n\\n\")\n    \n    f.write(\"HYPERPARAMETERS:\\n\")\n    f.write(f\"  Epochs: {EPOCHS}\\n\")\n    f.write(f\"  Batch Size: {BATCH_SIZE}\\n\")\n    f.write(f\"  Learning Rate: {LEARNING_RATE}\\n\")\n    f.write(f\"  Optimizer: {OPTIMIZER}\\n\")\n    f.write(f\"  Loss Function: CrossEntropyLoss\\n\")\n    f.write(f\"  Dropout Rate: {DROPOUT_RATE}\\n\")\n    f.write(f\"  Validation Split: {VALIDATION_SPLIT} ({VALIDATION_SPLIT*100:.0f}%)\\n\\n\")\n    \n    f.write(\"INPUT SPECIFICATIONS:\\n\")\n    f.write(f\"  Input Shape: {X_train_tensor.shape[1:]}\\n\")\n    f.write(f\"  Number of Classes: {NUM_CLASSES}\\n\")\n    f.write(f\"  Image Size: 28x28x3 (RGB)\\n\\n\")\n    \n    f.write(\"TRAINING DATA:\\n\")\n    f.write(f\"  Training Samples: {len(X_train_tensor)}\\n\")\n    f.write(f\"  Validation Samples: {len(X_val_tensor)}\\n\\n\")\n    \n    f.write(\"TEST DATASETS:\\n\")\n    for name in test_tensors:\n        f.write(f\"  {name}: {len(test_tensors[name]['X'])} samples\\n\")\n    f.write(\"\\n\")\n    \n    f.write(\"FINAL TRAINING METRICS:\\n\")\n    f.write(f\"  Final Training Loss: {train_loss_history[-1]:.4f}\\n\")\n    f.write(f\"  Final Training Accuracy: {train_acc_history[-1]*100:.2f}%\\n\")\n    f.write(f\"  Final Validation Loss: {val_loss_history[-1]:.4f}\\n\")\n    f.write(f\"  Final Validation Accuracy: {val_acc_history[-1]*100:.2f}%\\n\\n\")\n    \n    f.write(\"SAVED FILES:\\n\")\n    f.write(f\"  Model State Dict: {model_save_path}\\n\")\n    f.write(f\"  Full Model: {full_model_path}\\n\")\n    f.write(f\"  Training History: {history_path}\\n\")\n    f.write(f\"  Metadata: {metadata_path}\\n\\n\")\n    \n    f.write(\"=\"*60 + \"\\n\")\n\nprint(f\"Model metadata saved to: {metadata_path}\")","metadata":{"_uuid":"3d4403d5-3c02-41d6-8421-52666d0543f9","_cell_guid":"718be46e-74fe-43b3-8607-268017fb4797","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-10T00:00:56.353673Z","iopub.execute_input":"2026-02-10T00:00:56.353969Z","iopub.status.idle":"2026-02-10T00:00:56.371046Z","shell.execute_reply.started":"2026-02-10T00:00:56.353932Z","shell.execute_reply":"2026-02-10T00:00:56.370308Z"},"papermill":{"duration":0.027148,"end_time":"2026-02-09T17:55:31.912286","exception":false,"start_time":"2026-02-09T17:55:31.885138","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Evaluate on all test datasets\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ntest_results = {}\nall_predictions = {}\nall_targets = {}\n\nfor name, loader in test_loaders.items():\n    model.eval()\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    predictions = []\n    targets = []\n    \n    with torch.no_grad():\n        for data, target in loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n            \n            test_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            test_total += target.size(0)\n            test_correct += (predicted == target).sum().item()\n            \n            predictions.extend(predicted.cpu().numpy())\n            targets.extend(target.cpu().numpy())\n    \n    avg_test_loss = test_loss / len(loader)\n    test_accuracy = test_correct / test_total\n    \n    test_results[name] = {\n        'loss': avg_test_loss,\n        'accuracy': test_accuracy\n    }\n    all_predictions[name] = np.array(predictions)\n    all_targets[name] = np.array(targets)\n    \n    print(f\"\\n{name} Results:\")\n    print(f\"  Loss: {avg_test_loss:.4f}\")\n    print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n\n# Visualization 1: Accuracy comparison across all datasets\nplt.figure(figsize=(12, 6))\ndataset_names = list(test_results.keys())\naccuracies = [test_results[name]['accuracy'] * 100 for name in dataset_names]\n\nplt.bar(dataset_names, accuracies, color='steelblue', edgecolor='black')\nplt.xlabel('Dataset', fontsize=12, fontweight='bold')\nplt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\nplt.title('Model Accuracy on All Test Datasets', fontsize=14, fontweight='bold')\nplt.ylim(0, 100)\nplt.grid(axis='y', alpha=0.3)\n\nfor i, (name, acc) in enumerate(zip(dataset_names, accuracies)):\n    plt.text(i, acc + 1, f'{acc:.2f}%', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(NAME+'_accuracy_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Visualization 2: Training and Validation Accuracy vs Epoch\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nepochs_range = range(1, len(train_acc_history) + 1)\nplt.plot(epochs_range, [acc * 100 for acc in train_acc_history], 'b-o', label='Training Accuracy', linewidth=2)\nplt.plot(epochs_range, [acc * 100 for acc in val_acc_history], 'r-s', label='Validation Accuracy', linewidth=2)\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\nplt.title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, train_loss_history, 'b-o', label='Training Loss', linewidth=2)\nplt.plot(epochs_range, val_loss_history, 'r-s', label='Validation Loss', linewidth=2)\nplt.xlabel('Epoch', fontsize=12, fontweight='bold')\nplt.ylabel('Loss', fontsize=12, fontweight='bold')\nplt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Visualization 3: Confusion Matrix for each dataset\nnum_datasets = len(test_results)\ncols = min(3, num_datasets)\nrows = (num_datasets + cols - 1) // cols\n\nfig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows))\nif num_datasets == 1:\n    axes = [axes]\nelse:\n    axes = axes.flatten() if num_datasets > 1 else [axes]\n\nfor idx, name in enumerate(dataset_names):\n    cm = confusion_matrix(all_targets[name], all_predictions[name])\n    \n    ax = axes[idx] if num_datasets > 1 else axes[0]\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n                cbar_kws={'label': 'Count'}, square=True)\n    ax.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n    ax.set_ylabel('True Label', fontsize=11, fontweight='bold')\n    ax.set_title(f'Confusion Matrix - {name}\\nAccuracy: {test_results[name][\"accuracy\"]*100:.2f}%', \n                 fontsize=12, fontweight='bold')\n\n# Hide extra subplots\nfor idx in range(num_datasets, len(axes)):\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"EVALUATION SUMMARY\")\nprint(\"=\"*50)\nfor name in dataset_names:\n    print(f\"{name:15s}: {test_results[name]['accuracy']*100:6.2f}%\")\nprint(\"=\"*50)","metadata":{"_uuid":"72d11bfa-899a-4357-bac3-9c68ce745399","_cell_guid":"9315c17a-f5e6-49c9-bf72-ec5d55dad9f6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-02-10T00:00:56.372229Z","iopub.execute_input":"2026-02-10T00:00:56.372801Z","iopub.status.idle":"2026-02-10T00:01:06.504487Z","shell.execute_reply.started":"2026-02-10T00:00:56.372777Z","shell.execute_reply":"2026-02-10T00:01:06.503704Z"},"papermill":{"duration":10.280294,"end_time":"2026-02-09T17:55:42.195863","exception":false,"start_time":"2026-02-09T17:55:31.915569","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}