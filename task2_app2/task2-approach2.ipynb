{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14760510,"sourceType":"datasetVersion","datasetId":9434511},{"sourceId":740453,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":564868,"modelId":577353},{"sourceId":740472,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":564886,"modelId":577368},{"sourceId":740500,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":564913,"modelId":577397},{"sourceId":745111,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":568438,"modelId":580771}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom collections import defaultdict\nfrom sklearn.metrics import mutual_info_score\n\n# =============================================================================\n# PHASE 0: CONFIGURATION & UTILITIES\n# =============================================================================\n\nclass ProberConfig:\n    # --- Paths ---\n    MODEL_PATH = r\"/kaggle/input/task1app3models/pytorch/default/2/task4_irm_modelv1.pth\"\n    # Using RG95 training data for Color bias discovery\n    DATA_RG_PATH = r\"/kaggle/input/cmnistneo1/train_data_rg95z.npz\"\n    # Using BW100 test data for Shape bias discovery (clean signal)\n    DATA_BW_PATH = r\"/kaggle/input/cmnistneo1/test_data_bw100z.npz\"\n\n    OUTPUT_DIR = r\"results/\"\n    \n    # --- Optimization Engine ---\n    LR = 0.05\n    ITERATIONS = 512\n    TV_WEIGHT = 1e-4\n    L2_WEIGHT = 1e-5\n    JITTER = 2\n    BLUR_FREQ = 40\n    \n    # --- Diagnostic Settings ---\n    BATCH_SIZE = 256\n    NUM_SAMPLES_PROFILE = 2000  # Number of images to use for neuron profiling\n    TOP_K = 5                  # Top K neurons to identify/visualize\n    POLY_TOP_N = 5             # Number of example images to show for polysemantic neurons\n    POLY_MIN_SCORE = 0.2       # Minimum poly_score threshold (entropy × strength)\n    POLY_STRENGTH_PERCENTILE = 95  # Percentile for computing neuron activation strength\n    VERBOSE_VIZ = False        # Print optimization diagnostics for each neuron\n    \n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef ensure_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n# =============================================================================\n# PHASE 1: MODEL DEFINITION & LOADING\n# =============================================================================\n\nclass CNN3Layer(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CNN3Layer, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        # We need hooks to grab activations, so standard forward pass is fine.\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = self.pool3(F.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\ndef load_model():\n    model = CNN3Layer().to(ProberConfig.DEVICE)\n    try:\n        state_dict = torch.load(ProberConfig.MODEL_PATH, map_location=ProberConfig.DEVICE)\n        \n        # KEY REMAPPING FOR IRM MODELS\n        new_state_dict = {}\n        key_map = {\n            # Features (Convs)\n            \"features.0.weight\": \"conv1.weight\",\n            \"features.0.bias\":   \"conv1.bias\",\n            \"features.3.weight\": \"conv2.weight\",\n            \"features.3.bias\":   \"conv2.bias\",\n            \"features.6.weight\": \"conv3.weight\",\n            \"features.6.bias\":   \"conv3.bias\",\n            # Classifier (FCs)\n            \"classifier.0.weight\": \"fc1.weight\",\n            \"classifier.0.bias\":   \"fc1.bias\",\n            \"classifier.3.weight\": \"fc2.weight\",\n            \"classifier.3.bias\":   \"fc2.bias\"\n        }\n\n        for k, v in state_dict.items():\n            if k in key_map:\n                new_state_dict[key_map[k]] = v\n            else:\n                new_state_dict[k] = v\n                \n        # Load with strict=False to allow flexibility, but check keys\n        missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n        print(f\"[Phase 1] Model loaded from {ProberConfig.MODEL_PATH}\")\n        if missing: print(f\"  Missing keys: {missing}\")\n        if unexpected: print(f\"  Unexpected keys: {unexpected}\")\n        \n    except Exception as e:\n        print(f\"[Phase 1] Error loading model: {e}\")\n        print(\"[Phase 1] Running with random weights (WARNING: Results meant for testing pipeline only)\")\n    model.eval()\n    return model\n\ndef load_data(path, limit=None):\n    \"\"\"Loads a subset of the dataset for profiling neurons.\"\"\"\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Data file not found: {path}\")\n\n    data = np.load(path)\n    images = data['images']\n    labels = data['labels']\n    \n    if limit:\n        indices = np.random.choice(len(images), min(limit, len(images)), replace=False)\n        images = images[indices]\n        labels = labels[indices]\n        \n    # Preprocess: [0, 255] -> [0, 1], NHWC -> NCHW\n    images = images.astype('float32') / 255.0\n    images = torch.from_numpy(images).permute(0, 3, 1, 2)\n    labels = torch.from_numpy(labels).long()\n    \n    return TensorDataset(images, labels)\n\n# =============================================================================\n# PHASE 1: OPTIMIZATION ENGINE (VISUALIZATION)\n# =============================================================================\n\nclass FeatureVisualizer:\n    def __init__(self, model):\n        self.model = model\n        self.device = ProberConfig.DEVICE\n\n    def total_variation_loss(self, img):\n        b, c, h, w = img.shape\n        tv_h = torch.pow(img[:, :, 1:, :] - img[:, :, :-1, :], 2).sum()\n        tv_w = torch.pow(img[:, :, :, 1:] - img[:, :, :, :-1], 2).sum()\n        return (tv_h + tv_w) / (c * h * w)\n\n    def jitter_transform(self, img, lim=ProberConfig.JITTER):\n        ox, oy = random.randint(-lim, lim), random.randint(-lim, lim)\n        return torch.roll(img, shifts=(ox, oy), dims=(2, 3))\n\n    def generate_ideal_image(self, layer, channel_idx, verbose=False):\n        \"\"\"\n        Generates the image that maximizes a specific neuron's activation.\n        \n        Common reasons for empty/noisy visualizations:\n        1. Dead neurons: Neuron outputs near-zero for all inputs\n        2. Weak gradients: Optimization can't find strong activating patterns\n        3. Over-regularization: TV/L2 penalties suppress the signal\n        4. Poor initialization: Starting point too far from optimal\n        \"\"\"\n        input_img = torch.rand(1, 3, 28, 28, device=self.device) * 0.1 + 0.45\n        input_img.requires_grad = True\n        optimizer = optim.Adam([input_img], lr=ProberConfig.LR)\n\n        activations = {}\n        def hook_fn(m, i, o): activations['act'] = o\n        handle = layer.register_forward_hook(hook_fn)\n\n        best_activation = -float('inf')\n        best_img = None\n        \n        for i in range(ProberConfig.ITERATIONS):\n            optimizer.zero_grad()\n            \n            # Apply jitter for robustness\n            img_jit = self.jitter_transform(input_img)\n            \n            self.model(img_jit)\n            act = activations['act']\n            \n            # Maximize mean activation of target channel\n            current_activation = act[0, channel_idx].mean()\n            obj_loss = -current_activation\n            \n            # Track best result\n            if current_activation.item() > best_activation:\n                best_activation = current_activation.item()\n                best_img = input_img.detach().clone()\n            \n            # Regularizers\n            tv_loss = self.total_variation_loss(input_img) * ProberConfig.TV_WEIGHT\n            l2_loss = torch.norm(input_img) * ProberConfig.L2_WEIGHT\n            \n            loss = obj_loss + tv_loss + l2_loss\n            loss.backward()\n            optimizer.step()\n            \n            with torch.no_grad():\n                input_img.clamp_(0, 1)\n                \n            # Diagnostic output\n            if verbose and (i % 100 == 0 or i == ProberConfig.ITERATIONS - 1):\n                print(f\"  Iter {i}: Activation={current_activation.item():.4f}, Loss={loss.item():.4f}\")\n\n        handle.remove()\n        \n        # If optimization failed (very weak activation), return best attempt\n        if best_activation < 0.01:\n            if verbose:\n                print(f\"  WARNING: Weak neuron (max activation={best_activation:.4f}). May be dead/inactive.\")\n        \n        return (best_img if best_img is not None else input_img).detach().cpu()\n\n# =============================================================================\n# PHASE 2: TRAITOR DETECTOR (NEURON PROFILING)\n# =============================================================================\n\nclass NeuronProfiler:\n    def __init__(self, model):\n        self.model = model\n        self.device = ProberConfig.DEVICE\n        self.activations = defaultdict(list)\n        \n    def _get_hooks(self):\n        hooks = []\n        layers = {'conv1': self.model.conv1, 'conv2': self.model.conv2, 'conv3': self.model.conv3}\n        \n        for name, layer in layers.items():\n            def get_hook(n):\n                return lambda m, i, o: self.activations[n].append(o.detach().cpu())\n            hooks.append(layer.register_forward_hook(get_hook(name)))\n        return hooks\n\n    def collect_activations(self, dataset):\n        \"\"\"Passes dataset through model and records activations.\"\"\"\n        self.activations.clear()\n        loader = DataLoader(dataset, batch_size=ProberConfig.BATCH_SIZE, shuffle=False)\n        hooks = self._get_hooks()\n        \n        extracted_features = defaultdict(list)\n        \n        with torch.no_grad():\n            for images, _ in loader:\n                images = images.to(self.device)\n                self.model(images)\n                \n        for h in hooks: h.remove()\n        \n        # Aggregate activations\n        for layer_name, batches in self.activations.items():\n            # [N, C, H, W] -> Global Average Pooling -> [N, C]\n            full_acts = torch.cat(batches, dim=0)\n            extracted_features[layer_name] = full_acts.mean(dim=(2, 3)).numpy()\n            \n        return extracted_features\n\n    def score_neurons(self, rg_features, bw_features, rg_colors, bw_labels):\n        \"\"\"\n        Computes Color_Score (on RG data) and Shape_Score (on BW data).\n        \"\"\"\n        print(\"[Phase 2] Scoring neurons...\")\n        scores = {}\n        \n        for layer_name in rg_features.keys():\n            rg_acts = rg_features[layer_name] # [N_rg, C]\n            bw_acts = bw_features[layer_name] # [N_bw, C]\n            n_channels = rg_acts.shape[1]\n            \n            layer_scores = {}\n            for ch in range(n_channels):\n                # 1. Color Score (on RG data): MI(Act, Color)\n                # Binarize using median split\n                act_rg = rg_acts[:, ch]\n                thr = np.median(act_rg)\n                bin_rg = (act_rg > thr).astype(int)\n                \n                # Assume RG labels: 0-4 Red(0), 5-9 Green(1)\n                colors = (rg_colors >= 5).numpy().astype(int)\n                color_score = mutual_info_score(bin_rg, colors)\n                \n                # 2. Shape Score (on BW data): MI(Act, Digit)\n                # If neuron fires on BW images, it MUST be seeing shape.\n                act_bw = bw_acts[:, ch]\n                # Use same threshold? Or dataset specific? Let's use dataset specifics to be fair.\n                thr_bw = np.median(act_bw) if np.var(act_bw) > 1e-5 else 0\n                bin_bw = (act_bw > thr_bw).astype(int)\n                \n                shape_score = mutual_info_score(bin_bw, bw_labels.numpy())\n                \n                # Bias Ratio: High Color / Low Shape\n                ratio = color_score / (shape_score + 1e-6)\n                \n                layer_scores[ch] = {\n                    'shape': shape_score,\n                    'color': color_score,\n                    'ratio': ratio\n                }\n            scores[layer_name] = layer_scores\n        return scores\n\n    def identify_roles(self, scores):\n        \"\"\"\n        Identifies neuron roles based on mutual information scores.\n        \n        Selection Criteria:\n        - TRAITORS: Top-K neurons with highest Color/Shape ratio (color-biased)\n        - HEROES: Top-K neurons with highest Shape MI score (shape-focused)\n        \"\"\"\n        roles = {'traitors': [], 'heroes': [], 'all': []}\n        all_neurons = []\n        \n        for layer, channels in scores.items():\n            for ch, metrics in channels.items():\n                neuron_data = {\n                    'layer': layer, 'channel': ch, **metrics\n                }\n                all_neurons.append(neuron_data)\n        \n        # Store all for plotting\n        roles['all'] = all_neurons\n\n        # Traitors: High Ratio (Color > Shape) - sorted by ratio descending\n        roles['traitors'] = sorted(all_neurons, key=lambda x: x['ratio'], reverse=True)[:ProberConfig.TOP_K]\n        \n        # Heroes: High Shape Score - sorted by shape MI descending\n        # Filter out neurons with very low activity\n        active_neurons = [n for n in all_neurons if n['shape'] > 0.05]\n        roles['heroes'] = sorted(active_neurons, key=lambda x: x['shape'], reverse=True)[:ProberConfig.TOP_K]\n        \n        return roles\n\n# =============================================================================\n# PHASE 3: POLYSEMANTICITY DETECTOR (DATA DRIVEN)\n# =============================================================================\n\nclass PolysemanticDetector:\n    def __init__(self, model, dataset):\n        self.model = model\n        self.device = ProberConfig.DEVICE\n        # Create loader with shuffling disabled to track indices if needed, \n        # but here we just need the images and labels.\n        self.loader = DataLoader(dataset, batch_size=ProberConfig.BATCH_SIZE, shuffle=False)\n        \n    def find_polysemantic_neurons(self):\n        \"\"\"\n        Finds neurons that activate for multiple different digit classes.\n        \n        Uses principled entropy-based scoring:\n            PolyScore = Activation_Strength × Label_Entropy\n        \n        This automatically:\n        - Penalizes blank neurons (low activation → low score)\n        - Rewards neurons that fire strongly AND ambiguously\n        - Avoids the \"inactive neuron illusion\" where dead neurons appear polysemantic\n        \"\"\"\n        print(\"[Phase 3] Scanning for Polysemantic Neurons...\")\n        \n        # 1. Collect all activations and labels\n        # Structure: layer -> channel -> [(activation, label), ...]\n        # We need a way to store top K without keeping everything in memory if dataset is huge.\n        # But for 2000 samples (ProberConfig), we can store all.\n        \n        activations = defaultdict(list)\n        all_labels = []\n        all_images = [] # Keep ref to images for visualization\n        \n        hooks = []\n        layers = {'conv1': self.model.conv1, 'conv2': self.model.conv2, 'conv3': self.model.conv3}\n        \n        for name, layer in layers.items():\n            def get_hook(n):\n                return lambda m, i, o: activations[n].append(o.detach().cpu())\n            hooks.append(layer.register_forward_hook(get_hook(name)))\n            \n        with torch.no_grad():\n            for imgs, lbls in self.loader:\n                imgs = imgs.to(self.device)\n                self.model(imgs)\n                all_labels.extend(lbls.numpy())\n                # Store images on CPU to save GPU RAM\n                all_images.append(imgs.cpu())\n                \n        for h in hooks: h.remove()\n        \n        all_labels = np.array(all_labels)\n        full_images = torch.cat(all_images)\n        \n        # 2. Analyze each neuron with entropy-based scoring\n        poly_candidates = []\n        total_neurons_checked = 0\n        weak_neurons_filtered = 0\n        \n        for layer_name, batches in activations.items():\n            full_acts = torch.cat(batches, dim=0)  # [N, C, H, W]\n            \n            # Robust spatial activation: top-k mean instead of single max\n            flat = full_acts.flatten(2)  # [N, C, H*W]\n            vals, _ = flat.topk(5, dim=2)  # Top-5 spatial activations\n            act = vals.mean(dim=2).numpy()  # [N, C] - robust activation per image\n            \n            n_channels = act.shape[1]\n            total_neurons_checked += n_channels\n            \n            for ch in range(n_channels):\n                # Neuron-level activation strength: 95th percentile\n                # This ignores noise and focuses on genuine firing regime\n                strength = np.percentile(act[:, ch], ProberConfig.POLY_STRENGTH_PERCENTILE)\n                \n                # FILTER: Skip nearly dead neurons\n                if strength < 0.05:\n                    weak_neurons_filtered += 1\n                    continue\n                \n                # Focus ONLY on genuinely strong activations (70% of neuron's peak)\n                strong_threshold = 0.7 * strength\n                strong_idx = act[:, ch] > strong_threshold\n                strong_labels = all_labels[strong_idx]\n                \n                # Need enough samples for meaningful entropy\n                if len(strong_labels) < 5:\n                    weak_neurons_filtered += 1\n                    continue\n                \n                # Compute label entropy on STRONG activations only\n                counts = np.bincount(strong_labels, minlength=10)\n                probs = counts / counts.sum()\n                entropy = -np.sum(probs * np.log(probs + 1e-8))\n                \n                # Polysemantic Score = Entropy × Strength\n                # High score → neuron fires strongly AND ambiguously\n                poly_score = entropy * strength\n                \n                # Only keep genuinely polysemantic neurons\n                if poly_score > ProberConfig.POLY_MIN_SCORE:\n                    # Get top-N examples for visualization\n                    strong_act_values = act[:, ch][strong_idx]\n                    top_indices = np.where(strong_idx)[0]\n                    sorted_order = np.argsort(strong_act_values)[::-1]\n                    topn_idx = top_indices[sorted_order[:ProberConfig.POLY_TOP_N]]\n                    topn_labels = all_labels[topn_idx]\n                    \n                    poly_candidates.append({\n                        'layer': layer_name,\n                        'channel': ch,\n                        'topn_idx': topn_idx,\n                        'topn_labels': topn_labels,\n                        'entropy': entropy,\n                        'strength': strength,\n                        'poly_score': poly_score,\n                        'n_strong': len(strong_labels),\n                        'label_dist': counts\n                    })\n        \n        # Sort by polysemantic score (entropy × strength)\n        # Highest = strongest AND most ambiguous neurons\n        poly_candidates.sort(key=lambda x: x['poly_score'], reverse=True)\n        \n        print(f\"[Phase 3] Analyzed {total_neurons_checked} neurons, filtered {weak_neurons_filtered} weak/low-entropy neurons\")\n        print(f\"[Phase 3] Found {len(poly_candidates)} neurons with poly_score > {ProberConfig.POLY_MIN_SCORE}\")\n        \n        return poly_candidates, full_images\n\n# =============================================================================\n# PHASE 4: DASHBOARD & EXECUTION\n# =============================================================================\n\ndef run_suite():\n    \"\"\"\n    Runs the complete interpretability suite.\n    \n    NOTE ON EMPTY/NOISY VISUALIZATIONS:\n    Some neurons may show noisy or empty ideal images because:\n    - Dead/Inactive neurons: Output near-zero for all inputs (common after dropout/regularization)\n    - Polysemantic neurons: Respond to multiple unrelated features, creating incoherent visualizations\n    - Inhibitory neurons: Activate by suppressing signals rather than detecting patterns\n    - High-level abstract features: Require specific contexts that single-image optimization can't capture\n    \n    NOTE ON POLYSEMANTIC DETECTION:\n    Uses entropy-based scoring: PolyScore = Entropy × Activation_Strength\n    - Entropy: computed on labels of STRONG activations only (avoids noise)\n    - Strength: 95th percentile of neuron's activations (robust measure)\n    - This automatically filters dead neurons whose random activations falsely appear polysemantic\n    - Only neurons with poly_score > POLY_MIN_SCORE are considered genuinely polysemantic\n    \n    Set VERBOSE_VIZ=True to see optimization diagnostics and identify weak neurons.\n    \"\"\"\n    ensure_dir(ProberConfig.OUTPUT_DIR)\n    set_seed()\n    \n    # 1. Setup\n    model = load_model()\n    vis = FeatureVisualizer(model)\n    profiler = NeuronProfiler(model)\n    # mixer = FeatureMixer(vis) # Removed\n    \n    # 2. Load Data\n    print(f\"Loading RG Data: {ProberConfig.DATA_RG_PATH}\")\n    ds_rg = load_data(ProberConfig.DATA_RG_PATH, limit=ProberConfig.NUM_SAMPLES_PROFILE)\n    print(f\"Loading BW Data: {ProberConfig.DATA_BW_PATH}\")\n    ds_bw = load_data(ProberConfig.DATA_BW_PATH, limit=ProberConfig.NUM_SAMPLES_PROFILE)\n    \n    # 3. Phase 2: Profile & Identify\n    feats_rg = profiler.collect_activations(ds_rg)\n    feats_bw = profiler.collect_activations(ds_bw)\n    \n    # RG contains labels, BW contains labels. We pass labels for scoring.\n    # ds_rg.tensors[1] is labels. \n    scores = profiler.score_neurons(feats_rg, feats_bw, ds_rg.tensors[1], ds_bw.tensors[1])\n    roles = profiler.identify_roles(scores)\n    \n    print(f\"\\n[Phase 2] Identified Roles:\")\n    print(f\"  Total Neurons analyzed: {len(roles['all'])}\")\n    # Simple threshold heuristics for counting 'general' population\n    n_traitors = sum(1 for n in roles['all'] if n['ratio'] > 2.0)\n    n_heroes   = sum(1 for n in roles['all'] if n['ratio'] < 0.5 and n['shape'] > 0.1)\n    print(f\"  Approx. Traitors (Ratio > 2.0): {n_traitors}\")\n    print(f\"  Approx. Heroes (Ratio < 0.5): {n_heroes}\")\n\n    print(\"\\n=== TOP 5 TRAITORS (Color Biased) ===\")\n    for n in roles['traitors']:\n        print(f\"{n['layer']} Ch{n['channel']} | ColorMI: {n['color']:.3f} | ShapeMI: {n['shape']:.3f} | Ratio: {n['ratio']:.1f}\")\n\n    print(\"\\n=== TOP 5 HEROES (Shape Focused) ===\")\n    for n in roles['heroes']:\n        print(f\"{n['layer']} Ch{n['channel']} | ColorMI: {n['color']:.3f} | ShapeMI: {n['shape']:.3f}\")\n        \n    # --- 2D SCATTER PLOT ---\n    plt.figure(figsize=(10, 8))\n    \n    # Extract data\n    shapes = [n['shape'] for n in roles['all']]\n    colors = [n['color'] for n in roles['all']]\n    ratios = [n['ratio'] for n in roles['all']]\n    \n    # Normalize ratio for color map: 0 = Blue (Hero), 1 = Red (Traitor)\n    # Using log scale for ratio because it can vary wildly\n    # Clipping for better visualization\n    ratios = np.array(ratios)\n    ratios = np.clip(ratios, 0.1, 10)\n    norm = plt.Normalize(0.1, 10)\n    \n    scatter = plt.scatter(shapes, colors, c=ratios, cmap='coolwarm', norm=norm, alpha=0.7, edgecolors='k')\n    plt.colorbar(scatter, label='Bias Ratio (Color/Shape)')\n    \n    plt.xlabel('Shape MI (Shape Score)')\n    plt.ylabel('Color MI (Color Score)')\n    plt.title('Neuron Landscape: Heroes vs Traitors')\n    plt.grid(True, alpha=0.3)\n    \n    # Annotate Top Traitors & Heroes\n    for n in roles['traitors']:\n        plt.annotate(f\"{n['layer']}:{n['channel']}\", (n['shape'], n['color']), fontsize=8, color='red')\n    for n in roles['heroes']:\n        plt.annotate(f\"{n['layer']}:{n['channel']}\", (n['shape'], n['color']), fontsize=8, color='blue')\n\n    plt.savefig(os.path.join(ProberConfig.OUTPUT_DIR, \"neuron_landscape.png\"))\n    print(f\"[Phase 2] Scatter plot saved to {ProberConfig.OUTPUT_DIR}/neuron_landscape.png\")\n\n    # 4. Phase 1 & 4: Viz Dashboard\n    # Dynamic grid based on TOP_K\n    n_cols = ProberConfig.TOP_K\n    fig, axes = plt.subplots(2, n_cols, figsize=(3 * n_cols, 6))\n    fig.suptitle(f\"Task 2: The Prober - Top {n_cols} Neuron Roles\", fontsize=16)\n    \n    # Ensure axes is 2D array even if n_cols=1\n    if n_cols == 1:\n        axes = axes.reshape(2, 1)\n    \n    # Traitors\n    for i, node in enumerate(roles['traitors']):\n        layer = getattr(model, node['layer'])\n        if ProberConfig.VERBOSE_VIZ:\n            print(f\"\\nGenerating ideal image for Traitor {node['layer']}:{node['channel']}\")\n        img = vis.generate_ideal_image(layer, node['channel'], verbose=ProberConfig.VERBOSE_VIZ)\n        img_np = img.squeeze().permute(1, 2, 0).numpy()\n        ax = axes[0, i]\n        ax.imshow(img_np)\n        ax.set_title(f\"Traitor {node['layer']}:{node['channel']}\\nColor MI: {node['color']:.2f}\")\n        ax.axis('off')\n\n    # Heroes\n    for i, node in enumerate(roles['heroes']):\n        layer = getattr(model, node['layer'])\n        if ProberConfig.VERBOSE_VIZ:\n            print(f\"\\nGenerating ideal image for Hero {node['layer']}:{node['channel']}\")\n        img = vis.generate_ideal_image(layer, node['channel'], verbose=ProberConfig.VERBOSE_VIZ)\n        img_np = img.squeeze().permute(1, 2, 0).numpy()\n        ax = axes[1, i]\n        ax.imshow(img_np)\n        ax.set_title(f\"Hero {node['layer']}:{node['channel']}\\nShape MI: {node['shape']:.2f}\")\n        ax.axis('off')\n        \n    p = os.path.join(ProberConfig.OUTPUT_DIR, \"neuron_roles_dashboard.png\")\n    plt.tight_layout()\n    plt.savefig(p)\n    print(f\"\\n[Phase 4] Dashboard saved to {p}\")\n    \n    # 5. Phase 3: Polysemantic Detector\n    # Using the BW dataset because we care about SHAPE polysemanticity (e.g. 7 and 1)\n    # RG dataset is naturally \"polysemantic\" for color+shape, which is trivial.\n    # We want to find neurons confused about *digits* in clean data.\n    \n    # Reload BW dataset purely for this analysis (or reuse ds_bw)\n    poly_detector = PolysemanticDetector(model, ds_bw)\n    poly_neurons, val_images = poly_detector.find_polysemantic_neurons()\n    \n    print(f\"\\n[Phase 3] Total polysemantic neurons found: {len(poly_neurons)} (activating for >1 digit class)\")\n    print(f\"[Phase 3] Visualizing top {ProberConfig.TOP_K} most polysemantic neurons\")\n    \n    # Print stats for top polysemantic neurons\n    if poly_neurons:\n        print(f\"\\n=== TOP {min(5, len(poly_neurons))} POLYSEMANTIC NEURONS ===\", )\n        for i, n in enumerate(poly_neurons[:5]):\n            print(f\"{i+1}. {n['layer']} Ch{n['channel']} | Entropy: {n['entropy']:.3f} | Strength: {n['strength']:.3f} | PolyScore: {n['poly_score']:.3f}\")\n    \n    # Visualize Top K Polysemantic Neurons\n    n_viz = min(len(poly_neurons), ProberConfig.TOP_K)\n    top_poly = poly_neurons[:n_viz]\n    \n    if top_poly:\n        # Grid: Rows = Neurons, Cols = 1 Ideal Image + 1 Text Info + N Top Activating Images\n        fig_p, axes_p = plt.subplots(n_viz, ProberConfig.POLY_TOP_N + 2, figsize=(2 * (ProberConfig.POLY_TOP_N + 2), 2.5 * n_viz))\n        fig_p.suptitle(f\"Top {n_viz} Polysemantic Neurons (Total: {len(poly_neurons)}, Entropy×Strength Scoring)\", fontsize=14)\n        \n        # Ensure axes_p is 2D array even if n_viz=1\n        if n_viz == 1:\n            axes_p = axes_p.reshape(1, -1)\n            \n        for row, neuron in enumerate(top_poly):\n            # Generate Ideal Image for this neuron\n            layer_obj = getattr(model, neuron['layer'])\n            if ProberConfig.VERBOSE_VIZ:\n                print(f\"\\nGenerating ideal image for Polysemantic {neuron['layer']}:{neuron['channel']}\")\n            ideal_img = vis.generate_ideal_image(layer_obj, neuron['channel'], verbose=ProberConfig.VERBOSE_VIZ)\n            ideal_img_np = ideal_img.squeeze().permute(1, 2, 0).numpy()\n            \n            ax_ideal = axes_p[row, 0]\n            ax_ideal.imshow(ideal_img_np)\n            ax_ideal.set_title(f\"Ideal\\n{neuron['layer']}:{neuron['channel']}\\nPS={neuron['poly_score']:.2f}\", fontsize=9)\n            ax_ideal.axis('off')\n            \n            # Info Text\n            info = f\"Entropy: {neuron['entropy']:.2f}\\nStrength: {neuron['strength']:.2f}\\nLabels: {neuron['topn_labels']}\"\n            ax_text = axes_p[row, 1]\n            ax_text.text(0.5, 0.5, info, ha='center', va='center', fontsize=9, wrap=True)\n            ax_text.axis('off')\n            \n            # Top N Activating Images\n            for i, idx in enumerate(neuron['topn_idx']):\n                img_t = val_images[idx].permute(1, 2, 0).numpy()\n                lbl = neuron['topn_labels'][i]\n                \n                ax_img = axes_p[row, i+2]\n                ax_img.imshow(img_t)\n                ax_img.set_title(f\"Lbl: {lbl}\", fontsize=9)\n                ax_img.axis('off')\n                \n        p_poly = os.path.join(ProberConfig.OUTPUT_DIR, \"polysemantic_examples.png\")\n        plt.tight_layout()\n        plt.savefig(p_poly)\n        print(f\"[Phase 3] Polysemantic examples saved to {p_poly}\")\n\nif __name__ == \"__main__\":\n    run_suite()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-09T09:19:00.630776Z","iopub.execute_input":"2026-02-09T09:19:00.631345Z","iopub.status.idle":"2026-02-09T09:19:27.251717Z","shell.execute_reply.started":"2026-02-09T09:19:00.631312Z","shell.execute_reply":"2026-02-09T09:19:27.251082Z"}},"outputs":[],"execution_count":null}]}