{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Task 4: The Intervention - Method 2: Saliency-Guided Training\n",
                "\n",
                "This notebook implements **Saliency-Guided Training**. We use Grad-CAM from scratch to generate attention heatmaps during training and penalize the model if it looks at the \"wrong\" places (i.e., the colored background) instead of the digit shape."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch_directml\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "import os\n",
                "\n",
                "device = torch_directml.device()\n",
                "print(f\"Using DirectML device: {device}\")\n",
                "print(f\"Device name: {torch_directml.device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load",
            "metadata": {},
            "source": [
                "## 1. Load Data and Generate Masks\n",
                "The masks represent the \"ground truth\" attention regions (the digit strokes)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data",
            "metadata": {},
            "source": [
                "TRAIN_PATH = '../mydata/dataset/train_data_rg.npz'\n",
                "TEST_PATH = '../mydata/dataset/test_data_rg.npz'\n",
                "\n",
                "def load_npz_with_masks(path):\n",
                "    data = np.load(path)\n",
                "    X = data['images'].astype('float32') / 255.0\n",
                "    y = data['labels']\n",
                "    # Generate a simple mask by taking the max across channels (finds the digit)\n",
                "    # Then thresholding to isolate digits from background noise\n",
                "    masks = (X.max(axis=-1) > 0.2).astype('float32')\n",
                "    return X, y, masks\n",
                "\n",
                "X_train, y_train, M_train = load_npz_with_masks(TRAIN_PATH)\n",
                "X_test, y_test, M_test = load_npz_with_masks(TEST_PATH)\n",
                "\n",
                "X_train_t = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
                "y_train_t = torch.LongTensor(y_train)\n",
                "M_train_t = torch.FloatTensor(M_train).unsqueeze(1) # (N, 1, H, W)\n",
                "\n",
                "X_test_t = torch.FloatTensor(X_test).permute(0, 3, 1, 2)\n",
                "y_test_t = torch.LongTensor(y_test)\n",
                "\n",
                "print(f\"Train images: {X_train_t.shape}, Masks: {M_train_t.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model",
            "metadata": {},
            "source": [
                "## 2. Model with Grad-CAM Support\n",
                "We need a model where we can easily access features and gradients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model_def",
            "metadata": {},
            "source": [
                "class CNNGradCAM(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(CNNGradCAM, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
                "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
                "        self.pool = nn.MaxPool2d(2, 2)\n",
                "        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
                "        self.fc2 = nn.Linear(128, 10)\n",
                "        \n",
                "        self.gradients = None\n",
                "        self.activations = None\n",
                "\n",
                "    def activations_hook(self, grad):\n",
                "        self.gradients = grad\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.pool(F.relu(self.conv1(x)))\n",
                "        x = self.pool(F.relu(self.conv2(x)))\n",
                "        # Hook into the last conv layer\n",
                "        x = self.conv3(x)\n",
                "        if x.requires_grad:\n",
                "            h = x.register_hook(self.activations_hook)\n",
                "        self.activations = x\n",
                "        x = self.pool(F.relu(x))\n",
                "        \n",
                "        x = x.view(x.size(0), -1)\n",
                "        x = F.relu(self.fc1(x))\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "    def get_gradcam(self, class_idx):\n",
                "        # Assuming class_idx is the target for the whole batch\n",
                "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
                "        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n",
                "        cam = F.relu(cam)\n",
                "        # Rescale to input size\n",
                "        cam = F.interpolate(cam, size=(28, 28), mode='bilinear', align_corners=False)\n",
                "        # Normalize per image\n",
                "        batch_min = cam.view(cam.size(0), -1).min(dim=1)[0].view(-1, 1, 1, 1)\n",
                "        batch_max = cam.view(cam.size(0), -1).max(dim=1)[0].view(-1, 1, 1, 1)\n",
                "        cam = (cam - batch_min) / (batch_max - batch_min + 1e-8)\n",
                "        return cam\n",
                "\n",
                "model = CNNGradCAM().to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "train_logic",
            "metadata": {},
            "source": [
                "## 3. Training with Saliency Penalty\n",
                "The penalty encourages the Grad-CAM map to match the binary mask of the digit."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training",
            "metadata": {},
            "source": [
                "def train_saliency(model, loader, optimizer, alpha, device):\n",
                "    model.train()\n",
                "    total_ce = 0\n",
                "    total_sal = 0\n",
                "    \n",
                "    for x, y, m in loader:\n",
                "        x, y, m = x.to(device), y.to(device), m.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        logits = model(x)\n",
                "        ce_loss = F.cross_entropy(logits, y)\n",
                "        \n",
                "        # Saliency penalty: Get Grad-CAM for the correct class\n",
                "        # Note: Grad-CAM requires a backward pass to get gradients\n",
                "        # 1. First backward to get gradients for the hook\n",
                "        ce_loss.backward(retain_graph=True)\n",
                "        \n",
                "        cam = model.get_gradcam(y)\n",
                "        saliency_loss = F.mse_loss(cam, m)\n",
                "        \n",
                "        # 2. Add saliency gradient\n",
                "        (alpha * saliency_loss).backward()\n",
                "        \n",
                "        optimizer.step()\n",
                "        \n",
                "        total_ce += ce_loss.item()\n",
                "        total_sal += saliency_loss.item()\n",
                "        \n",
                "    return total_ce / len(loader), total_sal / len(loader)\n",
                "\n",
                "batch_size = 128\n",
                "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t, M_train_t), batch_size=batch_size, shuffle=True)\n",
                "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
                "\n",
                "epochs = 15\n",
                "alpha = 5.0 # Weight for saliency loss\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    avg_ce, avg_sal = train_saliency(model, train_loader, optimizer, alpha, device)\n",
                "    \n",
                "    # Test Accuracy\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        logits = model(X_test_t.to(device))\n",
                "        acc = (logits.argmax(1) == y_test_t.to(device)).float().mean().item()\n",
                "        \n",
                "    print(f\"Epoch {epoch+1}/{epochs} | CE: {avg_ce:.4f} | Sal: {avg_sal:.4f} | Test Acc: {acc:.2%}\")\n",
                "\n",
                "torch.save(model.state_dict(), 'task4_method2.pth')\n",
                "print(\"Model saved as task4_method2.pth\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "viz_results",
            "metadata": {},
            "source": [
                "## 4. Visual Verification\n",
                "Compare the model's Grad-CAM heatmap with the digit mask."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "viz",
            "metadata": {},
            "source": [
                "model.eval()\n",
                "idx = 0\n",
                "img = X_test_t[idx:idx+1].to(device)\n",
                "img.requires_grad = True\n",
                "out = model(img)\n",
                "out.argmax().backward()\n",
                "cam = model.get_gradcam(0).detach().cpu().numpy()[0, 0]\n",
                "\n",
                "plt.figure(figsize=(10, 3))\n",
                "plt.subplot(1, 3, 1)\n",
                "plt.imshow(X_test[idx])\n",
                "plt.title('Original Image')\n",
                "plt.subplot(1, 3, 2)\n",
                "plt.imshow(M_test[idx], cmap='gray')\n",
                "plt.title('Shape Mask')\n",
                "plt.subplot(1, 3, 3)\n",
                "plt.imshow(cam, cmap='jet')\n",
                "plt.title('Intervention Heatmap')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}