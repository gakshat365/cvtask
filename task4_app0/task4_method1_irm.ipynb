{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Task 4: The Intervention - Method 1: IRM (Gradient Penalty)\n",
                "\n",
                "This notebook implements **Invariant Risk Minimization (IRM)** style gradient penalty to force the model to ignore spurious color correlations and focus on digit shapes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch_directml\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from sklearn.model_selection import train_test_split\n",
                "import os\n",
                "\n",
                "device = torch_directml.device()\n",
                "print(f\"Using DirectML device: {device}\")\n",
                "print(f\"Device name: {torch_directml.device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load_data",
            "metadata": {},
            "source": [
                "## 1. Load Biased Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data_loading",
            "metadata": {},
            "source": [
                "TRAIN_PATH = '../mydata/dataset/train_data_rg.npz'\n",
                "TEST_PATH = '../mydata/dataset/test_data_rg.npz'\n",
                "\n",
                "def load_npz(path):\n",
                "    data = np.load(path)\n",
                "    X = data['images'].astype('float32') / 255.0\n",
                "    y = data['labels']\n",
                "    return X, y\n",
                "\n",
                "X_train, y_train = load_npz(TRAIN_PATH)\n",
                "X_test, y_test = load_npz(TEST_PATH)\n",
                "\n",
                "# Convert to tensors and permute to (N, C, H, W)\n",
                "X_train_tensor = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
                "y_train_tensor = torch.LongTensor(y_train)\n",
                "X_test_tensor = torch.FloatTensor(X_test).permute(0, 3, 1, 2)\n",
                "y_test_tensor = torch.LongTensor(y_test)\n",
                "\n",
                "print(f\"Train shape: {X_train_tensor.shape}\")\n",
                "print(f\"Test shape: {X_test_tensor.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_def",
            "metadata": {},
            "source": [
                "## 2. Define Model Architecture\n",
                "Using the same 3-layer CNN as Task 1 for a fair comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model_code",
            "metadata": {},
            "source": [
                "class CNN3Layer(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(CNN3Layer, self).__init__()\n",
                "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
                "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
                "        self.pool = nn.MaxPool2d(2, 2)\n",
                "        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
                "        self.fc2 = nn.Linear(128, 10)\n",
                "        self.dropout = nn.Dropout(0.5)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.pool(F.relu(self.conv1(x)))\n",
                "        x = self.pool(F.relu(self.conv2(x)))\n",
                "        x = self.pool(F.relu(self.conv3(x)))\n",
                "        x = x.view(x.size(0), -1)\n",
                "        x = F.relu(self.fc1(x))\n",
                "        x = self.dropout(x)\n",
                "        return self.fc2(x)\n",
                "\n",
                "model = CNN3Layer().to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "custom_loss",
            "metadata": {},
            "source": [
                "## 3. Custom IRM Loss Implementation\n",
                "\n",
                "The IRM penalty is calculated as the squared gradient of the loss with respect to a dummy scalar classifier $w=1.0$ applied to the model outputs. This encourages the model to have a representation where the optimal classifier is stable across potentially heterogeneous samples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "loss_logic",
            "metadata": {},
            "source": [
                "def compute_irm_penalty(logits, y):\n",
                "    scale = torch.tensor(1.).to(device).requires_grad_(True)\n",
                "    loss = F.cross_entropy(logits * scale, y)\n",
                "    grad = torch.autograd.grad(loss, [scale], create_graph=True)[0]\n",
                "    return torch.sum(grad**2)\n",
                "\n",
                "def train_irm(model, loader, optimizer, penalty_weight, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    total_penalty = 0\n",
                "    \n",
                "    for x, y in loader:\n",
                "        x, y = x.to(device), y.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        logits = model(x)\n",
                "        ce_loss = F.cross_entropy(logits, y)\n",
                "        penalty = compute_irm_penalty(logits, y)\n",
                "        \n",
                "        loss = ce_loss + penalty_weight * penalty\n",
                "        \n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += ce_loss.item()\n",
                "        total_penalty += penalty.item()\n",
                "        \n",
                "    return total_loss / len(loader), total_penalty / len(loader)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training_loop",
            "metadata": {},
            "source": [
                "## 4. Training with Increasing Penalty\n",
                "Often in IRM, we start with a standard CE loss and gradually ramp up the penalty to ensure the model first learns something useful before being constrained by invariance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "execution",
            "metadata": {},
            "source": [
                "batch_size = 128\n",
                "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
                "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
                "\n",
                "epochs = 20\n",
                "penalty_anneal_iters = 5 # Number of epochs to wait before applying penalty\n",
                "penalty_weight = 100.0\n",
                "\n",
                "history = {'loss': [], 'penalty': [], 'test_acc': []}\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    weight = penalty_weight if epoch >= penalty_anneal_iters else 1.0\n",
                "    avg_loss, avg_penalty = train_irm(model, train_loader, optimizer, weight, device)\n",
                "    \n",
                "    # Evaluate\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    with torch.no_grad():\n",
                "        logits = model(X_test_tensor.to(device))\n",
                "        preds = logits.argmax(dim=1)\n",
                "        correct = (preds == y_test_tensor.to(device)).sum().item()\n",
                "    \n",
                "    acc = correct / len(y_test_tensor)\n",
                "    \n",
                "    history['loss'].append(avg_loss)\n",
                "    history['penalty'].append(avg_penalty)\n",
                "    history['test_acc'].append(acc)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Penalty: {avg_penalty:.6f} | Test Acc: {acc:.4%}\")\n",
                "\n",
                "torch.save(model.state_dict(), 'task4_method1.pth')\n",
                "print(\"Model saved as task4_method1.pth\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "viz",
            "metadata": {},
            "source": [
                "## 5. Result Analysis\n",
                "Compare with the baseline performance from Task 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plotting",
            "metadata": {},
            "source": [
                "plt.figure(figsize=(12, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history['loss'], label='CE Loss')\n",
                "plt.title('Training Loss')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history['test_acc'], label='Hard Test Acc')\n",
                "plt.axhline(0.7, color='r', linestyle='--', label='Target')\n",
                "plt.title('Performance on Hard Set')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}