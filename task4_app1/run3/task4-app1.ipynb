{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14760510,"sourceType":"datasetVersion","datasetId":9434511},{"sourceId":744672,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":568438,"modelId":580771}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom itertools import cycle\n\n# ==============================================================================\n# Configuration & Hyperparameters\n# ==============================================================================\nclass Config:\n    # Data Paths\n    DATA_DIR = r\"/kaggle/input/cmnistneo1\"\n    TRAIN_FILE = \"train_data_rg95z.npz\"\n    \n    # Multiple Test Datasets for Comprehensive Evaluation\n    TEST_DATASETS = {\n        'rg95z': 'test_data_rg95z.npz',   # Biased (Easy)\n        'gr95z': 'test_data_gr95z.npz',   # Inverted (Hard)\n        'gr95e': 'test_data_gr95e.npz',\n        'gr95m': 'test_data_gr95m.npz',\n        'gr95h': 'test_data_gr95h.npz',\n        'gr95vh': 'test_data_gr95vh.npz',\n        'bw95z': 'test_data_bw95z.npz',   # Grayscale\n        'bw100z': 'test_data_bw100z.npz'\n    }\n    \n    # Training Hyperparameters\n    BATCH_SIZE = 256\n    EPOCHS = 50\n    LR = 1e-3\n    WEIGHT_DECAY = 1e-3\n    \n    # IRM Specifics\n    IRM_PENALTY_WEIGHT = 100.0\n    IRM_ANNEAL_EPOCHS = 10  # Epochs before penalty kicks in\n    \n    # System\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    SEED = 42\n\n# ==============================================================================\n# Model Architecture\n# ==============================================================================\nclass CNN3Layer(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CNN3Layer, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            \n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(64 * 3 * 3, 128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n# ==============================================================================\n# Utils: IRM Penalty & Data Loading\n# ==============================================================================\ndef irm_penalty(logits, y):\n    \"\"\"\n    Computes the IRM v1 penalty: gradient norm of the loss w.r.t a fixed scalar 1.0.\n    This effectively asks: \"If I multiplied the classifier output by a scalar 'w',\n    would the optimal 'w' result in 0 gradient at w=1.0 across all environments?\"\n    \"\"\"\n    scale = torch.tensor(1.).to(logits.device).requires_grad_()\n    loss = nn.CrossEntropyLoss()(logits * scale, y)\n    grad = torch.autograd.grad(loss, [scale], create_graph=True)[0]\n    return torch.sum(grad**2)\n\ndef get_dominant_color(img_tensor):\n    \"\"\"Returns dominant channel index (0=R, 1=G, 2=B) for a single image tensor.\"\"\"\n    means = torch.mean(img_tensor, dim=(1, 2))\n    return torch.argmax(means).item()\n\ndef load_data(config):\n    \"\"\"Loads data and splits training set into two IRM environments.\"\"\"\n    print(f\"\\n[Data] Loading from {config.DATA_DIR}...\")\n    \n    train_path = os.path.join(config.DATA_DIR, config.TRAIN_FILE)\n    if not os.path.exists(train_path):\n        raise FileNotFoundError(f\"Train file not found: {train_path}\")\n\n    # Load Train (Biased)\n    train_data = np.load(train_path)\n    X_train = torch.tensor(train_data['images'].astype('float32') / 255.0).permute(0, 3, 1, 2)\n    y_train = torch.tensor(train_data['labels']).long()\n\n    # 1. Identify Spurious Correlation (Color Bias)\n    print(\"[Data] Analyzing bias...\")\n    digit_bias_color = {}\n    for d in range(10):\n        # Sample subset to determine majority color\n        indices = (y_train == d).nonzero(as_tuple=True)[0][:100]\n        colors = [get_dominant_color(X_train[i]) for i in indices]\n        majority = max(set(colors), key=colors.count)\n        digit_bias_color[d] = majority\n    \n    # 2. Split into Environments (Aligned vs Conflict)\n    # Calculate dominant color for entire batch efficiently\n    img_means = torch.mean(X_train, dim=(2, 3)) # (N, 3)\n    img_colors = torch.argmax(img_means, dim=1) # (N,)\n    \n    # Expected color based on label\n    expected_colors = torch.tensor([digit_bias_color[y.item()] for y in y_train], device=X_train.device)\n    \n    # Mask: True if image follows bias (Environment 1), False if conflict (Environment 2)\n    aligned_mask = (img_colors == expected_colors.cpu())\n    \n    env1_idx = torch.nonzero(aligned_mask, as_tuple=True)[0]\n    env2_idx = torch.nonzero(~aligned_mask, as_tuple=True)[0]\n    \n    print(f\"  Env 1 (Aligned/Biased): {len(env1_idx)} samples\")\n    print(f\"  Env 2 (Conflict/OOD):   {len(env2_idx)} samples\")\n\n    # Create Datasets\n    ds_env1 = TensorDataset(X_train[env1_idx], y_train[env1_idx])\n    ds_env2 = TensorDataset(X_train[env2_idx], y_train[env2_idx])\n    \n    return ds_env1, ds_env2\n\ndef get_test_loaders(config):\n    loaders = {}\n    print(\"\\n[Data] Loading Test Sets...\")\n    for name, filename in config.TEST_DATASETS.items():\n        path = os.path.join(config.DATA_DIR, filename)\n        if os.path.exists(path):\n            data = np.load(path)\n            X = torch.tensor(data['images'].astype('float32') / 255.0).permute(0, 3, 1, 2)\n            y = torch.tensor(data['labels']).long()\n            ds = TensorDataset(X, y)\n            loaders[name] = DataLoader(ds, batch_size=config.BATCH_SIZE, shuffle=False)\n        else:\n            print(f\"  [Warn] {filename} not found.\")\n    return loaders\n\n# ==============================================================================\n# Visualization Logic (User Provided + Adapted)\n# ==============================================================================\ndef evaluate_and_plot(model, test_loaders, train_acc_history, val_acc_history, train_loss_history, val_loss_history, device):\n    print(\"\\n[Evaluate] Starting comprehensive evaluation...\")\n    criterion = nn.CrossEntropyLoss()\n    \n    test_results = {}\n    all_predictions = {}\n    all_targets = {}\n\n    for name, loader in test_loaders.items():\n        model.eval()\n        test_loss = 0.0\n        test_correct = 0\n        test_total = 0\n        predictions = []\n        targets = []\n        \n        with torch.no_grad():\n            for data, target in loader:\n                data, target = data.to(device), target.to(device)\n                output = model(data)\n                loss = criterion(output, target)\n                \n                test_loss += loss.item()\n                _, predicted = torch.max(output.data, 1)\n                test_total += target.size(0)\n                test_correct += (predicted == target).sum().item()\n                \n                predictions.extend(predicted.cpu().numpy())\n                targets.extend(target.cpu().numpy())\n        \n        avg_test_loss = test_loss / len(loader)\n        test_accuracy = test_correct / test_total\n        \n        test_results[name] = {\n            'loss': avg_test_loss,\n            'accuracy': test_accuracy\n        }\n        all_predictions[name] = np.array(predictions)\n        all_targets[name] = np.array(targets)\n        \n        print(f\"{name:<10} | Loss: {avg_test_loss:.4f} | Acc: {test_accuracy*100:.2f}%\")\n\n    # Visualization 1: Accuracy comparison across all datasets\n    plt.figure(figsize=(12, 6))\n    dataset_names = list(test_results.keys())\n    accuracies = [test_results[name]['accuracy'] * 100 for name in dataset_names]\n\n    plt.bar(dataset_names, accuracies, color='steelblue', edgecolor='black')\n    plt.xlabel('Dataset', fontsize=12, fontweight='bold')\n    plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n    plt.title('Model Accuracy on All Test Datasets', fontsize=14, fontweight='bold')\n    plt.ylim(0, 100)\n    plt.grid(axis='y', alpha=0.3)\n\n    for i, (name, acc) in enumerate(zip(dataset_names, accuracies)):\n        plt.text(i, acc + 1, f'{acc:.2f}%', ha='center', fontweight='bold')\n\n    plt.tight_layout()\n    plt.savefig('accuracy_comparison.png', dpi=150, bbox_inches='tight')\n    print(\"Saved accuracy_comparison.png\")\n    # plt.show() # Skipped for non-interactive env\n\n    # Visualization 2: Training and Validation Accuracy vs Epoch\n    if len(train_acc_history) > 0:\n        plt.figure(figsize=(12, 5))\n\n        plt.subplot(1, 2, 1)\n        epochs_range = range(1, len(train_acc_history) + 1)\n        plt.plot(epochs_range, [acc * 100 for acc in train_acc_history], 'b-o', label='Training Accuracy', linewidth=2)\n        plt.plot(epochs_range, [acc * 100 for acc in val_acc_history], 'r-s', label='Validation Accuracy', linewidth=2)\n        plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n        plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n        plt.title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')\n        plt.legend(fontsize=10)\n        plt.grid(True, alpha=0.3)\n\n        plt.subplot(1, 2, 2)\n        plt.plot(epochs_range, train_loss_history, 'b-o', label='Training Loss', linewidth=2)\n        plt.plot(epochs_range, val_loss_history, 'r-s', label='Validation Loss', linewidth=2)\n        plt.xlabel('Epoch', fontsize=12, fontweight='bold')\n        plt.ylabel('Loss', fontsize=12, fontweight='bold')\n        plt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n        plt.legend(fontsize=10)\n        plt.grid(True, alpha=0.3)\n\n        plt.tight_layout()\n        plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n        print(\"Saved training_curves.png\")\n        # plt.show()\n\n    # Visualization 3: Confusion Matrix for each dataset\n    num_datasets = len(test_results)\n    cols = min(3, num_datasets)\n    rows = (num_datasets + cols - 1) // cols\n\n    fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows))\n    if num_datasets == 1:\n        axes = [axes]\n    else:\n        axes = axes.flatten() if num_datasets > 1 else [axes]\n\n    for idx, name in enumerate(dataset_names):\n        cm = confusion_matrix(all_targets[name], all_predictions[name])\n        \n        ax = axes[idx] if num_datasets > 1 else axes[0]\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n                    cbar_kws={'label': 'Count'}, square=True)\n        ax.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n        ax.set_ylabel('True Label', fontsize=11, fontweight='bold')\n        ax.set_title(f'Confusion Matrix - {name}\\nAccuracy: {test_results[name][\"accuracy\"]*100:.2f}%', \n                     fontsize=12, fontweight='bold')\n\n    # Hide extra subplots\n    for idx in range(num_datasets, len(axes)):\n        axes[idx].axis('off')\n\n    plt.tight_layout()\n    plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n    print(\"Saved confusion_matrices.png\")\n    # plt.show()\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"EVALUATION SUMMARY\")\n    print(\"=\"*50)\n    for name in dataset_names:\n        print(f\"{name:15s}: {test_results[name]['accuracy']*100:6.2f}%\")\n    print(\"=\"*50)\n\n# ==============================================================================\n# Training Loop\n# ==============================================================================\ndef train(config):\n    torch.manual_seed(config.SEED)\n    \n    # Load Data\n    ds_env1, ds_env2 = load_data(config)\n    test_loaders = get_test_loaders(config)\n    \n    # Use gr95z (Inverse) as validation set for curves if available, else gr95z_hard or similar\n    # Defaulting to gr95z for OOD validation\n    val_loader = test_loaders.get('gr95z')\n    \n    # Loaders\n    loader1 = DataLoader(ds_env1, batch_size=config.BATCH_SIZE, shuffle=True)\n    loader2 = DataLoader(ds_env2, batch_size=config.BATCH_SIZE, shuffle=True)\n    \n    # Env 2 is smaller (5%), so we cycle it to match Env 1 iterations\n    iter2 = cycle(loader2)\n    \n    # Model & Optim\n    model = CNN3Layer().to(config.DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n    \n    print(f\"\\n[Train] Starting IRM Training on {config.DEVICE}...\")\n    print(f\"  Penalty: {config.IRM_PENALTY_WEIGHT} (Annealed for {config.IRM_ANNEAL_EPOCHS} epochs)\")\n    \n    # Metrics history\n    train_acc_history = []\n    val_acc_history = []\n    train_loss_history = []\n    val_loss_history = []\n    \n    for epoch in range(config.EPOCHS):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for x1, y1 in loader1:\n            x2, y2 = next(iter2)\n            \n            x1, y1 = x1.to(config.DEVICE), y1.to(config.DEVICE)\n            x2, y2 = x2.to(config.DEVICE), y2.to(config.DEVICE)\n            \n            # Forward\n            logits1 = model(x1)\n            logits2 = model(x2)\n            \n            # 1. Standard ERM Risk (Cross Entropy)\n            nll1 = nn.CrossEntropyLoss()(logits1, y1)\n            nll2 = nn.CrossEntropyLoss()(logits2, y2)\n            risk = (nll1 + nll2) / 2\n            \n            # 2. Invariance Penalty\n            penalty = torch.tensor(0.).to(config.DEVICE)\n            if epoch >= config.IRM_ANNEAL_EPOCHS:\n                p1 = irm_penalty(logits1, y1)\n                p2 = irm_penalty(logits2, y2)\n                penalty = (p1 + p2) / 2\n            \n            # Total Loss\n            loss = risk + (config.IRM_PENALTY_WEIGHT if epoch >= config.IRM_ANNEAL_EPOCHS else 1.0) * penalty\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            # Stats for Training Curve\n            total_loss += loss.item() # This includes penalty! For curves, we might want just risk, but loss is fine.\n            _, preds = torch.max(logits1, 1) # Acc on env1\n            correct += (preds == y1).sum().item()\n            total += y1.size(0)\n            \n        # End of Epoch Stats\n        epoch_train_loss = total_loss / len(loader1)\n        epoch_train_acc = correct / total\n        \n        train_loss_history.append(epoch_train_loss)\n        train_acc_history.append(epoch_train_acc)\n        \n        # Validation Stats\n        val_loss = 0.0\n        val_acc = 0.0\n        if val_loader:\n            model.eval()\n            v_loss = 0\n            v_correct = 0\n            v_total = 0\n            with torch.no_grad():\n                for vx, vy in val_loader:\n                    vx, vy = vx.to(config.DEVICE), vy.to(config.DEVICE)\n                    vout = model(vx)\n                    v_loss += nn.CrossEntropyLoss()(vout, vy).item()\n                    _, vpred = torch.max(vout, 1)\n                    v_correct += (vpred == vy).sum().item()\n                    v_total += vy.size(0)\n            val_loss = v_loss / len(val_loader)\n            val_acc = v_correct / v_total\n            model.train()\n        \n        val_loss_history.append(val_loss)\n        val_acc_history.append(val_acc)\n        \n        print(f\"Epoch [{epoch+1}/{config.EPOCHS}] Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc*100:.1f}% | Val Acc (OOD): {val_acc*100:.1f}%\")\n\n    # Final Save\n    save_path = \"task4_irm_model.pth\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"\\n[Done] Model saved to {save_path}\")\n    \n    # User Visualizations\n    evaluate_and_plot(model, test_loaders, train_acc_history, val_acc_history, train_loss_history, val_loss_history, config.DEVICE)\n\nif __name__ == \"__main__\":\n    train(Config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T13:56:55.294101Z","iopub.execute_input":"2026-02-09T13:56:55.294873Z","iopub.status.idle":"2026-02-09T13:57:11.154173Z","shell.execute_reply.started":"2026-02-09T13:56:55.294831Z","shell.execute_reply":"2026-02-09T13:57:11.153249Z"}},"outputs":[],"execution_count":null}]}