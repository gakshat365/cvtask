{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4f0f5d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-08T05:54:32.957674Z",
     "iopub.status.busy": "2026-02-08T05:54:32.957436Z",
     "iopub.status.idle": "2026-02-08T05:58:10.488057Z",
     "shell.execute_reply": "2026-02-08T05:58:10.487324Z"
    },
    "papermill": {
     "duration": 217.535317,
     "end_time": "2026-02-08T05:58:10.489645",
     "exception": false,
     "start_time": "2026-02-08T05:54:32.954328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Data] Loading from /kaggle/input/cmnistneo1...\n",
      "[Data] Analyzing bias...\n",
      "  Env 1 (Aligned/Biased): 57948 samples\n",
      "  Env 2 (Conflict/OOD):   2052 samples\n",
      "\n",
      "[Train] Starting IRM Training on cuda...\n",
      "  Penalty: 10000.0 (Annealed for 10 epochs)\n",
      "Epoch [1/50] Risk: 0.5146 | Penalty: 0.000000 | Train Acc (Env1): 88.1%\n",
      "Epoch [2/50] Risk: 0.0805 | Penalty: 0.000000 | Train Acc (Env1): 98.1%\n",
      "Epoch [3/50] Risk: 0.0382 | Penalty: 0.000000 | Train Acc (Env1): 98.9%\n",
      "Epoch [4/50] Risk: 0.0225 | Penalty: 0.000000 | Train Acc (Env1): 99.3%\n",
      "Epoch [5/50] Risk: 0.0161 | Penalty: 0.000000 | Train Acc (Env1): 99.4%\n",
      "  >>> Hard Test Acc (OOD): 94.48%\n",
      "Epoch [6/50] Risk: 0.0130 | Penalty: 0.000000 | Train Acc (Env1): 99.5%\n",
      "Epoch [7/50] Risk: 0.0100 | Penalty: 0.000000 | Train Acc (Env1): 99.6%\n",
      "Epoch [8/50] Risk: 0.0089 | Penalty: 0.000000 | Train Acc (Env1): 99.7%\n",
      "Epoch [9/50] Risk: 0.0075 | Penalty: 0.000000 | Train Acc (Env1): 99.7%\n",
      "Epoch [10/50] Risk: 0.0071 | Penalty: 0.000000 | Train Acc (Env1): 99.7%\n",
      "  >>> Hard Test Acc (OOD): 88.17%\n",
      "Epoch [11/50] Risk: 1.6667 | Penalty: 0.201999 | Train Acc (Env1): 40.4%\n",
      "Epoch [12/50] Risk: 1.7853 | Penalty: 0.002661 | Train Acc (Env1): 27.3%\n",
      "Epoch [13/50] Risk: 1.8217 | Penalty: 0.001444 | Train Acc (Env1): 25.1%\n",
      "Epoch [14/50] Risk: 1.8619 | Penalty: 0.001662 | Train Acc (Env1): 23.0%\n",
      "Epoch [15/50] Risk: 1.9088 | Penalty: 0.001216 | Train Acc (Env1): 18.1%\n",
      "  >>> Hard Test Acc (OOD): 20.21%\n",
      "Epoch [16/50] Risk: 1.9369 | Penalty: 0.000861 | Train Acc (Env1): 14.0%\n",
      "Epoch [17/50] Risk: 1.9785 | Penalty: 0.000697 | Train Acc (Env1): 9.9%\n",
      "Epoch [18/50] Risk: 2.0010 | Penalty: 0.000671 | Train Acc (Env1): 7.5%\n",
      "Epoch [19/50] Risk: 2.0230 | Penalty: 0.000574 | Train Acc (Env1): 5.8%\n",
      "Epoch [20/50] Risk: 2.0456 | Penalty: 0.000495 | Train Acc (Env1): 5.2%\n",
      "  >>> Hard Test Acc (OOD): 7.61%\n",
      "Epoch [21/50] Risk: 2.0591 | Penalty: 0.000361 | Train Acc (Env1): 4.7%\n",
      "Epoch [22/50] Risk: 2.0779 | Penalty: 0.000385 | Train Acc (Env1): 4.4%\n",
      "Epoch [23/50] Risk: 2.0954 | Penalty: 0.000371 | Train Acc (Env1): 4.1%\n",
      "Epoch [24/50] Risk: 2.1158 | Penalty: 0.000366 | Train Acc (Env1): 3.6%\n",
      "Epoch [25/50] Risk: 2.1397 | Penalty: 0.000373 | Train Acc (Env1): 3.2%\n",
      "  >>> Hard Test Acc (OOD): 3.46%\n",
      "Epoch [26/50] Risk: 2.1500 | Penalty: 0.000218 | Train Acc (Env1): 3.1%\n",
      "Epoch [27/50] Risk: 2.1729 | Penalty: 0.000208 | Train Acc (Env1): 3.2%\n",
      "Epoch [28/50] Risk: 2.1887 | Penalty: 0.000192 | Train Acc (Env1): 2.7%\n",
      "Epoch [29/50] Risk: 2.2080 | Penalty: 0.000208 | Train Acc (Env1): 2.6%\n",
      "Epoch [30/50] Risk: 2.2257 | Penalty: 0.000109 | Train Acc (Env1): 2.4%\n",
      "  >>> Hard Test Acc (OOD): 2.52%\n",
      "Epoch [31/50] Risk: 2.2366 | Penalty: 0.000088 | Train Acc (Env1): 2.5%\n",
      "Epoch [32/50] Risk: 2.2477 | Penalty: 0.000075 | Train Acc (Env1): 2.5%\n",
      "Epoch [33/50] Risk: 2.2600 | Penalty: 0.000053 | Train Acc (Env1): 2.7%\n",
      "Epoch [34/50] Risk: 2.2674 | Penalty: 0.000044 | Train Acc (Env1): 2.9%\n",
      "Epoch [35/50] Risk: 2.2740 | Penalty: 0.000031 | Train Acc (Env1): 3.3%\n",
      "  >>> Hard Test Acc (OOD): 4.32%\n",
      "Epoch [36/50] Risk: 2.2790 | Penalty: 0.000025 | Train Acc (Env1): 3.5%\n",
      "Epoch [37/50] Risk: 2.2829 | Penalty: 0.000018 | Train Acc (Env1): 4.0%\n",
      "Epoch [38/50] Risk: 2.2874 | Penalty: 0.000017 | Train Acc (Env1): 4.8%\n",
      "Epoch [39/50] Risk: 2.2901 | Penalty: 0.000009 | Train Acc (Env1): 5.4%\n",
      "Epoch [40/50] Risk: 2.2908 | Penalty: 0.000009 | Train Acc (Env1): 6.1%\n",
      "  >>> Hard Test Acc (OOD): 7.98%\n",
      "Epoch [41/50] Risk: 2.2921 | Penalty: 0.000008 | Train Acc (Env1): 7.1%\n",
      "Epoch [42/50] Risk: 2.2931 | Penalty: 0.000007 | Train Acc (Env1): 10.0%\n",
      "Epoch [43/50] Risk: 2.2938 | Penalty: 0.000006 | Train Acc (Env1): 10.4%\n",
      "Epoch [44/50] Risk: 2.2952 | Penalty: 0.000005 | Train Acc (Env1): 9.9%\n",
      "Epoch [45/50] Risk: 2.2953 | Penalty: 0.000004 | Train Acc (Env1): 4.9%\n",
      "  >>> Hard Test Acc (OOD): 3.92%\n",
      "Epoch [46/50] Risk: 2.2962 | Penalty: 0.000005 | Train Acc (Env1): 3.4%\n",
      "Epoch [47/50] Risk: 2.2971 | Penalty: 0.000003 | Train Acc (Env1): 3.4%\n",
      "Epoch [48/50] Risk: 2.2971 | Penalty: 0.000003 | Train Acc (Env1): 3.4%\n",
      "Epoch [49/50] Risk: 2.2980 | Penalty: 0.000003 | Train Acc (Env1): 3.8%\n",
      "Epoch [50/50] Risk: 2.2976 | Penalty: 0.000002 | Train Acc (Env1): 3.6%\n",
      "  >>> Hard Test Acc (OOD): 4.10%\n",
      "\n",
      "[Done] Model saved to task4_irm_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import argparse\n",
    "from itertools import cycle\n",
    "\n",
    "# ==============================================================================\n",
    "# Configuration & Hyperparameters\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    # Data Paths\n",
    "    DATA_DIR = r\"/kaggle/input/cmnistneo1\"\n",
    "    TRAIN_FILE = \"train_data_rg95z.npz\"\n",
    "    TEST_FILE = \"test_data_gr95z.npz\"\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    BATCH_SIZE = 256\n",
    "    EPOCHS = 50\n",
    "    LR = 1e-3\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    \n",
    "    # IRM Specifics\n",
    "    IRM_PENALTY_WEIGHT = 10000.0\n",
    "    IRM_ANNEAL_EPOCHS = 10  # Epochs before penalty kicks in\n",
    "    \n",
    "    # System\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    SEED = 42\n",
    "\n",
    "# ==============================================================================\n",
    "# Model Architecture\n",
    "# ==============================================================================\n",
    "class CNN3Layer(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN3Layer, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 3 * 3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ==============================================================================\n",
    "# Utils: IRM Penalty & Data Loading\n",
    "# ==============================================================================\n",
    "def irm_penalty(logits, y):\n",
    "    \"\"\"\n",
    "    Computes the IRM v1 penalty: gradient norm of the loss w.r.t a fixed scalar 1.0.\n",
    "    This effectively asks: \"If I multiplied the classifier output by a scalar 'w',\n",
    "    would the optimal 'w' result in 0 gradient at w=1.0 across all environments?\"\n",
    "    \"\"\"\n",
    "    scale = torch.tensor(1.).to(logits.device).requires_grad_()\n",
    "    loss = nn.CrossEntropyLoss()(logits * scale, y)\n",
    "    grad = torch.autograd.grad(loss, [scale], create_graph=True)[0]\n",
    "    return torch.sum(grad**2)\n",
    "\n",
    "def get_dominant_color(img_tensor):\n",
    "    \"\"\"Returns dominant channel index (0=R, 1=G, 2=B) for a single image tensor.\"\"\"\n",
    "    means = torch.mean(img_tensor, dim=(1, 2))\n",
    "    return torch.argmax(means).item()\n",
    "\n",
    "def load_data(config):\n",
    "    \"\"\"Loads data and splits training set into two IRM environments.\"\"\"\n",
    "    print(f\"\\n[Data] Loading from {config.DATA_DIR}...\")\n",
    "    \n",
    "    train_path = os.path.join(config.DATA_DIR, config.TRAIN_FILE)\n",
    "    if not os.path.exists(train_path):\n",
    "        raise FileNotFoundError(f\"Train file not found: {train_path}\")\n",
    "\n",
    "    # Load Train (Biased)\n",
    "    train_data = np.load(train_path)\n",
    "    X_train = torch.tensor(train_data['images'].astype('float32') / 255.0).permute(0, 3, 1, 2)\n",
    "    y_train = torch.tensor(train_data['labels']).long()\n",
    "\n",
    "    # 1. Identify Spurious Correlation (Color Bias)\n",
    "    print(\"[Data] Analyzing bias...\")\n",
    "    digit_bias_color = {}\n",
    "    for d in range(10):\n",
    "        # Sample subset to determine majority color\n",
    "        indices = (y_train == d).nonzero(as_tuple=True)[0][:100]\n",
    "        colors = [get_dominant_color(X_train[i]) for i in indices]\n",
    "        majority = max(set(colors), key=colors.count)\n",
    "        digit_bias_color[d] = majority\n",
    "    \n",
    "    # 2. Split into Environments (Aligned vs Conflict)\n",
    "    # Calculate dominant color for entire batch efficiently\n",
    "    img_means = torch.mean(X_train, dim=(2, 3)) # (N, 3)\n",
    "    img_colors = torch.argmax(img_means, dim=1) # (N,)\n",
    "    \n",
    "    # Expected color based on label\n",
    "    expected_colors = torch.tensor([digit_bias_color[y.item()] for y in y_train], device=X_train.device)\n",
    "    \n",
    "    # Mask: True if image follows bias (Environment 1), False if conflict (Environment 2)\n",
    "    aligned_mask = (img_colors == expected_colors.cpu())\n",
    "    \n",
    "    env1_idx = torch.nonzero(aligned_mask, as_tuple=True)[0]\n",
    "    env2_idx = torch.nonzero(~aligned_mask, as_tuple=True)[0]\n",
    "    \n",
    "    print(f\"  Env 1 (Aligned/Biased): {len(env1_idx)} samples\")\n",
    "    print(f\"  Env 2 (Conflict/OOD):   {len(env2_idx)} samples\")\n",
    "\n",
    "    # Load Test (Hard OOD)\n",
    "    test_path = os.path.join(config.DATA_DIR, config.TEST_FILE)\n",
    "    test_data = np.load(test_path)\n",
    "    X_test = torch.tensor(test_data['images'].astype('float32') / 255.0).permute(0, 3, 1, 2)\n",
    "    y_test = torch.tensor(test_data['labels']).long()\n",
    "    \n",
    "    # Create Datasets\n",
    "    ds_env1 = TensorDataset(X_train[env1_idx], y_train[env1_idx])\n",
    "    ds_env2 = TensorDataset(X_train[env2_idx], y_train[env2_idx])\n",
    "    ds_test = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    return ds_env1, ds_env2, ds_test\n",
    "\n",
    "# ==============================================================================\n",
    "# Training Loop\n",
    "# ==============================================================================\n",
    "def train(config):\n",
    "    torch.manual_seed(config.SEED)\n",
    "    \n",
    "    # Load Data\n",
    "    ds_env1, ds_env2, ds_test = load_data(config)\n",
    "    \n",
    "    # Loaders\n",
    "    loader1 = DataLoader(ds_env1, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    loader2 = DataLoader(ds_env2, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    loader_test = DataLoader(ds_test, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Env 2 is smaller (5%), so we cycle it to match Env 1 iterations\n",
    "    iter2 = cycle(loader2)\n",
    "    \n",
    "    # Model & Optim\n",
    "    model = CNN3Layer().to(config.DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n",
    "    \n",
    "    print(f\"\\n[Train] Starting IRM Training on {config.DEVICE}...\")\n",
    "    print(f\"  Penalty: {config.IRM_PENALTY_WEIGHT} (Annealed for {config.IRM_ANNEAL_EPOCHS} epochs)\")\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_penalty = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x1, y1 in loader1:\n",
    "            x2, y2 = next(iter2)\n",
    "            \n",
    "            x1, y1 = x1.to(config.DEVICE), y1.to(config.DEVICE)\n",
    "            x2, y2 = x2.to(config.DEVICE), y2.to(config.DEVICE)\n",
    "            \n",
    "            # Forward\n",
    "            logits1 = model(x1)\n",
    "            logits2 = model(x2)\n",
    "            \n",
    "            # 1. Standard ERM Risk (Cross Entropy)\n",
    "            nll1 = nn.CrossEntropyLoss()(logits1, y1)\n",
    "            nll2 = nn.CrossEntropyLoss()(logits2, y2)\n",
    "            risk = (nll1 + nll2) / 2\n",
    "            \n",
    "            # 2. Invariance Penalty\n",
    "            penalty = torch.tensor(0.).to(config.DEVICE)\n",
    "            if epoch >= config.IRM_ANNEAL_EPOCHS:\n",
    "                p1 = irm_penalty(logits1, y1)\n",
    "                p2 = irm_penalty(logits2, y2)\n",
    "                penalty = (p1 + p2) / 2\n",
    "            \n",
    "            # Total Loss\n",
    "            loss = risk + (config.IRM_PENALTY_WEIGHT if epoch >= config.IRM_ANNEAL_EPOCHS else 1.0) * penalty\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Stats\n",
    "            total_loss += risk.item()\n",
    "            total_penalty += penalty.item()\n",
    "            _, preds = torch.max(logits1, 1)\n",
    "            correct += (preds == y1).sum().item()\n",
    "            total += y1.size(0)\n",
    "            \n",
    "        # Logging\n",
    "        avg_risk = total_loss / len(loader1)\n",
    "        avg_penalty = total_penalty / len(loader1)\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{config.EPOCHS}] Risk: {avg_risk:.4f} | Penalty: {avg_penalty:.6f} | Train Acc (Env1): {train_acc:.1f}%\")\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            val_acc = evaluate(model, loader_test, config.DEVICE)\n",
    "            print(f\"  >>> Hard Test Acc (OOD): {val_acc:.2f}%\")\n",
    "\n",
    "    # Final Save\n",
    "    save_path = \"task4_irm_model.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"\\n[Done] Model saved to {save_path}\")\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train(Config)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9434511,
     "sourceId": 14760510,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 580771,
     "modelInstanceId": 568438,
     "sourceId": 744672,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 224.947034,
   "end_time": "2026-02-08T05:58:14.179785",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-08T05:54:29.232751",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
