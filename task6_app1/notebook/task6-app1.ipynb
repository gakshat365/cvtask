{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14760510,"sourceType":"datasetVersion","datasetId":9434511},{"sourceId":745111,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":568438,"modelId":580771}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Task 6: The Decomposition - Sparse Autoencoders for Feature Discovery\n# ======================================================================\n# This script trains Sparse Autoencoders (SAEs) to decompose the intermediate\n# hidden states of our biased CNN into an overcomplete representation.\n# We'll explore whether we can find meaningful features, especially color-related ones.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# ===== CONFIGURATION =====\n# Data paths - UPDATE THESE TO YOUR ACTUAL DATA LOCATIONS\n# For Kaggle, use paths like: '/kaggle/input/cmnistneo1/train_data_rg95z.npz'\n# For local, provide the full path to your training data\n\n# You may need to download/copy the training data from Kaggle\n# If you have training data at a different location, update this path:\nTRAIN_DATA_PATH = '/kaggle/input/cmnistneo1/train_data_rg95z.npz'  # Using test data if training data unavailable\nTEST_DATA_PATH = '/kaggle/input/cmnistneo1/test_data_gr95z.npz'  # Test data with different color mapping\n\n# Model path (your trained biased model)\nMODEL_PATH = '/kaggle/input/task1app3models/pytorch/default/2/task1approach3sc1_modelv1.pth'\n\n# SAE hyperparameters\nSAE_EXPANSION_FACTOR = 4  # Overcomplete representation multiplier\nSAE_SPARSITY_WEIGHT = 0.001  # L1 penalty for sparsity\nSAE_LEARNING_RATE = 0.001\nSAE_EPOCHS = 50\nSAE_BATCH_SIZE = 256\n\n# Model hyperparameters (from your biased model)\nNUM_CLASSES = 10\nCONV1_CHANNELS = 32\nCONV2_CHANNELS = 64\nCONV3_CHANNELS = 64\nFC1_UNITS = 128\nDROPOUT_RATE = 0.1\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# ===== MODEL DEFINITION =====\n# Same CNN architecture as in task1_app3\nclass CNN3Layer(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super(CNN3Layer, self).__init__()\n        \n        # First Convolutional Layer\n        self.conv1 = nn.Conv2d(3, CONV1_CHANNELS, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        \n        # Second Convolutional Layer\n        self.conv2 = nn.Conv2d(CONV1_CHANNELS, CONV2_CHANNELS, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        \n        # Third Convolutional Layer\n        self.conv3 = nn.Conv2d(CONV2_CHANNELS, CONV3_CHANNELS, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(2, 2)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(CONV3_CHANNELS * 3 * 3, FC1_UNITS)\n        self.dropout = nn.Dropout(DROPOUT_RATE)\n        self.fc2 = nn.Linear(FC1_UNITS, num_classes)\n    \n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = self.pool3(F.relu(self.conv3(x)))\n        x = x.reshape(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n    \n    def get_intermediate_activations(self, x):\n        \"\"\"Extract intermediate activations from various layers\"\"\"\n        activations = {}\n        \n        # Conv1 output (before pooling)\n        conv1_out = F.relu(self.conv1(x))\n        activations['conv1'] = conv1_out.clone()\n        x = self.pool1(conv1_out)\n        \n        # Conv2 output (before pooling)\n        conv2_out = F.relu(self.conv2(x))\n        activations['conv2'] = conv2_out.clone()\n        x = self.pool2(conv2_out)\n        \n        # Conv3 output (before pooling)\n        conv3_out = F.relu(self.conv3(x))\n        activations['conv3'] = conv3_out.clone()\n        x = self.pool3(conv3_out)\n        \n        # Flattened output\n        x_flat = x.reshape(x.size(0), -1)\n        activations['flattened'] = x_flat.clone()\n        \n        # FC1 output\n        fc1_out = F.relu(self.fc1(x_flat))\n        activations['fc1'] = fc1_out.clone()\n        \n        return activations\n\n\n# ===== SPARSE AUTOENCODER DEFINITION =====\nclass SparseAutoencoder(nn.Module):\n    \"\"\"\n    Sparse Autoencoder for decomposing hidden states into overcomplete representation.\n    Uses L1 regularization to encourage sparsity in the latent space.\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim):\n        super(SparseAutoencoder, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        \n        # Encoder: maps to overcomplete representation\n        self.encoder = nn.Linear(input_dim, hidden_dim)\n        \n        # Decoder: reconstructs original representation\n        self.decoder = nn.Linear(hidden_dim, input_dim)\n    \n    def encode(self, x):\n        \"\"\"Encode input to sparse latent representation\"\"\"\n        return F.relu(self.encoder(x))\n    \n    def decode(self, z):\n        \"\"\"Decode latent representation back to input space\"\"\"\n        return self.decoder(z)\n    \n    def forward(self, x):\n        z = self.encode(x)\n        x_reconstructed = self.decode(z)\n        return x_reconstructed, z\n\n\n# ===== HELPER FUNCTIONS =====\ndef load_data(path):\n    \"\"\"Load and preprocess data from npz file\"\"\"\n    if not os.path.exists(path):\n        print(f\"Warning: {path} not found. Please provide the correct path.\")\n        return None, None\n    data = np.load(path)\n    images = data['images'].astype('float32') / 255.0\n    labels = data['labels']\n    return images, labels\n\n\ndef extract_activations(model, data_loader, layer_name='fc1'):\n    \"\"\"Extract activations from a specific layer for all samples\"\"\"\n    model.eval()\n    all_activations = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for data, labels in data_loader:\n            data = data.to(device)\n            activations = model.get_intermediate_activations(data)\n            \n            # Flatten spatial activations for conv layers\n            act = activations[layer_name]\n            if len(act.shape) > 2:  # Conv layers have shape (B, C, H, W)\n                act = act.reshape(act.size(0), -1)\n            \n            all_activations.append(act.cpu())\n            all_labels.append(labels)\n    \n    return torch.cat(all_activations, dim=0), torch.cat(all_labels, dim=0)\n\n\ndef train_sae(sae, activations, epochs=SAE_EPOCHS, sparsity_weight=SAE_SPARSITY_WEIGHT):\n    \"\"\"Train the Sparse Autoencoder\"\"\"\n    sae.train()\n    optimizer = optim.Adam(sae.parameters(), lr=SAE_LEARNING_RATE)\n    \n    # Create data loader\n    dataset = TensorDataset(activations)\n    loader = DataLoader(dataset, batch_size=SAE_BATCH_SIZE, shuffle=True)\n    \n    losses = []\n    \n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        epoch_recon_loss = 0.0\n        epoch_sparsity_loss = 0.0\n        \n        for batch in loader:\n            x = batch[0].to(device)\n            \n            # Forward pass\n            x_recon, z = sae(x)\n            \n            # Reconstruction loss\n            recon_loss = F.mse_loss(x_recon, x)\n            \n            # Sparsity loss (L1 penalty on latent activations)\n            sparsity_loss = torch.mean(torch.abs(z))\n            \n            # Total loss\n            loss = recon_loss + sparsity_weight * sparsity_loss\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            epoch_recon_loss += recon_loss.item()\n            epoch_sparsity_loss += sparsity_loss.item()\n        \n        avg_loss = epoch_loss / len(loader)\n        avg_recon = epoch_recon_loss / len(loader)\n        avg_sparsity = epoch_sparsity_loss / len(loader)\n        losses.append(avg_loss)\n        \n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch [{epoch+1}/{epochs}] - Total Loss: {avg_loss:.6f}, \"\n                  f\"Recon: {avg_recon:.6f}, Sparsity: {avg_sparsity:.6f}\")\n    \n    return losses\n\n\ndef analyze_features(sae, activations, labels, images, top_k=5):\n    \"\"\"Analyze learned features by finding samples that maximally activate each feature\"\"\"\n    sae.eval()\n    \n    with torch.no_grad():\n        z = sae.encode(activations.to(device))\n        z = z.cpu().numpy()\n    \n    labels = labels.numpy()\n    num_features = z.shape[1]\n    \n    # Find top activating samples for each feature\n    feature_analysis = []\n    \n    for feat_idx in range(min(50, num_features)):  # Analyze first 50 features\n        # Get activation values for this feature across all samples\n        feat_activations = z[:, feat_idx]\n        \n        # Find top-k samples that maximally activate this feature\n        top_indices = np.argsort(feat_activations)[-top_k:][::-1]\n        \n        # Get labels and mean activation for these samples\n        top_labels = labels[top_indices]\n        mean_activation = feat_activations[top_indices].mean()\n        \n        # Compute label distribution for top activating samples\n        label_counts = np.bincount(top_labels, minlength=10)\n        dominant_label = np.argmax(label_counts)\n        \n        feature_analysis.append({\n            'feature_idx': feat_idx,\n            'mean_activation': mean_activation,\n            'top_labels': top_labels,\n            'dominant_label': dominant_label,\n            'label_counts': label_counts,\n            'top_indices': top_indices\n        })\n    \n    return feature_analysis\n\n\ndef visualize_feature(images, labels, feature_analysis, feature_idx, save_path=None):\n    \"\"\"Visualize samples that maximally activate a specific feature\"\"\"\n    feat_info = None\n    for fa in feature_analysis:\n        if fa['feature_idx'] == feature_idx:\n            feat_info = fa\n            break\n    \n    if feat_info is None:\n        print(f\"Feature {feature_idx} not found in analysis\")\n        return\n    \n    top_indices = feat_info['top_indices']\n    \n    fig, axes = plt.subplots(1, len(top_indices), figsize=(15, 3))\n    fig.suptitle(f\"Feature {feature_idx} - Dominant Label: {feat_info['dominant_label']}\", fontsize=14)\n    \n    for i, idx in enumerate(top_indices):\n        ax = axes[i] if len(top_indices) > 1 else axes\n        img = images[idx]\n        label = labels[idx]\n        ax.imshow(img)\n        ax.set_title(f\"Label: {label}\")\n        ax.axis('off')\n    \n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n\n\ndef intervention_experiment(model, sae, images, labels, feature_idx, scale_factors=[0.0, 0.5, 2.0, 5.0]):\n    \"\"\"\n    Experiment with interventions by modifying specific feature activations.\n    This tests if dialing up/down features changes model predictions.\n    \"\"\"\n    model.eval()\n    sae.eval()\n    \n    # Get a batch of images\n    batch_size = min(100, len(images))\n    sample_indices = np.random.choice(len(images), batch_size, replace=False)\n    sample_images = torch.FloatTensor(images[sample_indices]).permute(0, 3, 1, 2).to(device)\n    sample_labels = labels[sample_indices]\n    \n    print(f\"\\n=== Intervention Experiment on Feature {feature_idx} ===\")\n    \n    for scale in scale_factors:\n        # Get original activations\n        with torch.no_grad():\n            activations = model.get_intermediate_activations(sample_images)\n            fc1_act = activations['fc1']  # Shape: (B, 128)\n            \n            # Encode to SAE space\n            z = sae.encode(fc1_act)  # Shape: (B, hidden_dim)\n            \n            # Modify the specific feature\n            z_modified = z.clone()\n            z_modified[:, feature_idx] = z_modified[:, feature_idx] * scale\n            \n            # Decode back\n            fc1_modified = sae.decode(z_modified)\n            \n            # Continue forward pass with modified activations\n            fc1_modified = F.relu(fc1_modified)  # Apply ReLU\n            output = model.fc2(fc1_modified)\n            \n            # Get predictions\n            predictions = torch.argmax(output, dim=1).cpu().numpy()\n            \n            # Calculate accuracy\n            accuracy = (predictions == sample_labels).mean() * 100\n            \n            # Calculate prediction distribution\n            pred_counts = np.bincount(predictions, minlength=10)\n            \n            print(f\"\\nScale Factor: {scale}\")\n            print(f\"  Accuracy: {accuracy:.2f}%\")\n            print(f\"  Prediction Distribution: {pred_counts}\")\n\n\ndef analyze_color_features(model, sae, train_images, train_labels, test_images, test_labels):\n    \"\"\"\n    Analyze whether SAE features are sensitive to color vs. shape.\n    Uses training data (biased colors) and test data (reversed colors) to compare.\n    \"\"\"\n    model.eval()\n    sae.eval()\n    \n    # Create data loaders\n    train_tensor = torch.FloatTensor(train_images).permute(0, 3, 1, 2)\n    test_tensor = torch.FloatTensor(test_images).permute(0, 3, 1, 2)\n    \n    train_dataset = TensorDataset(train_tensor, torch.LongTensor(train_labels))\n    test_dataset = TensorDataset(test_tensor, torch.LongTensor(test_labels))\n    \n    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n    \n    # Extract activations\n    print(\"Extracting training activations...\")\n    train_acts, train_labs = extract_activations(model, train_loader, 'fc1')\n    print(\"Extracting test activations...\")\n    test_acts, test_labs = extract_activations(model, test_loader, 'fc1')\n    \n    # Get SAE encodings\n    with torch.no_grad():\n        train_z = sae.encode(train_acts.to(device)).cpu().numpy()\n        test_z = sae.encode(test_acts.to(device)).cpu().numpy()\n    \n    # Compute mean activations per class for each dataset\n    num_features = train_z.shape[1]\n    train_class_means = np.zeros((10, num_features))\n    test_class_means = np.zeros((10, num_features))\n    \n    for c in range(10):\n        train_mask = train_labs.numpy() == c\n        test_mask = test_labs.numpy() == c\n        \n        if train_mask.sum() > 0:\n            train_class_means[c] = train_z[train_mask].mean(axis=0)\n        if test_mask.sum() > 0:\n            test_class_means[c] = test_z[test_mask].mean(axis=0)\n    \n    # Find features that differ significantly between train and test\n    # These might be color-sensitive features\n    feature_diffs = np.abs(train_class_means - test_class_means).mean(axis=0)\n    \n    # Get top color-sensitive features\n    color_sensitive_features = np.argsort(feature_diffs)[-20:][::-1]\n    \n    # Get features that are consistent (shape-sensitive)\n    shape_sensitive_features = np.argsort(feature_diffs)[:20]\n    \n    print(\"\\n=== Feature Analysis Results ===\")\n    print(f\"\\nTop 20 Color-Sensitive Features (high train/test difference):\")\n    print(f\"  Feature indices: {color_sensitive_features}\")\n    print(f\"  Mean differences: {feature_diffs[color_sensitive_features]}\")\n    \n    print(f\"\\nTop 20 Shape-Sensitive Features (low train/test difference):\")\n    print(f\"  Feature indices: {shape_sensitive_features}\")\n    print(f\"  Mean differences: {feature_diffs[shape_sensitive_features]}\")\n    \n    return color_sensitive_features, shape_sensitive_features, feature_diffs\n\n\ndef main():\n    \"\"\"Main function to run the Sparse Autoencoder analysis\"\"\"\n    \n    print(\"=\" * 60)\n    print(\"Task 6: The Decomposition - Sparse Autoencoders\")\n    print(\"=\" * 60)\n    \n    # ===== LOAD DATA =====\n    print(\"\\n[1/6] Loading data...\")\n    \n    train_images, train_labels = load_data(TRAIN_DATA_PATH)\n    test_images, test_labels = load_data(TEST_DATA_PATH)\n    \n    if train_images is None:\n        print(\"Please update TRAIN_DATA_PATH and TEST_DATA_PATH to your data locations.\")\n        print(\"The script expects npz files with 'images' and 'labels' arrays.\")\n        return\n    \n    print(f\"Training data: {train_images.shape}\")\n    if test_images is not None:\n        print(f\"Test data: {test_images.shape}\")\n    \n    # ===== LOAD BIASED MODEL =====\n    print(\"\\n[2/6] Loading biased model...\")\n    \n    model = CNN3Layer(num_classes=NUM_CLASSES).to(device)\n    \n    if os.path.exists(MODEL_PATH):\n        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n        print(f\"Loaded model from {MODEL_PATH}\")\n    else:\n        print(f\"Warning: {MODEL_PATH} not found. Using untrained model.\")\n        print(\"Please update MODEL_PATH to point to your trained biased model.\")\n    \n    model.eval()\n    \n    # ===== EXTRACT ACTIVATIONS =====\n    print(\"\\n[3/6] Extracting intermediate activations...\")\n    \n    # Convert to tensors\n    train_tensor = torch.FloatTensor(train_images).permute(0, 3, 1, 2)\n    train_labels_tensor = torch.LongTensor(train_labels)\n    \n    train_dataset = TensorDataset(train_tensor, train_labels_tensor)\n    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False)\n    \n    # Extract FC1 activations (these capture both color and shape information)\n    activations, labels_extracted = extract_activations(model, train_loader, 'fc1')\n    print(f\"Extracted activations shape: {activations.shape}\")\n    \n    # ===== TRAIN SPARSE AUTOENCODER =====\n    print(\"\\n[4/6] Training Sparse Autoencoder...\")\n    \n    input_dim = activations.shape[1]\n    hidden_dim = input_dim * SAE_EXPANSION_FACTOR  # Overcomplete representation\n    \n    print(f\"SAE Architecture: {input_dim} -> {hidden_dim} -> {input_dim}\")\n    \n    sae = SparseAutoencoder(input_dim, hidden_dim).to(device)\n    losses = train_sae(sae, activations)\n    \n    # Plot training loss\n    plt.figure(figsize=(10, 4))\n    plt.plot(losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('SAE Training Loss')\n    plt.grid(True)\n    plt.savefig('sae_training_loss.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # ===== ANALYZE FEATURES =====\n    print(\"\\n[5/6] Analyzing learned features...\")\n    \n    feature_analysis = analyze_features(sae, activations, labels_extracted, train_images, top_k=10)\n    \n    # Print summary of interesting features\n    print(\"\\nFeature Analysis Summary (first 20 features):\")\n    print(\"-\" * 50)\n    for fa in feature_analysis[:20]:\n        print(f\"Feature {fa['feature_idx']:3d}: Dominant Label = {fa['dominant_label']}, \"\n              f\"Mean Act = {fa['mean_activation']:.4f}, \"\n              f\"Label Dist = {fa['label_counts']}\")\n    \n    # Visualize some interesting features\n    print(\"\\nVisualizing top 5 features...\")\n    for i in range(min(5, len(feature_analysis))):\n        visualize_feature(train_images, train_labels, feature_analysis, \n                         feature_analysis[i]['feature_idx'],\n                         save_path=f'feature_{i}_samples.png')\n    \n    # ===== COLOR VS SHAPE ANALYSIS =====\n    print(\"\\n[6/6] Analyzing color vs. shape features...\")\n    \n    if test_images is not None:\n        color_features, shape_features, diffs = analyze_color_features(\n            model, sae, train_images, train_labels, test_images, test_labels\n        )\n        \n        # Visualize feature differences\n        plt.figure(figsize=(14, 5))\n        plt.bar(range(len(diffs)), diffs)\n        plt.xlabel('Feature Index')\n        plt.ylabel('Train/Test Activation Difference')\n        plt.title('Feature Sensitivity: Higher = More Color-Sensitive')\n        plt.axhline(y=np.median(diffs), color='r', linestyle='--', label='Median')\n        plt.legend()\n        plt.savefig('feature_color_sensitivity.png', dpi=150, bbox_inches='tight')\n        plt.show()\n        \n        # ===== INTERVENTION EXPERIMENTS =====\n        print(\"\\n=== Running Intervention Experiments ===\")\n        \n        # Try intervening on a few features\n        print(\"\\nIntervening on most color-sensitive features:\")\n        for feat_idx in color_features[:3]:\n            intervention_experiment(model, sae, train_images, train_labels, feat_idx)\n        \n        print(\"\\nIntervening on most shape-sensitive features:\")\n        for feat_idx in shape_features[:3]:\n            intervention_experiment(model, sae, train_images, train_labels, feat_idx)\n    \n    # ===== SAVE SAE MODEL =====\n    sae_save_path = 'sae_model.pth'\n    torch.save(sae.state_dict(), sae_save_path)\n    print(f\"\\nSAE model saved to {sae_save_path}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"Task 6 Complete!\")\n    print(\"=\" * 60)\n    print(\"\"\"\n    Key Findings:\n    1. The SAE decomposes FC1 activations into {hidden_dim} features\n    2. Some features are highly class-specific (likely shape detectors)\n    3. Features with high train/test difference are likely color-sensitive\n    4. Interventions on color-sensitive features may change predictions\n    \n    Explore further by:\n    - Adjusting SAE_SPARSITY_WEIGHT for more/less sparse representations\n    - Analyzing different layers (conv1, conv2, conv3)\n    - Visualizing decoder weights to understand feature meanings\n    - Manual labeling of features to categorize color vs. shape\n    \"\"\".format(hidden_dim=hidden_dim))\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-08T16:34:42.868834Z","iopub.execute_input":"2026-02-08T16:34:42.869142Z","iopub.status.idle":"2026-02-08T16:35:32.527978Z","shell.execute_reply.started":"2026-02-08T16:34:42.869114Z","shell.execute_reply":"2026-02-08T16:35:32.527344Z"}},"outputs":[],"execution_count":null}]}