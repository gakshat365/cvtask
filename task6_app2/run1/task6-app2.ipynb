{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14760510,"sourceType":"datasetVersion","datasetId":9434511},{"sourceId":745111,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":568438,"modelId":580771}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\n\n# CELL 1:  Imports and Setup\n# CELL 2:  Configuration and Constants\n# CELL 3:  CNN3Layer Model Class\n# CELL 4:  TopK Sparse Autoencoder Class\n# CELL 5:  Contrastive Feature Learner Class\n# CELL 6:  Contrastive Loss Function\n# CELL 7:  Concept Steering Vectors Class\n# CELL 8:  Causal Neuron Analyzer Class\n# CELL 9:  Feature Clustering Function\n# CELL 10: Feature Classifier Class (Phase 1 - Traitors & Heroes)\n# CELL 11: Grad-CAM Class (Phase 2)\n# CELL 12: Structural Analyzer Class (Phase 3 - Circuits)\n# CELL 13: Causal Intervener Class (Phase 4 - The Cure)\n# CELL 14: Validation Function - Traitors with Grad-CAM\n# CELL 15: Evaluation Function - SAE Reconstruction Quality\n# CELL 16: Training Function - Concept Probes\n# CELL 17: Main Execution Part 1 - Data Loading\n# CELL 18: Main Execution Part 2 - Load Pre-trained Model\n# CELL 19: Main Execution Part 3 - Train TopK Sparse Autoencoder\n# CELL 20: Main Execution Part 4 - Concept Steering Vectors\n# CELL 21: Main Execution Part 5 - Causal Neuron Ablation Analysis\n# CELL 22: Main Execution Part 6 - Cluster SAE Features\n# CELL 23: Main Execution Part 7 - PHASE 1 Feature Classification\n# CELL 24: Main Execution Part 8 - PHASE 2 Grad-CAM Validation\n# CELL 25: Main Execution Part 9 - Train Concept Probes\n# CELL 26: Main Execution Part 10 - Visualization: Analysis Plots\n# CELL 27: Main Execution Part 11 - Visualization: Steering Effect Plot\n# CELL 28: Main Execution Part 12 - PHASE 3 Structural Analysis\n# CELL 29: Main Execution Part 13 - PHASE 4 Causal Intervention\n# CELL 30: Run Main Function\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:00.897617Z","iopub.execute_input":"2026-02-09T23:21:00.897961Z","iopub.status.idle":"2026-02-09T23:21:00.902163Z","shell.execute_reply.started":"2026-02-09T23:21:00.897935Z","shell.execute_reply":"2026-02-09T23:21:00.901429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 1: Imports and Setup\n# ============================================================================\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport os\nimport gc\nimport time\nimport logging\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import Tuple, List, Dict, Optional, Any, Union\nfrom functools import wraps\nfrom tqdm import tqdm\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Timing decorator\ndef timing_decorator(func):\n    \"\"\"Log execution time of functions.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        elapsed = time.time() - start\n        logger.info(f\"{func.__name__} completed in {elapsed:.2f}s\")\n        return result\n    return wrapper\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:00.903409Z","iopub.execute_input":"2026-02-09T23:21:00.903713Z","iopub.status.idle":"2026-02-09T23:21:00.939874Z","shell.execute_reply.started":"2026-02-09T23:21:00.903692Z","shell.execute_reply":"2026-02-09T23:21:00.939284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: Configuration and Constants\n# ============================================================================\n\n@dataclass\nclass Config:\n    \"\"\"Centralized configuration for SAE analysis.\"\"\"\n    \n    # Paths\n    train_data_path: str = '/kaggle/input/cmnistneo1/train_data_rg95z.npz'\n    test_data_path: str = '/kaggle/input/cmnistneo1/test_data_gr95z.npz'\n    model_path: str = '/kaggle/input/task1app3models/pytorch/default/2/task1approach3sc1_modelv1.pth'\n    \n    # SAE Hyperparameters\n    topk_k: int = 32  # Number of active features in TopK SAE\n    sae_hidden_dim: int = 512\n    sae_epochs: int = 20\n    sae_batch_size: int = 64\n    sae_learning_rate: float = 0.001\n    \n    # Model Architecture\n    num_classes: int = 10\n    conv1_channels: int = 32\n    conv2_channels: int = 64\n    conv3_channels: int = 64\n    fc1_units: int = 128\n    dropout_rate: float = 0.1\n    \n    # Analysis Thresholds\n    traitor_threshold: float = 2.0  # Features with >2x activation on color vs BW\n    hero_lower_bound: float = 0.8   # Shape-invariant features lower bound\n    hero_upper_bound: float = 1.2   # Shape-invariant features upper bound\n    activation_epsilon: float = 1e-4  # Prevent division by zero\n    min_activation: float = 0.01     # Ignore effectively dead neurons\n    correlation_threshold: float = 0.3  # Circuit detection threshold\n    dead_feature_threshold: float = 1e-5  # Variance threshold for dead feature detection\n    circuit_threshold: float = 0.3  # Alias for correlation_threshold (backward compatibility)\n    \n    # Surgery/Intervention parameters\n    traitor_suppression: float = 0.09  # Multiply traitors by this (0.1 = 90% suppression)\n    max_hero_boost: float = 4.0  # Maximum multiplier for hero features\n    \n    # Memory Management\n    memory_limit_samples: int = 500\n    batch_size_inference: int = 64\n    \n    # Device\n    device: str = field(default_factory=lambda: 'cuda' if torch.cuda.is_available() else 'cpu')\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Export configuration as dictionary.\"\"\"\n        return {k: v for k, v in self.__dict__.items()}\n\n# Global config instance\nconfig = Config()\ndevice = torch.device(config.device)\nlogger.info(f\"Using device: {device}\")\n\n# Legacy constants for backward compatibility (will be removed)\nTRAIN_DATA_PATH = config.train_data_path\nTEST_DATA_PATH = config.test_data_path\nMODEL_PATH = config.model_path\nTOPK_K = config.topk_k\nSAE_HIDDEN_DIM = config.sae_hidden_dim\nSAE_EPOCHS = config.sae_epochs\nSAE_BATCH_SIZE = config.sae_batch_size\nSAE_LEARNING_RATE = config.sae_learning_rate\nMEMORY_LIMIT_SAMPLES = config.memory_limit_samples\nNUM_CLASSES = config.num_classes\nCONV1_CHANNELS = config.conv1_channels\nCONV2_CHANNELS = config.conv2_channels\nCONV3_CHANNELS = config.conv3_channels\nFC1_UNITS = config.fc1_units\nDROPOUT_RATE = config.dropout_rate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:00.941116Z","iopub.execute_input":"2026-02-09T23:21:00.941629Z","iopub.status.idle":"2026-02-09T23:21:00.965346Z","shell.execute_reply.started":"2026-02-09T23:21:00.941607Z","shell.execute_reply":"2026-02-09T23:21:00.964681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 2.5: Utility Classes\n# ============================================================================\n\nclass ActivationUtils:\n    \"\"\"Centralized activation computation utilities.\"\"\"\n    \n    @staticmethod\n    def get_activations_batched(\n        model: nn.Module,\n        images: np.ndarray,\n        device: torch.device,\n        batch_size: int = 64,\n        layer: str = 'fc1',\n        return_tensor: bool = False,\n        show_progress: bool = False\n    ) -> Union[np.ndarray, torch.Tensor]:\n        \"\"\"\n        Compute activations with consistent API.\n        \n        Args:\n            model: Neural network model\n            images: numpy array (N, H, W, C)\n            device: torch.device\n            batch_size: Batch size for processing\n            layer: 'fc1' or 'conv3'\n            return_tensor: If True, return torch.Tensor; else numpy\n            show_progress: Show progress bar\n        \n        Returns:\n            Activations as torch.Tensor (if return_tensor=True) or numpy.ndarray\n        \"\"\"\n        all_acts = []\n        model.eval()\n        \n        iterator = range(0, len(images), batch_size)\n        if show_progress:\n            iterator = tqdm(iterator, desc=f\"Computing {layer} activations\", leave=False)\n        \n        with torch.no_grad():\n            for i in iterator:\n                batch = torch.FloatTensor(images[i:i+batch_size])\n                batch = batch.permute(0, 3, 1, 2).to(device)\n                \n                if layer == 'fc1':\n                    acts = model.get_fc1_activations(batch)\n                elif layer == 'conv3':\n                    # Get output after conv3\n                    x = model.pool1(F.relu(model.conv1(batch)))\n                    x = model.pool2(F.relu(model.conv2(x)))\n                    acts = model.pool3(F.relu(model.conv3(x)))\n                else:\n                    raise ValueError(f\"Unknown layer: {layer}\")\n                \n                if not return_tensor:\n                    acts = acts.cpu()\n                \n                all_acts.append(acts)\n                del batch\n                \n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n        \n        if return_tensor:\n            return torch.cat(all_acts, dim=0)\n        else:\n            return torch.cat(all_acts, dim=0).numpy()\n\n\nclass ActivationCache:\n    \"\"\"\n    Cache model activations to avoid recomputation.\n    Saves ~80% of forward pass time in analysis phase.\n    \"\"\"\n    def __init__(self, model: nn.Module, device: torch.device):\n        self.model = model\n        self.device = device\n        self.cache: Dict[int, np.ndarray] = {}\n    \n    def get_activations(\n        self, \n        images: np.ndarray, \n        layer: str = 'fc1',\n        batch_size: int = 64\n    ) -> np.ndarray:\n        \"\"\"Get cached activations or compute if not cached.\"\"\"\n        key = self._hash_images(images)\n        \n        if key not in self.cache:\n            self.cache[key] = ActivationUtils.get_activations_batched(\n                self.model, images, self.device, batch_size, layer\n            )\n        \n        return self.cache[key]\n    \n    def _hash_images(self, images: np.ndarray) -> int:\n        \"\"\"Fast hash using first/last image checksums and length.\"\"\"\n        return hash((images[0].tobytes(), images[-1].tobytes(), len(images)))\n    \n    def clear(self):\n        \"\"\"Free cached memory.\"\"\"\n        self.cache.clear()\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n\nclass PlotContext:\n    \"\"\"Context manager to ensure matplotlib figures are properly closed.\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        self.fig = plt.figure(*args, **kwargs)\n    \n    def __enter__(self):\n        return self.fig\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        plt.close(self.fig)\n        gc.collect()\n\n\ndef get_stratified_subset(\n    images: np.ndarray, \n    labels: np.ndarray, \n    n_samples: int = 500, \n    random_state: int = 42\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Get balanced subset with equal class representation.\n    Better than arbitrary [:500] which might miss classes.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    \n    if len(images) <= n_samples:\n        return images, labels\n    \n    _, subset_images, _, subset_labels = train_test_split(\n        images, labels, \n        test_size=n_samples,\n        stratify=labels,\n        random_state=random_state\n    )\n    \n    return subset_images, subset_labels\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.048046Z","iopub.execute_input":"2026-02-09T23:21:01.048712Z","iopub.status.idle":"2026-02-09T23:21:01.062405Z","shell.execute_reply.started":"2026-02-09T23:21:01.048688Z","shell.execute_reply":"2026-02-09T23:21:01.061594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 3: CNN3Layer Model Class\n# ============================================================================\n\n\n\nclass CNN3Layer(nn.Module):\n    def __init__(self, num_classes=NUM_CLASSES):\n        super(CNN3Layer, self).__init__()\n        self.conv1 = nn.Conv2d(3, CONV1_CHANNELS, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(CONV1_CHANNELS, CONV2_CHANNELS, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.conv3 = nn.Conv2d(CONV2_CHANNELS, CONV3_CHANNELS, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(CONV3_CHANNELS * 3 * 3, FC1_UNITS)\n        self.dropout = nn.Dropout(DROPOUT_RATE)\n        self.fc2 = nn.Linear(FC1_UNITS, num_classes)\n    \n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = self.pool3(F.relu(self.conv3(x)))\n        x = x.reshape(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n    \n    def get_fc1_activations(self, x):\n        \"\"\"Get FC1 activations (the hidden state we'll analyze)\"\"\"\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = self.pool3(F.relu(self.conv3(x)))\n        x = x.reshape(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        return x\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.063938Z","iopub.execute_input":"2026-02-09T23:21:01.064141Z","iopub.status.idle":"2026-02-09T23:21:01.094684Z","shell.execute_reply.started":"2026-02-09T23:21:01.064121Z","shell.execute_reply":"2026-02-09T23:21:01.094007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 4: TopK Sparse Autoencoder Class\n# ============================================================================\n\nclass TopKSparseAutoencoder(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, k):\n        super(TopKSparseAutoencoder, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.k = k\n        \n        # Learned encoder and decoder\n        self.encoder = nn.Linear(input_dim, hidden_dim)\n        self.decoder = nn.Linear(hidden_dim, input_dim)\n        \n        # Normalize decoder weights (helps with feature interpretability)\n        with torch.no_grad():\n            self.decoder.weight.data = F.normalize(self.decoder.weight.data, dim=0)\n    \n    def encode(self, x):\n        \"\"\"Encode with TopK sparsity constraint\"\"\"\n        pre_act = self.encoder(x)\n        \n        # TopK: only keep the top K activations, set rest to 0\n        topk_values, topk_indices = torch.topk(pre_act, self.k, dim=-1)\n        \n        # Create sparse activation\n        sparse_act = torch.zeros_like(pre_act)\n        sparse_act.scatter_(-1, topk_indices, F.relu(topk_values))\n        \n        return sparse_act, topk_indices\n    \n    def decode(self, z):\n        return self.decoder(z)\n    \n    def forward(self, x):\n        z, indices = self.encode(x)\n        x_recon = self.decode(z)\n        return x_recon, z, indices\n      ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.095673Z","iopub.execute_input":"2026-02-09T23:21:01.095952Z","iopub.status.idle":"2026-02-09T23:21:01.117792Z","shell.execute_reply.started":"2026-02-09T23:21:01.095924Z","shell.execute_reply":"2026-02-09T23:21:01.117065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: Contrastive Feature Learner Class\n# ============================================================================\n\nclass ContrastiveFeatureLearner(nn.Module):\n\n    def __init__(self, input_dim, feature_dim=64):\n        super(ContrastiveFeatureLearner, self).__init__()\n        self.feature_dim = feature_dim\n        \n        # Shape-invariant feature extractor\n        self.shape_encoder = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, feature_dim),\n        )\n        \n        # Color-specific feature extractor\n        self.color_encoder = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, feature_dim),\n        )\n    \n    def forward(self, x):\n        shape_features = self.shape_encoder(x)\n        color_features = self.color_encoder(x)\n        return shape_features, color_features\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.118772Z","iopub.execute_input":"2026-02-09T23:21:01.119351Z","iopub.status.idle":"2026-02-09T23:21:01.137268Z","shell.execute_reply.started":"2026-02-09T23:21:01.119319Z","shell.execute_reply":"2026-02-09T23:21:01.136601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 6: Contrastive Loss Function\n# ============================================================================\n\ndef contrastive_loss(shape_features, color_features, labels, margin=1.0):\n    \"\"\"\n    Custom contrastive loss:\n    - Shape features of same class should be similar\n    - Color features should capture the remaining variance\n    \"\"\"\n    batch_size = shape_features.size(0)\n    \n    # Normalize features\n    shape_norm = F.normalize(shape_features, dim=1)\n    color_norm = F.normalize(color_features, dim=1)\n    \n    # Shape similarity matrix\n    shape_sim = torch.mm(shape_norm, shape_norm.t())\n    \n    # Create label mask (1 if same class, 0 otherwise)\n    labels = labels.view(-1, 1)\n    label_mask = (labels == labels.t()).float()\n    \n    # Contrastive loss for shape features\n    # Pull same-class together, push different-class apart\n    pos_loss = (1 - shape_sim) * label_mask  # Same class should be similar\n    neg_loss = F.relu(shape_sim - margin) * (1 - label_mask)  # Different class apart\n    \n    shape_loss = (pos_loss.sum() + neg_loss.sum()) / (batch_size * batch_size)\n    \n    # Reconstruction: shape + color should reconstruct original\n    # (This is optional but helps training)\n    \n    return shape_loss\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.138957Z","iopub.execute_input":"2026-02-09T23:21:01.13926Z","iopub.status.idle":"2026-02-09T23:21:01.155244Z","shell.execute_reply.started":"2026-02-09T23:21:01.13923Z","shell.execute_reply":"2026-02-09T23:21:01.154661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 7: Concept Steering Vectors Class\n# ============================================================================\n\nclass ConceptSteeringVectors:\n    \"\"\"Compute and apply concept steering vectors for interventions.\"\"\"\n\n    def __init__(self, model: nn.Module, device: torch.device):\n        self.model = model\n        self.device = device\n        self.color_vectors: Dict[int, np.ndarray] = {}\n        self.shape_vectors: Dict[int, np.ndarray] = {}\n        self.mean_activations: Dict[str, np.ndarray] = {}\n        self.global_color_vector: Optional[np.ndarray] = None\n    \n    def compute_steering_vectors(\n        self, \n        env1_images: np.ndarray, \n        env1_labels: np.ndarray, \n        env2_images: np.ndarray, \n        env2_labels: np.ndarray\n    ) -> np.ndarray:\n        \"\"\"\n        Compute steering vectors from two different color environments.\n        \n        Args:\n            env1_images: Images from environment 1 (N, H, W, C)\n            env1_labels: Labels for env1 (N,)\n            env2_images: Images from environment 2 (M, H, W, C)\n            env2_labels: Labels for env2 (M,)\n            \n        Returns:\n            Global color steering vector (FC1_UNITS,)\n        \"\"\"\n        self.model.eval()\n        \n        # Get activations for both environments using centralized utility\n        logger.info(\"Computing activations for environment 1...\")\n        env1_acts = ActivationUtils.get_activations_batched(\n            self.model, env1_images, self.device, show_progress=True\n        )\n        logger.info(\"Computing activations for environment 2...\")\n        env2_acts = ActivationUtils.get_activations_batched(\n            self.model, env2_images, self.device, show_progress=True\n        )\n        \n        # Compute per-class means\n        self.color_vectors = {}\n        self.mean_activations = {}\n        \n        for class_idx in range(10):\n            env1_mask = env1_labels == class_idx\n            env2_mask = env2_labels == class_idx\n            \n            if env1_mask.sum() > 0 and env2_mask.sum() > 0:\n                env1_mean = env1_acts[env1_mask].mean(axis=0)\n                env2_mean = env2_acts[env2_mask].mean(axis=0)\n                \n                # Color vector: captures color changes for same shape\n                self.color_vectors[class_idx] = env1_mean - env2_mean\n                \n                self.mean_activations[f'env1_class{class_idx}'] = env1_mean\n                self.mean_activations[f'env2_class{class_idx}'] = env2_mean\n        \n        # Global color vector: average across classes\n        if self.color_vectors:\n            all_color_vecs = np.stack(list(self.color_vectors.values()))\n            self.global_color_vector = all_color_vecs.mean(axis=0)\n            self.global_color_vector = self.global_color_vector / np.linalg.norm(self.global_color_vector)\n        else:\n            self.global_color_vector = np.zeros(FC1_UNITS)\n        \n        logger.info(f\"Computed color steering vectors for {len(self.color_vectors)} classes\")\n        logger.info(f\"Global color vector norm: {np.linalg.norm(self.global_color_vector):.4f}\")\n        \n        return self.global_color_vector\n\n    \n    def steer_activations(\n        self, \n        activations: torch.Tensor, \n        direction: str = 'remove_color', \n        strength: float = 1.0\n    ) -> torch.Tensor:\n        \"\"\"Apply steering intervention to activations.\"\"\"\n        color_vec = torch.FloatTensor(self.global_color_vector).to(self.device)\n        \n        # Project activations onto color direction\n        color_projection = torch.sum(activations * color_vec, dim=-1, keepdim=True)\n        \n        if direction == 'remove_color':\n            steered = activations - strength * color_projection * color_vec\n        else:\n            steered = activations + strength * color_projection * color_vec\n        \n        return steered\n    \n    def intervention_experiment(\n        self, \n        images: np.ndarray, \n        labels: np.ndarray, \n        strengths: List[float] = None\n    ) -> List[Dict]:\n        \"\"\"Test how removing color direction affects predictions.\"\"\"\n        if strengths is None:\n            strengths = [0.0, 0.5, 1.0, 2.0]\n        \n        self.model.eval()\n        results = []\n        \n        with torch.no_grad():\n            images_tensor = torch.FloatTensor(images).permute(0, 3, 1, 2).to(self.device)\n            original_acts = self.model.get_fc1_activations(images_tensor)\n            \n            for strength in tqdm(strengths, desc=\"Testing intervention strengths\"):\n                # Steer activations\n                steered_acts = self.steer_activations(original_acts, 'remove_color', strength)\n                \n                # Continue forward pass\n                output = self.model.fc2(self.model.dropout(steered_acts))\n                predictions = torch.argmax(output, dim=1).cpu().numpy()\n                \n                accuracy = (predictions == labels).mean() * 100\n                \n                results.append({\n                    'strength': strength,\n                    'accuracy': accuracy,\n                    'predictions': predictions\n                })\n                \n                logger.info(f\"Steering strength {strength:.1f}: Accuracy = {accuracy:.2f}%\")\n        \n        return results\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.1561Z","iopub.execute_input":"2026-02-09T23:21:01.156325Z","iopub.status.idle":"2026-02-09T23:21:01.18153Z","shell.execute_reply.started":"2026-02-09T23:21:01.156306Z","shell.execute_reply":"2026-02-09T23:21:01.181009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 8: Causal Neuron Analyzer Class\n# ============================================================================\n\nclass CausalNeuronAnalyzer:\n    \"\"\"Analyze causal effects of individual neurons through ablation.\"\"\"\n\n    def __init__(self, model: nn.Module, device: torch.device):\n        self.model = model\n        self.device = device\n        self.neuron_effects: Dict[int, Dict[str, float]] = {}\n    \n    def ablate_neuron(\n        self, \n        activations: torch.Tensor, \n        labels: np.ndarray, \n        neuron_idx: int, \n        ablation_type: str = 'zero'\n    ) -> float:\n        \"\"\"\n        Test model accuracy when a specific FC1 neuron is ablated.\n        Uses pre-computed activations for speed.\n        \n        Args:\n            activations: Pre-computed FC1 activations (N, FC1_UNITS)\n            labels: Ground truth labels (N,)\n            neuron_idx: Index of neuron to ablate\n            ablation_type: 'zero' or 'mean'\n            \n        Returns:\n            Accuracy after ablation (percentage)\n        \"\"\"\n        self.model.eval()\n        \n        with torch.no_grad():\n            # Ablate the specific neuron\n            ablated_acts = activations.clone()\n            if ablation_type == 'zero':\n                ablated_acts[:, neuron_idx] = 0\n            else:  # mean\n                ablated_acts[:, neuron_idx] = activations[:, neuron_idx].mean()\n            \n            # Continue forward pass (only FC2 needed)\n            output = self.model.fc2(self.model.dropout(ablated_acts))\n            predictions = torch.argmax(output, dim=1).cpu().numpy()\n            \n            accuracy = (predictions == labels).mean() * 100\n        \n        return accuracy\n    \n    @timing_decorator\n    def find_causal_neurons(\n        self, \n        env1_images: np.ndarray, \n        env1_labels: np.ndarray, \n        env2_images: np.ndarray, \n        env2_labels: np.ndarray, \n        num_neurons: int = 128\n    ) -> Tuple[List, List]:\n        \"\"\"\n        Find neurons with causal effects through systematic ablation.\n        \n        Args:\n            env1_images: Images from environment 1\n            env1_labels: Labels for env1\n            env2_images: Images from environment 2\n            env2_labels: Labels for env2  \n            num_neurons: Number of neurons to analyze\n            \n        Returns:\n            (color_neurons, shape_neurons) tuple of neuron indices\n        \"\"\"\n        logger.info(\"Analyzing causal effect of each neuron...\")\n        \n        # Baseline accuracies\n        self.model.eval()\n        \n        with torch.no_grad():\n            # Pre-compute activations ONCE using centralized utility\n            logger.info(\"Pre-computing activations for efficiency...\")\n            env1_acts = ActivationUtils.get_activations_batched(\n                self.model, env1_images, self.device, return_tensor=True, show_progress=True\n            )\n            env2_acts = ActivationUtils.get_activations_batched(\n                self.model, env2_images, self.device, return_tensor=True, show_progress=True\n            )\n            \n            # Get baseline accuracy from activations\n            def get_acc_from_acts(acts, labels):\n                out = self.model.fc2(self.model.dropout(acts))\n                preds = torch.argmax(out, dim=1).cpu().numpy()\n                return (preds == labels).mean() * 100\n\n            env1_baseline = get_acc_from_acts(env1_acts, env1_labels)\n            env2_baseline = get_acc_from_acts(env2_acts, env2_labels)\n        \n            logger.info(f\"Baseline accuracies: Env1={env1_baseline:.2f}%, Env2={env2_baseline:.2f}%\")\n            \n            # Test each neuron with progress bar\n            self.neuron_effects = {}\n            color_neurons = []\n            shape_neurons = []\n            \n            for neuron_idx in tqdm(range(num_neurons), desc=\"Analyzing neurons\"):\n                # Pass activations instead of images\n                env1_ablated = self.ablate_neuron(env1_acts, env1_labels, neuron_idx)\n                env2_ablated = self.ablate_neuron(env2_acts, env2_labels, neuron_idx)\n                \n                env1_effect = env1_baseline - env1_ablated\n                env2_effect = env2_baseline - env2_ablated\n                \n                self.neuron_effects[neuron_idx] = {\n                    'env1_effect': env1_effect,\n                    'env2_effect': env2_effect,\n                    'asymmetry': env1_effect - env2_effect\n                }\n                \n                if env1_effect > 2 and env2_effect < 0:\n                    color_neurons.append(neuron_idx)\n                elif abs(env1_effect - env2_effect) < 1 and (env1_effect > 0.5 or env2_effect > 0.5):\n                    shape_neurons.append(neuron_idx)\n            \n            # Sort and print results\n            sorted_neurons = sorted(self.neuron_effects.items(), \n                                key=lambda x: x[1]['asymmetry'], reverse=True)\n            \n            logger.info(f\"\\nTop 10 COLOR-SPECIFIC neurons:\")\n            for neuron_idx, effects in sorted_neurons[:10]:\n                logger.info(f\"  Neuron {neuron_idx}: Env1 effect={effects['env1_effect']:.2f}%, \"\n                          f\"Asymmetry={effects['asymmetry']:.2f}\")\n            \n            return sorted_neurons[:10], sorted_neurons[-10:]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.182208Z","iopub.execute_input":"2026-02-09T23:21:01.182385Z","iopub.status.idle":"2026-02-09T23:21:01.200306Z","shell.execute_reply.started":"2026-02-09T23:21:01.18236Z","shell.execute_reply":"2026-02-09T23:21:01.199786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 9: Feature Clustering Function\n# ============================================================================\n\n@timing_decorator\ndef cluster_features_by_environment(model, sae, env1_data, env2_data, n_clusters=4):\n    \"\"\"\n    Cluster SAE features based on cross-environment behavior.\n    \n    Args:\n        model: CNN model\n        sae: Sparse autoencoder\n        env1_data: (images, labels) tuple for environment 1\n        env2_data: (images, labels) tuple for environment 2\n        n_clusters: Number of clusters\n        \n    Returns:\n        (clusters, cluster_info, feature_characteristics) tuple\n    \"\"\"\n    model.eval()\n    sae.eval()\n    \n    env1_images, env1_labels = env1_data\n    env2_images, env2_labels = env2_data\n    \n    logger.info(\"Computing SAE encodings for both environments...\")\n    \n    with torch.no_grad():\n        # Use centralized utility\n        env1_acts = ActivationUtils.get_activations_batched(\n            model, env1_images, device, show_progress=True, return_tensor=True\n        )\n        env2_acts = ActivationUtils.get_activations_batched(\n            model, env2_images, device, show_progress=True, return_tensor=True\n        )\n        \n        # Get SAE encodings\n        env1_z, _ = sae.encode(env1_acts)\n        env2_z, _ = sae.encode(env2_acts)\n        \n        env1_z = env1_z.cpu().numpy()\n        env2_z = env2_z.cpu().numpy()\n    \n    # Compute per-class mean activations for each SAE feature\n    n_features = env1_z.shape[1]\n    env1_class_means = np.zeros((10, n_features))\n    env2_class_means = np.zeros((10, n_features))\n    \n    for c in range(10):\n        env1_mask = env1_labels == c\n        env2_mask = env2_labels == c\n        \n        if env1_mask.sum() > 0:\n            env1_class_means[c] = env1_z[env1_mask].mean(axis=0)\n        if env2_mask.sum() > 0:\n            env2_class_means[c] = env2_z[env2_mask].mean(axis=0)\n    \n    # Feature characteristics: how each feature differs across environments\n    feature_characteristics = np.zeros((n_features, 3))\n    \n    for feat_idx in range(n_features):\n        # Mean activation difference across classes\n        mean_diff = np.abs(env1_class_means[:, feat_idx] - env2_class_means[:, feat_idx]).mean()\n        \n        # Variance within environment (selectivity)\n        env1_var = env1_class_means[:, feat_idx].var()\n        env2_var = env2_class_means[:, feat_idx].var()\n        \n        feature_characteristics[feat_idx] = [mean_diff, env1_var, env2_var]\n    \n    # Cluster features\n    logger.info(f\"Clustering {n_features} features into {n_clusters} groups...\")\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    clusters = kmeans.fit_predict(feature_characteristics)\n    \n    # Analyze clusters\n    logger.info(\"\\n=== Feature Clustering by Environment Behavior ===\")\n    cluster_info = []\n    \n    for cluster_idx in range(n_clusters):\n        cluster_mask = clusters == cluster_idx\n        cluster_features = np.where(cluster_mask)[0]\n        \n        mean_env_diff = feature_characteristics[cluster_mask, 0].mean()\n        mean_selectivity = (feature_characteristics[cluster_mask, 1] + \n                           feature_characteristics[cluster_mask, 2]).mean() / 2\n        \n        cluster_info.append({\n            'cluster': cluster_idx,\n            'n_features': len(cluster_features),\n            'mean_env_diff': mean_env_diff,\n            'mean_selectivity': mean_selectivity,\n            'features': cluster_features[:10]  # First 10\n        })\n        \n        cluster_type = \"COLOR\" if mean_env_diff > np.median(feature_characteristics[:, 0]) else \"SHAPE\"\n        logger.info(f\"\\nCluster {cluster_idx} ({cluster_type}):\")\n        logger.info(f\"  {len(cluster_features)} features\")\n        logger.info(f\"  Mean env difference: {mean_env_diff:.4f}\")\n        logger.info(f\"  Mean selectivity: {mean_selectivity:.4f}\")\n        logger.info(f\"  Example features: {cluster_features[:10]}\")\n    \n    return clusters, cluster_info, feature_characteristics\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.201875Z","iopub.execute_input":"2026-02-09T23:21:01.20219Z","iopub.status.idle":"2026-02-09T23:21:01.221333Z","shell.execute_reply.started":"2026-02-09T23:21:01.202169Z","shell.execute_reply":"2026-02-09T23:21:01.220828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 10: Feature Classifier Class (Phase 1 - Traitors & Heroes)\n# ============================================================================\n\n# ==========================================\n# PHASE 1: Feature Identification (Traitors & Heroes)\n# ==========================================\n\nclass FeatureClassifier:\n    def __init__(self, model, sae, device):\n        self.model = model\n        self.sae = sae\n        self.device = device\n        self.traitors = []\n        self.heroes = []\n        self.sensitivity_scores = None\n\n    def classify_features(self, images: np.ndarray, batch_size: int = 64) -> Tuple[List, List]:\n        \"\"\"\n        Classify SAE features based on sensitivity to color.\n        Generates B&W counterparts in-memory to ensure perfect pairing.\n        \n        Args:\n            images: RGB images (N, H, W, 3)\n            batch_size: Batch size for processing\n            \n        Returns:\n            (traitors, heroes) tuple of (feature_idx, score) lists\n        \"\"\"\n        logger.info(\"Running Feature Sensitivity Analysis...\")\n        self.model.eval()\n        self.sae.eval()\n\n        # 1. Prepare Data Pairs (Color vs BW)\n        images_bw = images.mean(axis=3, keepdims=True).repeat(3, axis=3)\n        \n        # 2. Extract SAE Latents for both\n        def get_latents(img_data):\n            z_list = []\n            with torch.no_grad():\n                for i in tqdm(range(0, len(img_data), batch_size), \n                             desc=\"Extracting latents\", leave=False):\n                    batch = torch.FloatTensor(img_data[i:i+batch_size]).permute(0, 3, 1, 2).to(self.device)\n                    acts = self.model.get_fc1_activations(batch)\n                    _, z, _ = self.sae(acts)\n                    z_list.append(z.cpu().numpy())\n            return np.concatenate(z_list, axis=0)\n\n        logger.info(\"Extracting latents for colored images...\")\n        z_color = get_latents(images)\n        logger.info(\"Extracting latents for B&W counterparts...\")\n        z_bw = get_latents(images_bw)\n\n        # 3. Calculate Sensitivity Index\n        mean_act_color = z_color.mean(axis=0)\n        mean_act_bw = z_bw.mean(axis=0)\n        \n        self.sensitivity_scores = (mean_act_color + config.activation_epsilon) / \\\n                                 (mean_act_bw + config.activation_epsilon)\n        \n        # 4. Classification (Vectorized)\n        active_mask = mean_act_color > config.min_activation\n        traitor_mask = (self.sensitivity_scores > config.traitor_threshold) & active_mask\n        hero_mask = ((self.sensitivity_scores >= config.hero_lower_bound) & \n                     (self.sensitivity_scores <= config.hero_upper_bound) & active_mask)\n        \n        # Convert to list of tuples\n        self.traitors = list(zip(\n            np.where(traitor_mask)[0].tolist(), \n            self.sensitivity_scores[traitor_mask].tolist()\n        ))\n        self.heroes = list(zip(\n            np.where(hero_mask)[0].tolist(), \n            self.sensitivity_scores[hero_mask].tolist()\n        ))\n\n        # Sort by score\n        self.traitors.sort(key=lambda x: x[1], reverse=True)\n        self.heroes.sort(key=lambda x: abs(1-x[1]))\n\n        logger.info(f\"Found {len(self.traitors)} Traitors (Color-Obsessed)\")\n        logger.info(f\"Found {len(self.heroes)} Heroes (Shape-Invariant)\")\n        \n        if self.traitors:\n            logger.info(f\"Top Traitor: Feature {self.traitors[0][0]} (Score {self.traitors[0][1]:.2f})\")\n        if self.heroes:\n            logger.info(f\"Top Hero: Feature {self.heroes[0][0]} (Score {self.heroes[0][1]:.2f})\")\n            \n        return self.traitors, self.heroes\n\n    def plot_sensitivity_analysis(self, save_path: str = 'feature_sensitivity.png'):\n        \"\"\"Generate sensitivity analysis visualization.\"\"\"\n        if self.sensitivity_scores is None:\n            logger.warning(\"No sensitivity scores to plot\")\n            return\n        \n        scores = self.sensitivity_scores\n        n_feats = len(scores)\n        \n        # Reshape for grid (Target 16x32 for 512 features, else square)\n        if n_feats == 512:\n            grid = scores.reshape(16, 32)\n        else:\n            side = int(np.ceil(np.sqrt(n_feats)))\n            grid = np.zeros(side*side)\n            grid[:n_feats] = scores\n            grid = grid.reshape(side, side)\n            \n        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n        \n        try:\n            # 1. Pixel Grid (Red=Traitor, Blue=Hero)\n            im = axes[0].imshow(grid, cmap='coolwarm', vmin=0, vmax=3.0, aspect='auto')\n            axes[0].set_title(f\"Feature Sensitivity Map ({n_feats} Features)\\nRed = Traitors (>{config.traitor_threshold}) | Blue = Heroes (~1.0)\")\n            axes[0].axis('off')\n            plt.colorbar(im, ax=axes[0], label='Sensitivity Index')\n            \n            # 2. Histogram with Thresholds\n            n_traitors = (scores > config.traitor_threshold).sum()\n            n_heroes = ((scores > config.hero_lower_bound) & (scores < config.hero_upper_bound)).sum()\n            \n            axes[1].hist(scores, bins=50, color='gray', alpha=0.7, log=True)\n            \n            # Thresholds\n            axes[1].axvline(config.traitor_threshold, color='red', linestyle='--', linewidth=2)\n            axes[1].axvline(1.0, color='blue', linestyle='--', linewidth=2)\n            \n            # Add Text Annotations\n            ymin, ymax = axes[1].get_ylim()\n            text_y = ymax * 0.5 \n            \n            axes[1].text(config.traitor_threshold + 0.1, text_y, \n                        f'Traitors\\n(>{config.traitor_threshold})\\nn={n_traitors}', \n                        color='red', fontweight='bold', ha='left')\n            axes[1].text(0.9, text_y, f'Heroes\\n(~1.0)\\nn={n_heroes}', \n                        color='blue', fontweight='bold', ha='right')\n\n            axes[1].set_title(\"Sensitivity Distribution\")\n            axes[1].set_xlabel(\"Sensitivity Index (Color/BW)\")\n            axes[1].set_ylabel(\"Count (Log)\")\n            \n            plt.tight_layout()\n            plt.savefig(save_path)\n            logger.info(f\"Saved sensitivity plot to {save_path}\")\n            plt.show()\n        finally:\n            plt.close(fig)\n            gc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.222402Z","iopub.execute_input":"2026-02-09T23:21:01.222671Z","iopub.status.idle":"2026-02-09T23:21:01.241877Z","shell.execute_reply.started":"2026-02-09T23:21:01.222633Z","shell.execute_reply":"2026-02-09T23:21:01.241317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 11: Grad-CAM Class (Phase 2)\n# ============================================================================\n\n# ==========================================\n# PHASE 2: Grad-CAM Implementation\n# ==========================================\n\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        self.handle_fwd = None\n        self.handle_bwd = None\n        self._register_hooks()\n\n    def _register_hooks(self):\n        def save_activation(module, input, output):\n            self.activations = output.detach()\n\n        def save_gradient(module, grad_input, grad_output):\n            self.gradients = grad_output[0].detach()\n\n        self.handle_fwd = self.target_layer.register_forward_hook(save_activation)\n        # Use register_full_backward_hook for newer pytorch, or register_backward_hook for older\n        try:\n            self.handle_bwd = self.target_layer.register_full_backward_hook(save_gradient)\n        except AttributeError:\n            self.handle_bwd = self.target_layer.register_backward_hook(save_gradient)\n\n    def remove_hooks(self):\n        if self.handle_fwd: self.handle_fwd.remove()\n        if self.handle_bwd: self.handle_bwd.remove()\n\n    def generate_heatmap(self, input_tensor, target_class_idx):\n        \"\"\"\n        Generate Grad-CAM heatmap for a specific target class.\n        input_tensor: (1, 3, 28, 28)\n        \"\"\"\n        self.model.eval()\n        self.model.zero_grad()\n        \n        # Forward pass\n        output = self.model(input_tensor)\n        \n        # Target specific class\n        if target_class_idx is None:\n            target_class_idx = output.argmax(dim=1).item()\n            \n        score = output[:, target_class_idx]\n        \n        # Backward pass\n        score.backward()\n        \n        # Generate CAM\n        # Global average pooling of gradients\n        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n        \n        # Weighted combination of activations\n        cam = torch.sum(weights * self.activations, dim=1).squeeze()\n        \n        # ReLU\n        cam = F.relu(cam)\n        \n        # Resize using torch interpolation (replaces cv2)\n        # Assuming cam is (H, W), we need (1, 1, H, W) for interpolate\n        if len(cam.shape) == 2:\n            cam = cam.unsqueeze(0).unsqueeze(0)\n            \n        cam = F.interpolate(cam, size=(28, 28), mode='bilinear', align_corners=False)\n        \n        # Normalize\n        cam = cam.squeeze().cpu().numpy()\n        cam = cam - np.min(cam)\n        cam = cam / (np.max(cam) + 1e-8)\n        \n        return cam, target_class_idx\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.243221Z","iopub.execute_input":"2026-02-09T23:21:01.243405Z","iopub.status.idle":"2026-02-09T23:21:01.257959Z","shell.execute_reply.started":"2026-02-09T23:21:01.243388Z","shell.execute_reply":"2026-02-09T23:21:01.257286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 12: Structural Analyzer Class (Phase 3 - Circuits)\n# ============================================================================\n\n# ==========================================\n# PHASE 3: Structural Analysis (Circuits)\n# ==========================================\n\nclass StructuralAnalyzer:\n    def __init__(self, sae, device):\n        self.sae = sae\n        self.device = device\n        self.correlation_matrix = None\n\n    @timing_decorator\n    def analyze_correlations(self, images: np.ndarray, model: nn.Module, \n                           batch_size: int = 64) -> np.ndarray:\n        \"\"\"Compute correlation matrix of SAE features across dataset.\"\"\"\n        logger.info(\"Running Structural Analysis (Polysemanticity check)\")\n        model.eval()\n        self.sae.eval()\n        \n        # Get all latent activations\n        all_z = []\n        with torch.no_grad():\n            for i in tqdm(range(0, len(images), batch_size), desc=\"Computing correlations\"):\n                batch = torch.FloatTensor(images[i:i+batch_size]).permute(0, 3, 1, 2).to(self.device)\n                acts = model.get_fc1_activations(batch)\n                _, z, _ = self.sae(acts)\n                all_z.append(z.cpu().numpy())\n        \n        Z = np.concatenate(all_z, axis=0)  # Shape: (N, HiddenDim)\n        \n        # Filter dead neurons to avoid NaN correlations\n        active_indices = np.where(Z.var(axis=0) > config.dead_feature_threshold)[0]\n        Z_active = Z[:, active_indices]\n        \n        logger.info(f\"Computing correlation matrix for {len(active_indices)} active features\")\n        # Add small epsilon noise to break perfect symmetries if any\n        Z_active = Z_active + np.random.normal(0, 1e-9, Z_active.shape)\n        \n        self.correlation_matrix = np.corrcoef(Z_active, rowvar=False)\n        self.active_indices = active_indices\n        return self.correlation_matrix\n\n    def find_circuits(self, traitors: List[Tuple[int, float]], \n                     heroes: List[Tuple[int, float]]) -> List[Dict]:\n        \"\"\"Find high correlations between Traitors (Color) and Heroes (Shape).\"\"\"\n        if self.correlation_matrix is None:\n            logger.warning(\"Run analyze_correlations first\")\n            return []\n\n        logger.info(\"Scanning for 'Circuits' (Traitor-Hero Pairs)\")\n        \n        # Create map from original index to active matrix index\n        idx_map = {orig: new for new, orig in enumerate(self.active_indices)}\n        \n        circuits = []\n        threshold = config.circuit_threshold\n        \n        for t_idx, t_score in traitors:\n            if t_idx not in idx_map: continue\n            \n            for h_idx, h_score in heroes:\n                if h_idx not in idx_map: continue\n                \n                corr = self.correlation_matrix[idx_map[t_idx], idx_map[h_idx]]\n                \n                if corr > threshold:\n                    circuits.append({\n                        'Traitor': t_idx, 'Traitor_Score': t_score,\n                        'Hero': h_idx, 'Hero_Score': h_score,\n                        'Correlation': corr\n                    })\n        \n        # Sort by correlation strength\n        circuits.sort(key=lambda x: x['Correlation'], reverse=True)\n        \n        logger.info(f\"Found {len(circuits)} potential circuits (Correlation > {threshold})\")\n        for i, c in enumerate(circuits[:5]):\n            logger.info(f\"  Circuit {i}: Traitor #{c['Traitor']} (Color) <--> Hero #{c['Hero']} (Shape) | r={c['Correlation']:.4f}\")\n            \n        return circuits\n\n    def plot_circuit_analysis(self, traitors: List[Tuple[int, float]], \n                             heroes: List[Tuple[int, float]], \n                             save_path: str = 'circuit_analysis.png'):\n        \"\"\"\n        Visualize the interaction between Top Traitors and Top Heroes.\n        Plots a correlation heatmap for the subset of features.\n        \"\"\"\n        if self.correlation_matrix is None:\n            logger.warning(\"No correlation matrix to plot\")\n            return\n        \n        # Filter for Top 20 of each for visibility\n        t_indices = [t[0] for t in traitors[:20]] \n        h_indices = [h[0] for h in heroes[:20]]\n        \n        if not t_indices or not h_indices:\n            logger.warning(\"No traitor or hero features to plot\")\n            return\n\n        # Map original feature indices to the active_indices used in correlation matrix\n        idx_map = {orig: new for new, orig in enumerate(self.active_indices)}\n        \n        matrix_subset = np.zeros((len(t_indices), len(h_indices)))\n        \n        # Fill subset matrix\n        for r, t_idx in enumerate(t_indices):\n            if t_idx in idx_map:\n                for c, h_idx in enumerate(h_indices):\n                    if h_idx in idx_map:\n                        matrix_subset[r, c] = self.correlation_matrix[idx_map[t_idx], idx_map[h_idx]]\n        \n        fig, ax = plt.subplots(figsize=(10, 8))\n        \n        try:\n            im = ax.imshow(matrix_subset, cmap='coolwarm', vmin=-1, vmax=1)\n            plt.colorbar(im, label='Pearson Correlation')\n            \n            # Axis labels\n            ax.set_xticks(np.arange(len(h_indices)))\n            ax.set_yticks(np.arange(len(t_indices)))\n            ax.set_xticklabels([f\"H{h}\" for h in h_indices], rotation=45, ha='right')\n            ax.set_yticklabels([f\"T{t}\" for t in t_indices])\n            \n            ax.set_xlabel(\"Heroes (Shape)\")\n            ax.set_ylabel(\"Traitors (Color)\")\n            ax.set_title(\"Circuit Map: Color-Shape Dependencies\\nRed = Positive Correlation (Co-occurence)\")\n            \n            plt.tight_layout()\n            plt.savefig(save_path)\n            logger.info(f\"Saved Circuit Analysis to {save_path}\")\n            plt.show()\n        finally:\n            plt.close(fig)\n            gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.25882Z","iopub.execute_input":"2026-02-09T23:21:01.259057Z","iopub.status.idle":"2026-02-09T23:21:01.279779Z","shell.execute_reply.started":"2026-02-09T23:21:01.259038Z","shell.execute_reply":"2026-02-09T23:21:01.279085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 13: Causal Intervener Class (Phase 4 - The Cure)\n# ============================================================================\n\n# ==========================================\n# PHASE 4: Causal Intervention (The Cure)\n# ==========================================\n\nclass CausalIntervener:\n    def __init__(self, model, sae, device):\n        self.model = model\n        self.sae = sae\n        self.device = device\n\n    def find_failure_cases(self, images: np.ndarray, labels: np.ndarray, \n                          batch_size: int = 64) -> List[Dict]:\n        \"\"\"Identify images where the model fails (Predictions != Labels).\"\"\"\n        self.model.eval()\n        failures = []\n        \n        with torch.no_grad():\n            for i in tqdm(range(0, len(images), batch_size), desc=\"Finding failures\"):\n                batch_imgs = images[i:i+batch_size]\n                batch_lbls = labels[i:i+batch_size]\n                \n                tensor = torch.FloatTensor(batch_imgs).permute(0, 3, 1, 2).to(self.device)\n                output = self.model(tensor)\n                preds = output.argmax(dim=1).cpu().numpy()\n                \n                # Check for errors\n                for idx, (p, l) in enumerate(zip(preds, batch_lbls)):\n                    if p != l:\n                        failures.append({\n                            'index': i + idx,\n                            'image': batch_imgs[idx],\n                            'label': l,\n                            'pred': p\n                        })\n        \n        logger.info(f\"Found {len(failures)} failure cases in {len(images)} samples\")\n        return failures\n\n    def perform_surgery(self, failure_cases: List[Dict], \n                       traitors: List[Tuple[int, float]], \n                       heroes: List[Tuple[int, float]], \n                       boost_factor: float = 2.0, \n                       verbose: bool = True) -> Tuple[int, List[Dict]]:\n        \"\"\"Attempt to cure failure cases by suppressing Traitors and boosting Heroes.\"\"\"\n        if verbose:\n            logger.info(f\"Performing Surgery with Boost Factor {boost_factor}x\")\n        success_count = 0\n        cured_cases_list = []\n        \n        # Helper lists\n        traitor_indices = [t[0] for t in traitors]\n        hero_indices = [h[0] for h in heroes]\n        \n        # Convert list of indices to tensors for faster indexing\n        t_tensor = torch.LongTensor(traitor_indices).to(self.device)\n        h_tensor = torch.LongTensor(hero_indices).to(self.device)\n\n        for case in tqdm(failure_cases, desc=\"Performing surgery\", disable=not verbose):\n            img_tensor = torch.FloatTensor(case['image']).unsqueeze(0).permute(0, 3, 1, 2).to(self.device)\n            target_label = case['label']\n            original_pred = case['pred']\n            \n            # 1. Custom Forward Pass with Intervention\n            with torch.no_grad():\n                # A. Get FC1 activations\n                fc1_act = self.model.get_fc1_activations(img_tensor)\n                \n                # B. Encode into SAE Latents\n                z, _ = self.sae.encode(fc1_act)\n                \n                # Store original magnitude for preservation\n                original_norm = z.norm(p=2, dim=1, keepdim=True)\n                \n                # C. THE SURGERY - Softer intervention to prevent artifacts\n                z_edited = z.clone()\n                \n                # Soft Suppression of Traitors (Color) -> 0.1x instead of 0.0x\n                # Complete zeroing creates unnatural activation patterns\n                if len(traitor_indices) > 0:\n                    z_edited[:, t_tensor] *= 0.1  # Reduce by 90% instead of 100%\n                \n                # Controlled Boost of Heroes (Shape)\n                # Clamp boost factor to prevent extreme values\n                effective_boost = min(boost_factor, 3.0)  # Max 3x boost\n                if len(hero_indices) > 0:\n                    z_edited[:, h_tensor] *= effective_boost\n                \n                # Magnitude Preservation - rescale to maintain similar L2 norm\n                # This prevents out-of-distribution activations\n                edited_norm = z_edited.norm(p=2, dim=1, keepdim=True)\n                if edited_norm.item() > 0:\n                    z_edited = z_edited * (original_norm / (edited_norm + 1e-8))\n                \n                # D. Decode back to FC1\n                fc1_recon = self.sae.decode(z_edited)\n                \n                # E. Finish Network Pass (Dropout + FC2)\n                # Note: CNN3Layer forward is: pool/conv -> fc1 -> relu -> dropout -> fc2\n                # get_fc1_activations returns \"F.relu(self.fc1(x))\"\n                # So we simulate the rest:\n                out = self.model.fc2(self.model.dropout(fc1_recon))\n                new_pred = out.argmax(dim=1).item()\n            \n            # Check success\n            if new_pred == target_label:\n                success_count += 1\n                cured_cases_list.append(case)\n                if success_count <= 3 and verbose:\n                    logger.info(f\"[CURED] Corrected Label {target_label} (Was {original_pred} -> Now {new_pred})\")\n        \n        cure_rate = success_count/len(failure_cases)*100 if failure_cases else 0\n        if verbose:\n            logger.info(f\"Surgery Results: Cured {success_count}/{len(failure_cases)} ({cure_rate:.1f}%)\")\n        return success_count, cured_cases_list\n\n    def sweep_intervention_strength(self, failures: List[Dict], \n                                   traitors: List[Tuple[int, float]], \n                                   heroes: List[Tuple[int, float]], \n                                   factors: List[float] = None) -> List[int]:\n        \"\"\"Sweep multiple boost factors to find optimal intervention strength.\"\"\"\n        if factors is None:\n            # Use more conservative range with softer interventions\n            factors = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n        \n        logger.info(\"Sweeping Intervention Strengths (Boost Factors)\")\n        results = []\n        for factor in tqdm(factors, desc=\"Testing boost factors\"):\n            n_cured, _ = self.perform_surgery(failures, traitors, heroes, boost_factor=factor, verbose=False)\n            results.append(n_cured)\n            logger.info(f\"Factor {factor}x: Cured {n_cured}/{len(failures)}\")\n            \n        fig = plt.figure(figsize=(8, 5))\n        try:\n            plt.plot(factors, results, 'o-', linewidth=2)\n            plt.xlabel(\"Hero Boost Factor (Multiplier)\")\n            plt.ylabel(\"Number of Cured Cases\")\n            plt.title(\"Intervention Efficacy vs Strength\")\n            plt.grid(True, alpha=0.3)\n            plt.savefig('intervention_sweep.png')\n            logger.info(\"Saved sweep plot to intervention_sweep.png\")\n            plt.show()\n        finally:\n            plt.close(fig)\n            gc.collect()\n        \n        return results\n\n    def visualize_cures(self, cured_cases: List[Dict], model: nn.Module, \n                       sae: nn.Module, traitors: List[Tuple[int, float]], \n                       heroes: List[Tuple[int, float]], device: torch.device, \n                       save_path: str = 'surgery_validation.png'):\n        \"\"\"Visualize the effect of surgery using Grad-CAM on the Intervened Model.\"\"\"\n        if not cured_cases:\n            return\n\n        logger.info(f\"Visualizing surgery effects on {min(5, len(cured_cases))} cured cases\")\n        \n        # 1. Define Intervened Model Wrapper\n        # This allows Grad-CAM to forward pass through the surgery logic\n        class IntervenedModel(nn.Module):\n            def __init__(self, original_model, sae_model, t_idxs, h_idxs, boost):\n                super().__init__()\n                self.model = original_model\n                self.sae = sae_model\n                self.t_idxs = torch.LongTensor(t_idxs).to(device)\n                self.h_idxs = torch.LongTensor(h_idxs).to(device)\n                self.boost = boost\n                \n            def forward(self, x):\n                # Manual Forward of CNN3Layer until FC1\n                x = self.model.pool1(F.relu(self.model.conv1(x)))\n                x = self.model.pool2(F.relu(self.model.conv2(x)))\n                x = self.model.pool3(F.relu(self.model.conv3(x))) # Conv3 Hook fires here\n                x = x.reshape(x.size(0), -1)\n                fc1_act = F.relu(self.model.fc1(x))\n                \n                # SAE Surgery - Non-inplace version for gradient compatibility\n                z, _ = self.sae.encode(fc1_act)\n                \n                # Store original magnitude\n                original_norm = z.norm(p=2, dim=1, keepdim=True)\n                \n                # Clone to avoid inplace operations that break gradients\n                z_edited = z.clone()\n                \n                # Soft suppression and controlled boost (non-inplace)\n                if len(self.t_idxs) > 0: \n                    z_edited[:, self.t_idxs] = z[:, self.t_idxs] * 0.1  # 90% suppression\n                \n                effective_boost = min(self.boost, 3.0)  # Cap at 3x\n                if len(self.h_idxs) > 0: \n                    z_edited[:, self.h_idxs] = z[:, self.h_idxs] * effective_boost\n                \n                # Magnitude preservation\n                edited_norm = z_edited.norm(p=2, dim=1, keepdim=True)\n                if edited_norm.item() > 0:\n                    z_edited = z_edited * (original_norm / (edited_norm + 1e-8))\n                \n                fc1_recon = self.sae.decode(z_edited)\n                \n                # Finish\n                return self.model.fc2(self.model.dropout(fc1_recon))\n\n        # Setup Models\n        t_indices = [t[0] for t in traitors]\n        h_indices = [h[0] for h in heroes]\n        \n        wrapped_model = IntervenedModel(model, sae, t_indices, h_indices, 2.0).to(device)\n        wrapped_model.eval()\n        \n        # Setup Grad-CAMs\n        # One for original model (to see error), One for new model (to see cure)\n        # Note: Both share 'model.conv3' so we must manage hooks carefully.\n        # GradCAM class adds hooks to the layer. Since both models use the SAME layer object,\n        # we can just use one GradCAM instance and swap the .model attribute for context.\n        \n        grad_cam = GradCAM(model, model.conv3) # Attached to conv3\n        \n        n_show = min(len(cured_cases), 5)\n        fig, axes = plt.subplots(n_show, 3, figsize=(12, 4*n_show))\n        if n_show == 1: axes = axes.reshape(1, -1)\n        \n        for i in range(n_show):\n            case = cured_cases[i]\n            img = case['image']\n            img_tensor = torch.FloatTensor(img).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n            \n            # A. PRE-SURGERY (Original Model)\n            grad_cam.model = model # Point to original\n            # TARGET: The WRONG prediction (we want to see why it was fooled)\n            hm_bad, _ = grad_cam.generate_heatmap(img_tensor, case['pred'])\n            \n            # B. POST-SURGERY (Intervened Model)\n            grad_cam.model = wrapped_model # Point to wrapper (Hooks still on conv3)\n            # TARGET: The CORRECT label (which is now the prediction)\n            hm_good, _ = grad_cam.generate_heatmap(img_tensor, case['label'])\n            \n            # Plot\n            ax0, ax1, ax2 = axes[i]\n            \n            ax0.imshow(img)\n            ax0.set_title(f\"Input (True: {case['label']})\")\n            ax0.axis('off')\n            \n            ax1.imshow(img)\n            ax1.imshow(hm_bad, cmap='jet', alpha=0.5)\n            ax1.set_title(f\"Original (Pred: {case['pred']})\\nConfusion\")\n            ax1.axis('off')\n            \n            ax2.imshow(img)\n            ax2.imshow(hm_good, cmap='jet', alpha=0.5)\n            ax2.set_title(f\"Cured (Pred: {case['label']})\\nFocus\")\n            ax2.axis('off')\n            \n        plt.tight_layout()\n        plt.savefig(save_path)\n        logger.info(f\"Saved Surgery Validation to {save_path}\")\n        plt.show()\n        grad_cam.remove_hooks()\n        plt.close(fig)\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.280806Z","iopub.execute_input":"2026-02-09T23:21:01.281193Z","iopub.status.idle":"2026-02-09T23:21:01.308832Z","shell.execute_reply.started":"2026-02-09T23:21:01.281162Z","shell.execute_reply":"2026-02-09T23:21:01.308202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ============================================================================\n# CELL 14: Validation Function - Traitors with Grad-CAM\n# ============================================================================\n\ndef validate_traitors_with_gradcam(model: nn.Module, sae: nn.Module, \n                                   traitors: List[Tuple[int, float]], \n                                   images: np.ndarray, labels: np.ndarray, \n                                   device: torch.device, n_examples: int = 3):\n    \"\"\"Phase 2 Validation: Run Grad-CAM on images where Traitor features are active.\"\"\"\n    if not traitors:\n        logger.warning(\"No traitors to validate\")\n        return\n\n    logger.info(\"Phase 2: Verifying Traitor Features with Grad-CAM\")\n    \n    # Initialize GradCAM on the last conv layer\n    grad_cam = GradCAM(model, model.conv3)\n    \n    # Select top few traitors\n    top_traitors = [t[0] for t in traitors[:3]]\n    \n    # Pre-compute activations to find max activating images for these traitors\n    subset_size = min(len(images), 1000)\n    subset_images = images[:subset_size]\n    subset_labels = labels[:subset_size]\n    \n    subset_tensor = torch.FloatTensor(subset_images).permute(0, 3, 1, 2).to(device)\n    with torch.no_grad():\n        fc1_acts = model.get_fc1_activations(subset_tensor)\n        _, sae_acts, _ = sae(fc1_acts)\n        sae_acts = sae_acts.cpu().numpy()\n    \n    fig, axes = plt.subplots(len(top_traitors), n_examples * 2, figsize=(3 * n_examples * 2, 3 * len(top_traitors)))\n    if len(top_traitors) == 1: axes = axes.reshape(1, -1)\n    \n    try:\n        for row_idx, trait_idx in enumerate(top_traitors):\n            # Find images where this traitor is most active\n            feature_acts = sae_acts[:, trait_idx]\n            top_img_indices = np.argsort(feature_acts)[-n_examples:][::-1]\n            \n            for col_idx, img_idx in enumerate(top_img_indices):\n                img = subset_images[img_idx]\n                label = subset_labels[img_idx]\n                activation_val = feature_acts[img_idx]\n                \n                img_tensor = torch.FloatTensor(img).unsqueeze(0).permute(0, 3, 1, 2).to(device)\n                \n                # Run Grad-CAM\n                heatmap, pred_class = grad_cam.generate_heatmap(img_tensor, None)\n                \n                # Visualization\n                ax_orig = axes[row_idx, col_idx * 2]\n                ax_cam = axes[row_idx, col_idx * 2 + 1]\n                \n                ax_orig.imshow(img)\n                ax_orig.set_title(f\"Traitor {trait_idx}\\nAct: {activation_val:.2f}\\nPred: {pred_class} (True {label})\")\n                ax_orig.axis('off')\n                \n                ax_cam.imshow(img)\n                ax_cam.imshow(heatmap, cmap='jet', alpha=0.5)\n                ax_cam.set_title(f\"Grad-CAM\\nDoes it look at color?\")\n                ax_cam.axis('off')\n\n        plt.tight_layout()\n        plt.savefig('traitor_gradcam_validation.png')\n        logger.info(\"Saved Grad-CAM validation to traitor_gradcam_validation.png\")\n        plt.show()\n    finally:\n        grad_cam.remove_hooks()\n        plt.close(fig)\n        gc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.309487Z","iopub.execute_input":"2026-02-09T23:21:01.30966Z","iopub.status.idle":"2026-02-09T23:21:01.330621Z","shell.execute_reply.started":"2026-02-09T23:21:01.309644Z","shell.execute_reply":"2026-02-09T23:21:01.330093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 15: Evaluation Function - SAE Reconstruction Quality\n# ============================================================================\n\ndef evaluate_reconstruction(model: nn.Module, sae: nn.Module, \n                           data: Tuple[np.ndarray, np.ndarray], \n                           device: torch.device):\n    \"\"\"Evaluate SAE reconstruction quality.\"\"\"\n    images, _ = data\n    model.eval()\n    sae.eval()\n    \n    subset_size = min(len(images), 1000)\n    subset = torch.FloatTensor(images[:subset_size]).permute(0, 3, 1, 2).to(device)\n    \n    with torch.no_grad():\n        fc1_acts = model.get_fc1_activations(subset)\n        recon_acts, _, _ = sae(fc1_acts)\n        \n        mse = F.mse_loss(recon_acts, fc1_acts).item()\n        l2_norm = torch.norm(fc1_acts, p=2).mean().item()\n        \n    logger.info(f\"SAE Quality Check - Reconstruction MSE: {mse:.6f} (vs Avg Activation Norm: {l2_norm:.4f})\")\n    if mse > 0.1:\n        logger.warning(\"SAE Reconstruction is poor. Interventions may be unreliable.\")\n    else:\n        logger.info(\" SAE Reconstruction looks reasonable\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.331431Z","iopub.execute_input":"2026-02-09T23:21:01.33162Z","iopub.status.idle":"2026-02-09T23:21:01.363425Z","shell.execute_reply.started":"2026-02-09T23:21:01.331601Z","shell.execute_reply":"2026-02-09T23:21:01.362824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 16: Training Function - Concept Probes\n# ============================================================================\n\n@timing_decorator\ndef train_concept_probes(model: nn.Module, images: np.ndarray, \n                        labels: np.ndarray, device: torch.device) -> Tuple[float, float]:\n    \"\"\"Train concept probes to measure shape vs color bias.\"\"\"\n    model.eval()\n    \n    # Get activations\n    with torch.no_grad():\n        images_tensor = torch.FloatTensor(images).permute(0, 3, 1, 2).to(device)\n        activations = model.get_fc1_activations(images_tensor).cpu().numpy()\n    \n    # Detect dominant color from images\n    mean_colors = images.mean(axis=(1, 2))  # Shape: (N, 3)\n    dominant_color_idx = mean_colors.argmax(axis=1)  # 0=R, 1=G, 2=B\n    \n    # Train probes\n    # 1. Shape probe (predict digit)\n    shape_probe = LogisticRegression(max_iter=1000, random_state=42)\n    shape_probe.fit(activations, labels)\n    shape_acc = shape_probe.score(activations, labels)\n    \n    # 2. Color probe (predict dominant color)\n    color_probe = LogisticRegression(max_iter=1000, random_state=42)\n    color_probe.fit(activations, dominant_color_idx)\n    color_acc = color_probe.score(activations, dominant_color_idx)\n    \n    logger.info(\"=== Linear Probe Analysis ===\")\n    logger.info(f\"Shape (digit) probe accuracy: {shape_acc*100:.2f}%\")\n    logger.info(f\"Color probe accuracy: {color_acc*100:.2f}%\")\n    \n    if color_acc > shape_acc:\n        logger.warning(\"Color is MORE linearly separable than shape!\")\n        logger.info(\" The model represents color more explicitly than shape\")\n    else:\n        logger.info(\" Shape is more linearly separable than color\")\n        logger.info(\" The model represents shape more explicitly\")\n    \n    return shape_acc, color_acc\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.365179Z","iopub.execute_input":"2026-02-09T23:21:01.365415Z","iopub.status.idle":"2026-02-09T23:21:01.386516Z","shell.execute_reply.started":"2026-02-09T23:21:01.365394Z","shell.execute_reply":"2026-02-09T23:21:01.386008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 17: Main Execution Part 1 - Data Loading\n# ============================================================================\n\ndef main():\n    logger.info(\"=\"*70)\n    logger.info(\"Task 6: ADVANCED DECOMPOSITION - SOTA Interpretability Techniques\")\n    logger.info(\"=\"*70)\n    \n    # Load data\n    logger.info(\"[1/7] Loading data\")\n    \n    def load_data(path: str) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        if not os.path.exists(path):\n            logger.warning(f\"{path} not found\")\n            return None, None\n        data = np.load(path)\n        return data['images'].astype('float32') / 255.0, data['labels']\n    \n    env1_images, env1_labels = load_data(TRAIN_DATA_PATH)\n    env2_images, env2_labels = load_data(TEST_DATA_PATH)\n    \n    if env1_images is None:\n        logger.error(\"Data not found. Please update paths\")\n        return\n    \n    logger.info(f\"Environment 1: {env1_images.shape}\")\n    logger.info(f\"Environment 2: {env2_images.shape}\")\n\n# ============================================================================\n# CELL 18: Main Execution Part 2 - Load Pre-trained Model\n# ============================================================================\n\n    # Load model\n    logger.info(\"[2/7] Loading biased model\")\n    model = CNN3Layer().to(device)\n    if os.path.exists(MODEL_PATH):\n        # Load the state dict\n        state_dict = torch.load(MODEL_PATH, map_location=device)\n        \n        # Create key mapping for different naming conventions\n        # Old model: features.0, features.3, features.6, classifier.0, classifier.3\n        # New model: conv1, conv2, conv3, fc1, fc2\n        key_mapping = {\n            'features.0.weight': 'conv1.weight',\n            'features.0.bias': 'conv1.bias',\n            'features.3.weight': 'conv2.weight',\n            'features.3.bias': 'conv2.bias',\n            'features.6.weight': 'conv3.weight',\n            'features.6.bias': 'conv3.bias',\n            'classifier.0.weight': 'fc1.weight',\n            'classifier.0.bias': 'fc1.bias',\n            'classifier.3.weight': 'fc2.weight',\n            'classifier.3.bias': 'fc2.bias',\n        }\n        \n        # Remap keys if needed\n        new_state_dict = {}\n        for old_key, value in state_dict.items():\n            new_key = key_mapping.get(old_key, old_key)\n            new_state_dict[new_key] = value\n        \n        model.load_state_dict(new_state_dict)\n        logger.info(f\"Loaded model from {MODEL_PATH}\")\n    else:\n        logger.error(\"Model not found!\")\n        return\n    model.eval()\n# ============================================================================\n# CELL 19: Main Execution Part 3 - Train TopK Sparse Autoencoder\n# ============================================================================\n    \n    # TECHNIQUE 1: TopK Sparse Autoencoder\n    logger.info(\"[3/7] Training TopK Sparse Autoencoder\")\n    logger.info(f\"Architecture: 128 -> {SAE_HIDDEN_DIM} (TopK={TOPK_K}) -> 128\")\n    \n    with torch.no_grad():\n        activations = ActivationUtils.get_activations_batched(\n            model, env1_images, device, return_tensor=True, show_progress=True\n        )\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    topk_sae = TopKSparseAutoencoder(FC1_UNITS, SAE_HIDDEN_DIM, TOPK_K).to(device)\n    optimizer = optim.Adam(topk_sae.parameters(), lr=SAE_LEARNING_RATE)\n    \n    dataset = TensorDataset(activations)\n    loader = DataLoader(dataset, batch_size=SAE_BATCH_SIZE, shuffle=True)\n    \n    for epoch in tqdm(range(SAE_EPOCHS), desc=\"Training SAE\"):\n        epoch_loss = 0\n        for batch in loader:\n            x = batch[0].to(device)\n            x_recon, z, _ = topk_sae(x)\n            loss = F.mse_loss(x_recon, x)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            with torch.no_grad():\n                topk_sae.decoder.weight.data = F.normalize(topk_sae.decoder.weight.data, dim=0)\n            \n            epoch_loss += loss.item()\n        \n        if (epoch + 1) % 10 == 0:\n            logger.info(f\"Epoch {epoch+1}/{SAE_EPOCHS}: Loss = {epoch_loss/len(loader):.6f}\")\n    \n# ============================================================================\n# CELL 20: Main Execution Part 4 - Concept Steering Vectors\n# ============================================================================\n\n    logger.info(\"[4/7] Computing Concept Steering Vectors\")\n    steering_vectors = ConceptSteeringVectors(model, device)\n    color_vector = steering_vectors.compute_steering_vectors(env1_images, env1_labels, \n                                            env2_images, env2_labels)\n\n    \n    logger.info(\"Testing color removal intervention\")\n    steering_vectors.intervention_experiment(env2_images[:MEMORY_LIMIT_SAMPLES], env2_labels[:MEMORY_LIMIT_SAMPLES])\n    \n\n    \n# ============================================================================\n# CELL 21: Main Execution Part 5 - Causal Neuron Ablation Analysis\n# ============================================================================\n\n    logger.info(\"[5/7] Performing Causal Neuron Ablation Analysis\")\n    gc.collect()\n    torch.cuda.empty_cache()\n    neuron_analyzer = CausalNeuronAnalyzer(model, device)\n    color_neurons, shape_neurons = neuron_analyzer.find_causal_neurons(\n        env1_images[:MEMORY_LIMIT_SAMPLES], env1_labels[:MEMORY_LIMIT_SAMPLES],\n        env2_images[:MEMORY_LIMIT_SAMPLES], env2_labels[:MEMORY_LIMIT_SAMPLES]\n    )\n# ============================================================================\n# CELL 22: Main Execution Part 6 - Cluster SAE Features\n# ============================================================================\n\n    logger.info(\"[6/7] Clustering SAE Features by Cross-Environment Behavior\")\n    clusters, cluster_info, feat_chars = cluster_features_by_environment(\n        model, topk_sae, \n        (env1_images, env1_labels), \n        (env2_images, env2_labels)\n    )\n# ============================================================================\n# CELL 23: Main Execution Part 7 - PHASE 1 Feature Classification\n# ============================================================================\n\n    # ==========================================\n    # PHASE 1 & 2 Execution\n    # ==========================================\n    logger.info(\"=\"*50)\n    logger.info(\"PHASE 1: Feature Identification & Classification\")\n    logger.info(\"=\"*50)\n    \n    # Initialize Classifier\n    feature_clf = FeatureClassifier(model, topk_sae, device)\n    \n    # Run Classification on Env1 (Colored) vs Generated B&W\n    traitors, heroes = feature_clf.classify_features(env1_images[:1000])\n    feature_clf.plot_sensitivity_analysis()\n# ============================================================================\n# CELL 24: Main Execution Part 8 - PHASE 2 Grad-CAM Validation\n# ============================================================================\n    \n    logger.info(\"=\"*50)\n    logger.info(\"PHASE 2: Feature Validation with Grad-CAM\")\n    logger.info(\"=\"*50)\n    \n    # Validate the discovered Traitors\n    validate_traitors_with_gradcam(\n        model, topk_sae, traitors, \n        env1_images, env1_labels, \n        device\n    )\n\n    \n\n    \n# ============================================================================\n# CELL 25: Main Execution Part 9 - Train Concept Probes\n# ============================================================================\n\n    logger.info(\"[7/7] Training Linear Concept Probes\")\n    shape_acc, color_acc = train_concept_probes(\n        model, env1_images, env1_labels, device\n    )\n    \n\n\n# ============================================================================\n# CELL 26: Main Execution Part 10 - Visualization: Analysis Plots\n# ============================================================================\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    try:\n        effects = [neuron_analyzer.neuron_effects[i] for i in range(FC1_UNITS)]\n        env1_effects = [e['env1_effect'] for e in effects]\n        env2_effects = [e['env2_effect'] for e in effects]\n        \n        axes[0].scatter(env1_effects, env2_effects, alpha=0.6, c='blue', edgecolors='k')\n        axes[0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n        axes[0].axvline(0, color='gray', linestyle='--', alpha=0.5)\n        axes[0].plot([-10, 10], [-10, 10], 'r--', alpha=0.5, label='Equal effect')\n        axes[0].set_xlabel('Effect on Env1 (Original Colors) %')\n        axes[0].set_ylabel('Effect on Env2 (Reversed Colors) %')\n        axes[0].set_title('Causal Neuron Analysis\\nNeurons above line = COLOR-specific')\n        axes[0].legend()\n        axes[0].set_xlim(-5, 5)\n        axes[0].set_ylim(-5, 5)\n        \n        axes[1].scatter(feat_chars[:, 0], feat_chars[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n        axes[1].set_xlabel('Cross-Environment Activation Difference')\n        axes[1].set_ylabel('Within-Environment Variance')\n        axes[1].set_title('SAE Feature Clustering\\nHigher X = More Color-Sensitive')\n        \n        plt.tight_layout()\n        plt.savefig('advanced_sae_analysis.png', dpi=150, bbox_inches='tight')\n        logger.info(\"Saved analysis plots to advanced_sae_analysis.png\")\n        plt.show()\n    finally:\n        plt.close(fig)\n        gc.collect()\n    \n# ============================================================================\n# CELL 27: Main Execution Part 11 - Visualization: Steering Effect Plot\n# ============================================================================\n\n    fig, ax = plt.subplots(figsize=(8, 5))\n    \n    try:\n        strengths = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n        results = steering_vectors.intervention_experiment(env2_images[:MEMORY_LIMIT_SAMPLES], env2_labels[:MEMORY_LIMIT_SAMPLES], strengths)\n        accs = [r['accuracy'] for r in results]\n        \n        ax.plot(strengths, accs, 'bo-', linewidth=2, markersize=8)\n        ax.axhline(accs[0], color='gray', linestyle='--', label='Baseline (no steering)')\n        ax.set_xlabel('Color Removal Strength')\n        ax.set_ylabel('Accuracy on Reversed-Color Data (%)')\n        ax.set_title('Effect of Removing \"Color Direction\" from Activations')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig('color_steering_effect.png', dpi=150, bbox_inches='tight')\n        logger.info(\"Saved steering effect plot to color_steering_effect.png\")\n        plt.show()\n    finally:\n        plt.close(fig)\n        gc.collect()\n# ============================================================================\n# CELL 28: Main Execution Part 12 - PHASE 3 Structural Analysis\n# ============================================================================\n\n    logger.info(\"=\"*50)\n    logger.info(\"PHASE 3: Structural Analysis\")\n    logger.info(\"=\"*50)\n    \n    analyzer = StructuralAnalyzer(topk_sae, device)\n    analyzer.analyze_correlations(env1_images[:1000], model)\n    circuits = analyzer.find_circuits(traitors, heroes)\n    analyzer.plot_circuit_analysis(traitors, heroes)\n    \n# ============================================================================\n# CELL 29: Main Execution Part 13 - PHASE 4 Causal Intervention\n# ============================================================================\n    \n    # Evaluate SAE Reconstruction Quality first\n    evaluate_reconstruction(model, topk_sae, (env1_images, env1_labels), device)\n    \n    logger.info(\"=\"*50)\n    logger.info(\"PHASE 4: Causal Intervention\")\n    logger.info(\"=\"*50)\n    \n    intervener = CausalIntervener(model, topk_sae, device)\n    \n    # 1. Identify failures in the Reversed (Hard) Environment\n    logger.info(\"Identifying failures in Env2 (Reversed Data)\")\n    failures = intervener.find_failure_cases(env2_images[:1000], env2_labels[:1000])\n    \n    # 2. Perform Surgery\n    if failures:\n        # Sweep first\n        intervener.sweep_intervention_strength(failures, traitors, heroes)\n        \n        # Then perform standard surgery with moderate boost factor\n        n_cured, cured_list = intervener.perform_surgery(\n            failures, traitors, heroes, boost_factor=2.0  # Use 2.0x instead of 2.5x\n        )\n        # 3. Visualize Cures\n        if n_cured > 0:\n            intervener.visualize_cures(cured_list, model, topk_sae, traitors, heroes, device)\n    else:\n        logger.info(\"No failures found to audit (Model is too good?)\")\n\n    # Save Models\n    torch.save(topk_sae.state_dict(), 'topk_sae_model.pth')\n    np.save('color_steering_vector.npy', color_vector)\n    logger.info(\"Saved SAE model and steering vector\")\n    \n    logger.info(\"Task 6 Complete - All Phases Executed\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.387279Z","iopub.execute_input":"2026-02-09T23:21:01.387535Z","iopub.status.idle":"2026-02-09T23:21:01.414476Z","shell.execute_reply.started":"2026-02-09T23:21:01.38751Z","shell.execute_reply":"2026-02-09T23:21:01.413944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 30: Run Main Function\n# ============================================================================\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T23:21:01.415348Z","iopub.execute_input":"2026-02-09T23:21:01.415628Z","iopub.status.idle":"2026-02-09T23:23:12.062643Z","shell.execute_reply.started":"2026-02-09T23:21:01.415599Z","shell.execute_reply":"2026-02-09T23:23:12.061916Z"}},"outputs":[],"execution_count":null}]}